{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Documentation Structure","text":"<p>This directory contains focused documentation for causaliq-workflow development.</p>"},{"location":"#essential-documents","title":"Essential Documents","text":""},{"location":"#start-here","title":"\ud83d\udccb Start Here","text":"<ul> <li>Development Roadmap - Complete feature list, progress tracking, and three-month implementation plan</li> <li>Example Workflows - Series-based workflow examples and YAML configuration patterns</li> </ul>"},{"location":"#design","title":"\ud83d\udd27 Design","text":"<ul> <li>Technical Architecture - System design, Series concept, component architecture, and workflow-specific development patterns</li> </ul>"},{"location":"#development-support","title":"\ud83d\udcd6 Development Support","text":"<ul> <li>LLM Development Guide - Comprehensive development standards and LLM communication patterns for the CausalIQ ecosystem</li> </ul>"},{"location":"#quick-navigation","title":"Quick Navigation","text":""},{"location":"#for-design-work","title":"For Design Work","text":"<p>Start with: Technical Architecture \u2192 Development Roadmap</p>"},{"location":"#for-understanding-requirements","title":"For Understanding Requirements","text":"<p>Start with: Development Roadmap \u2192 Example Workflows</p>"},{"location":"#for-llm-collaboration","title":"For LLM Collaboration","text":"<p>Use: LLM Development Guide (comprehensive, ecosystem-wide standards)</p>"},{"location":"#document-purpose-summary","title":"Document Purpose Summary","text":"Document Purpose When to Use <code>roadmap.md</code> Feature specifications and progress tracking Feature development planning <code>example_workflows.md</code> YAML workflow examples Understanding series concept <code>technical_architecture.md</code> Package architecture and design patterns Building components <p>This streamlined structure focuses on actionable implementation guidance while maintaining single sources of truth.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#core-action-framework","title":"Core Action Framework","text":""},{"location":"api/#causaliq_workflowaction","title":"causaliq_workflow.action","text":""},{"location":"api/#causaliq_workflow.action.Action","title":"Action","text":"<p>Base class for all workflow actions.</p> <p>Methods:</p> <ul> <li> <code>run</code>             \u2013              <p>Execute action with validated inputs, return outputs.</p> </li> <li> <code>validate_inputs</code>             \u2013              <p>Validate input values against input specifications.</p> </li> </ul>"},{"location":"api/#causaliq_workflow.action.Action.run","title":"run  <code>abstractmethod</code>","text":"<pre><code>run(inputs: Dict[str, Any]) -&gt; Dict[str, Any]\n</code></pre> <p>Execute action with validated inputs, return outputs.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Dict[str, Any]</code>           \u2013            <p>Dictionary of output values keyed by output name</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ActionExecutionError</code>             \u2013            <p>If action execution fails</p> </li> </ul>"},{"location":"api/#causaliq_workflow.action.Action.run(inputs)","title":"<code>inputs</code>","text":"(<code>Dict[str, Any]</code>)           \u2013            <p>Dictionary of input values keyed by input name</p>"},{"location":"api/#causaliq_workflow.action.Action.validate_inputs","title":"validate_inputs","text":"<pre><code>validate_inputs(inputs: Dict[str, Any]) -&gt; bool\n</code></pre> <p>Validate input values against input specifications.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if all inputs are valid</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ActionValidationError</code>             \u2013            <p>If validation fails</p> </li> </ul>"},{"location":"api/#causaliq_workflow.action.Action.validate_inputs(inputs)","title":"<code>inputs</code>","text":"(<code>Dict[str, Any]</code>)           \u2013            <p>Dictionary of input values to validate</p>"},{"location":"api/#causaliq_workflow.action.ActionInput","title":"ActionInput  <code>dataclass</code>","text":"<pre><code>ActionInput(\n    name: str,\n    description: str,\n    required: bool = False,\n    default: Any = None,\n    type_hint: str = \"Any\",\n)\n</code></pre> <p>Define action input specification.</p>"},{"location":"api/#causaliq_workflow.action.ActionOutput","title":"ActionOutput  <code>dataclass</code>","text":"<pre><code>ActionOutput(name: str, description: str, value: Any)\n</code></pre> <p>Define action output specification.</p>"},{"location":"api/#causaliq_workflow.action.ActionExecutionError","title":"ActionExecutionError","text":"<p>Raised when action execution fails.</p>"},{"location":"api/#causaliq_workflow.action.ActionValidationError","title":"ActionValidationError","text":"<p>Raised when action input validation fails.</p>"},{"location":"api/#workflow-execution-engine","title":"Workflow Execution Engine","text":""},{"location":"api/#causaliq_workflowworkflow","title":"causaliq_workflow.workflow","text":""},{"location":"api/#causaliq_workflow.workflow.WorkflowExecutor","title":"WorkflowExecutor","text":"<p>Parse and execute GitHub Actions-style workflows with matrix expansion.</p> <p>This class handles the parsing of YAML workflow files and expansion of matrix strategies into individual experiment jobs. It provides the foundation for executing multi-step causal discovery workflows with parameterised experiments using flexible action parameter templating.</p> <p>Methods:</p> <ul> <li> <code>expand_matrix</code>             \u2013              <p>Expand matrix variables into individual job configurations.</p> </li> <li> <code>parse_workflow</code>             \u2013              <p>Parse workflow YAML file with validation.</p> </li> </ul>"},{"location":"api/#causaliq_workflow.workflow.WorkflowExecutor.expand_matrix","title":"expand_matrix","text":"<pre><code>expand_matrix(matrix: Dict[str, List[Any]]) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Expand matrix variables into individual job configurations.</p> <p>Generates all combinations from matrix variables using cartesian product. Each combination becomes a separate job configuration.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>List[Dict[str, Any]]</code>           \u2013            <p>List of job configurations with matrix variables expanded</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>WorkflowExecutionError</code>             \u2013            <p>If matrix expansion fails</p> </li> </ul>"},{"location":"api/#causaliq_workflow.workflow.WorkflowExecutor.expand_matrix(matrix)","title":"<code>matrix</code>","text":"(<code>Dict[str, List[Any]]</code>)           \u2013            <p>Dictionary mapping variable names to lists of values</p>"},{"location":"api/#causaliq_workflow.workflow.WorkflowExecutor.parse_workflow","title":"parse_workflow","text":"<pre><code>parse_workflow(workflow_path: Union[str, Path]) -&gt; Dict[str, Any]\n</code></pre> <p>Parse workflow YAML file with validation.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Dict[str, Any]</code>           \u2013            <p>Parsed and validated workflow dictionary</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>WorkflowExecutionError</code>             \u2013            <p>If workflow parsing or validation fails</p> </li> </ul>"},{"location":"api/#causaliq_workflow.workflow.WorkflowExecutor.parse_workflow(workflow_path)","title":"<code>workflow_path</code>","text":"(<code>Union[str, Path]</code>)           \u2013            <p>Path to workflow YAML file</p>"},{"location":"api/#causaliq_workflow.workflow.WorkflowExecutionError","title":"WorkflowExecutionError","text":"<p>Raised when workflow execution fails.</p>"},{"location":"api/#schema-validation","title":"Schema Validation","text":""},{"location":"api/#causaliq_workflowschema","title":"causaliq_workflow.schema","text":""},{"location":"api/#causaliq_workflow.schema.WorkflowValidationError","title":"WorkflowValidationError","text":"<pre><code>WorkflowValidationError(message: str, schema_path: str = '')\n</code></pre> <p>Raised when workflow validation against JSON Schema fails.</p> <p>Parameters:</p>"},{"location":"api/#causaliq_workflow.schema.WorkflowValidationError(message)","title":"<code>message</code>","text":"(<code>str</code>)           \u2013            <p>Validation error description</p>"},{"location":"api/#causaliq_workflow.schema.WorkflowValidationError(schema_path)","title":"<code>schema_path</code>","text":"(<code>str</code>, default:                   <code>''</code> )           \u2013            <p>JSON Schema path where validation failed</p>"},{"location":"api/#causaliq_workflow.schema.load_schema","title":"load_schema","text":"<pre><code>load_schema(schema_path: Optional[Union[str, Path]] = None) -&gt; Dict[str, Any]\n</code></pre> <p>Load CausalIQ workflow JSON Schema.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Dict[str, Any]</code>           \u2013            <p>Parsed JSON Schema dictionary</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>WorkflowValidationError</code>             \u2013            <p>If schema file cannot be loaded</p> </li> </ul>"},{"location":"api/#causaliq_workflow.schema.load_schema(schema_path)","title":"<code>schema_path</code>","text":"(<code>Optional[Union[str, Path]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional path to custom schema file.         If None, loads default package schema.</p>"},{"location":"api/#causaliq_workflow.schema.validate_workflow","title":"validate_workflow","text":"<pre><code>validate_workflow(\n    workflow: Dict[str, Any], schema_path: Optional[Union[str, Path]] = None\n) -&gt; bool\n</code></pre> <p>Validate workflow against CausalIQ JSON Schema.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if workflow is valid</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>WorkflowValidationError</code>             \u2013            <p>If workflow validation fails</p> </li> </ul>"},{"location":"api/#causaliq_workflow.schema.validate_workflow(workflow)","title":"<code>workflow</code>","text":"(<code>Dict[str, Any]</code>)           \u2013            <p>Workflow configuration dictionary</p>"},{"location":"api/#causaliq_workflow.schema.validate_workflow(schema_path)","title":"<code>schema_path</code>","text":"(<code>Optional[Union[str, Path]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional path to custom schema file</p>"},{"location":"api/#causaliq_workflow.schema.load_workflow_file","title":"load_workflow_file","text":"<pre><code>load_workflow_file(file_path: Union[str, Path]) -&gt; Dict[str, Any]\n</code></pre> <p>Load workflow from YAML file.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Dict[str, Any]</code>           \u2013            <p>Parsed workflow dictionary</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>WorkflowValidationError</code>             \u2013            <p>If file cannot be loaded</p> </li> </ul>"},{"location":"api/#causaliq_workflow.schema.load_workflow_file(file_path)","title":"<code>file_path</code>","text":"(<code>Union[str, Path]</code>)           \u2013            <p>Path to workflow YAML file</p>"},{"location":"api/#built-in-actions","title":"Built-in Actions","text":""},{"location":"api/#causaliq_workflowactions","title":"causaliq_workflow.actions","text":""},{"location":"api/#causaliq_workflow.actions.DummyStructureLearnerAction","title":"DummyStructureLearnerAction","text":"<p>Dummy structure learning action that creates an empty graph file.</p> <p>Methods:</p> <ul> <li> <code>run</code>             \u2013              <p>Create a dummy empty GraphML file.</p> </li> <li> <code>validate_inputs</code>             \u2013              <p>Validate input values against input specifications.</p> </li> </ul>"},{"location":"api/#causaliq_workflow.actions.DummyStructureLearnerAction.run","title":"run","text":"<pre><code>run(inputs: Dict[str, Any]) -&gt; Dict[str, Any]\n</code></pre> <p>Create a dummy empty GraphML file.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Dict[str, Any]</code>           \u2013            <p>Dictionary with graph_path, node_count, and edge_count</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ActionExecutionError</code>             \u2013            <p>If file operations fail</p> </li> </ul>"},{"location":"api/#causaliq_workflow.actions.DummyStructureLearnerAction.run(inputs)","title":"<code>inputs</code>","text":"(<code>Dict[str, Any]</code>)           \u2013            <p>Dictionary containing data_path, output_dir, and matrix vars</p>"},{"location":"api/#causaliq_workflow.actions.DummyStructureLearnerAction.validate_inputs","title":"validate_inputs","text":"<pre><code>validate_inputs(inputs: Dict[str, Any]) -&gt; bool\n</code></pre> <p>Validate input values against input specifications.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if all inputs are valid</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ActionValidationError</code>             \u2013            <p>If validation fails</p> </li> </ul>"},{"location":"api/#causaliq_workflow.actions.DummyStructureLearnerAction.validate_inputs(inputs)","title":"<code>inputs</code>","text":"(<code>Dict[str, Any]</code>)           \u2013            <p>Dictionary of input values to validate</p>"},{"location":"api/#cli-interface","title":"CLI Interface","text":""},{"location":"api/#causaliq_workflowcli","title":"causaliq_workflow.cli","text":""},{"location":"api/#causaliq_workflow.cli","title":"cli","text":"<p>Command-line interface for causaliq-workflow.</p> <p>Functions:</p> <ul> <li> <code>cli</code>             \u2013              <p>Simple CLI example.</p> </li> <li> <code>main</code>             \u2013              <p>Entry point for the CLI.</p> </li> </ul>"},{"location":"api/#causaliq_workflow.cli.cli","title":"cli","text":"<pre><code>cli(name: str, greet: str) -&gt; None\n</code></pre> <p>Simple CLI example.</p> <p>NAME is the person to greet</p>"},{"location":"api/#causaliq_workflow.cli.main","title":"main","text":"<pre><code>main() -&gt; None\n</code></pre> <p>Entry point for the CLI.</p>"},{"location":"api/#usage-examples","title":"Usage Examples","text":""},{"location":"api/#using-workflowexecutor","title":"Using WorkflowExecutor","text":"<pre><code>from causaliq_workflow import WorkflowExecutor\nfrom causaliq_workflow.workflow import WorkflowExecutionError\n\n# Create executor instance\nexecutor = WorkflowExecutor()\n\ntry:\n    # Parse and validate workflow (includes template variable validation)\n    workflow = executor.parse_workflow(\"experiment.yml\")\n    print(f\"Workflow ID: {workflow['id']}\")\n    print(f\"Description: {workflow['description']}\")\n\n    # Expand matrix variables\n    if \"matrix\" in workflow:\n        jobs = executor.expand_matrix(workflow[\"matrix\"])\n        print(f\"Generated {len(jobs)} jobs from matrix\")\n\n        # Each job contains the expanded matrix variables\n        for i, job in enumerate(jobs):\n            print(f\"Job {i}: {job}\")\n            # Example: {'dataset': 'asia', 'algorithm': 'pc', 'alpha': 0.05}\n\nexcept WorkflowExecutionError as e:\n    if \"Unknown template variables\" in str(e):\n        print(f\"Template validation failed: {e}\")\n        # Example: \"Unknown template variables: missing_var. Available context: id, dataset, algorithm\"\n    else:\n        print(f\"Workflow execution failed: {e}\")\n</code></pre>"},{"location":"api/#template-variable-validation","title":"Template Variable Validation","text":"<pre><code># The WorkflowExecutor automatically validates template variables during parsing\n# Template variables ({{variable}}) are checked against:\n# - Workflow properties: id, description\n# - Matrix variables: variables defined in the matrix section\n\n# Example: Valid template usage\nvalid_workflow = {\n    \"id\": \"test-001\",\n    \"description\": \"Template validation example\", \n    \"matrix\": {\"dataset\": [\"asia\"], \"algorithm\": [\"pc\"]},\n    \"steps\": [{\n        \"uses\": \"dummy-structure-learner\",\n        \"with\": {\n            \"output\": \"/results/{{id}}/{{dataset}}_{{algorithm}}.xml\",\n            \"description\": \"Processing {{dataset}} with {{algorithm}}\"\n        }\n    }]\n}\n\n# Example: Invalid template usage (will raise WorkflowExecutionError)\ninvalid_workflow = {\n    \"id\": \"test-001\", \n    \"matrix\": {\"dataset\": [\"asia\"]},\n    \"steps\": [{\n        \"uses\": \"dummy-structure-learner\", \n        \"with\": {\n            \"output\": \"/results/{{unknown_var}}/result.xml\"  # unknown_var not in context\n        }\n    }]\n}\n</code></pre>"},{"location":"api/#flexible-path-configuration","title":"Flexible Path Configuration","text":"<pre><code># Example workflow YAML showing flexible action parameters\nworkflow_yaml = \"\"\"\nid: \"experiment-001\"\ndescription: \"Flexible causal discovery experiment\"\nmatrix:\n  dataset: [\"asia\", \"cancer\"]\n  algorithm: [\"pc\", \"ges\"]\n  alpha: [0.01, 0.05]\n\nsteps:\n  - name: \"Structure Learning\"\n    uses: \"dummy-structure-learner\"\n    with:\n      dataset: \"/experiments/data/{{dataset}}.csv\"\n      result: \"/experiments/results/{{id}}/{{algorithm}}/graph_{{dataset}}_{{alpha}}.xml\"\n      alpha: \"{{alpha}}\"\n      max_iter: 1000\n\"\"\"\n\n# Parse and expand\nexecutor = WorkflowExecutor()\nworkflow = executor.parse_workflow_from_string(workflow_yaml)\njobs = executor.expand_matrix(workflow[\"matrix\"])\n# Creates 8 jobs (2\u00d72\u00d72) with customizable file paths\n</code></pre>"},{"location":"api/#matrix-expansion-example","title":"Matrix Expansion Example","text":"<pre><code>from causaliq_workflow import WorkflowExecutor\n\nexecutor = WorkflowExecutor()\n\n# Define matrix\nmatrix = {\n    \"algorithm\": [\"pc\", \"ges\", \"lingam\"],\n    \"dataset\": [\"asia\", \"cancer\"],\n    \"alpha\": [0.01, 0.05]\n}\n\n# Expand into individual jobs\njobs = executor.expand_matrix(matrix)\n# Results in 12 jobs (3 \u00d7 2 \u00d7 2 combinations)\n\nfor job in jobs:\n    print(f\"Algorithm: {job['algorithm']}, Dataset: {job['dataset']}, Alpha: {job['alpha']}\")\n</code></pre>"},{"location":"api/#creating-a-custom-action","title":"Creating a Custom Action","text":"<pre><code>from causaliq_workflow.action import Action, ActionInput, ActionExecutionError\nfrom typing import Any, Dict\n\nclass MyStructureLearnerAction(Action):\n    \"\"\"Custom structure learning action.\"\"\"\n\n    name = \"my-structure-learner\"\n    version = \"1.0.0\"\n    description = \"Custom causal structure learning implementation\"\n    author = \"Your Name\"\n\n    inputs = {\n        \"data_path\": ActionInput(\n            name=\"data_path\",\n            description=\"Path to input CSV dataset\",\n            required=True,\n            type_hint=\"str\",\n        ),\n        \"output_dir\": ActionInput(\n            name=\"output_dir\",\n            description=\"Directory for output files\",\n            required=True,\n            type_hint=\"str\",\n        ),\n        \"alpha\": ActionInput(\n            name=\"alpha\",\n            description=\"Significance level for independence tests\",\n            required=False,\n            default=0.05,\n            type_hint=\"float\",\n        ),\n    }\n\n    outputs = {\n        \"graph_path\": \"Path to generated GraphML file\",\n        \"node_count\": \"Number of nodes in the learned graph\",\n        \"edge_count\": \"Number of edges in the learned graph\",\n    }\n\n    def run(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Execute the structure learning algorithm.\"\"\"\n        try:\n            # Your implementation here\n            return {\n                \"graph_path\": \"/path/to/output.graphml\",\n                \"node_count\": 5,\n                \"edge_count\": 8,\n            }\n        except Exception as e:\n            raise ActionExecutionError(f\"Structure learning failed: {e}\") from e\n</code></pre>"},{"location":"api/#validating-workflows","title":"Validating Workflows","text":"<pre><code>from causaliq_workflow.schema import validate_workflow, WorkflowValidationError\n\nworkflow_data = {\n    \"name\": \"My Experiment\",\n    \"id\": \"experiment-001\",\n    \"data_root\": \"/data\",\n    \"output_root\": \"/results\",\n    \"matrix\": {\n        \"dataset\": [\"asia\", \"cancer\"],\n        \"algorithm\": [\"pc\", \"ges\"],\n    },\n    \"steps\": [\n        {\n            \"name\": \"Structure Learning\",\n            \"uses\": \"my-structure-learner\",\n            \"with\": {\n                \"alpha\": 0.05,\n            },\n        }\n    ],\n}\n\ntry:\n    result = validate_workflow(workflow_data)\n    print(\"Workflow validation passed!\")\nexcept WorkflowValidationError as e:\n    print(f\"Validation failed: {e}\")\n    print(f\"Schema path: {e.schema_path}\")\n</code></pre>"},{"location":"api/#loading-custom-schemas","title":"Loading Custom Schemas","text":"<pre><code>from causaliq_workflow.schema import load_schema\nfrom pathlib import Path\n\n# Load custom schema\nschema_path = Path(\"my-custom-schema.json\")\nschema = load_schema(schema_path)\n\n# Use with validation\nvalidate_workflow(workflow_data, schema)\n</code></pre>"},{"location":"api/#development-guidelines","title":"Development Guidelines","text":""},{"location":"api/#action-implementation-patterns","title":"Action Implementation Patterns","text":"<ol> <li>Inherit from Action base class - Provides standardized interface</li> <li>Define comprehensive inputs - Use ActionInput for type safety</li> <li>Document outputs clearly - Help users understand action results</li> <li>Handle errors gracefully - Use ActionExecutionError and ActionValidationError</li> <li>Follow semantic versioning - Enable workflow compatibility tracking</li> <li>Create GraphML output - Use standardized format for causal graphs</li> </ol>"},{"location":"api/#testing-your-actions","title":"Testing Your Actions","text":"<pre><code>import pytest\nfrom pathlib import Path\nfrom causaliq_workflow.action import ActionExecutionError\n\ndef test_my_action_success():\n    \"\"\"Test successful action execution.\"\"\"\n    action = MyStructureLearnerAction()\n    inputs = {\n        \"data_path\": \"/path/to/test_data.csv\",\n        \"output_dir\": \"/path/to/output\",\n        \"alpha\": 0.05,\n    }\n\n    result = action.run(inputs)\n\n    assert \"graph_path\" in result\n    assert \"node_count\" in result\n    assert \"edge_count\" in result\n    assert Path(result[\"graph_path\"]).exists()\n\ndef test_my_action_missing_file():\n    \"\"\"Test action fails gracefully with missing input.\"\"\"\n    action = MyStructureLearnerAction()\n    inputs = {\n        \"data_path\": \"/nonexistent/file.csv\",\n        \"output_dir\": \"/path/to/output\",\n    }\n\n    with pytest.raises(ActionExecutionError):\n        action.run(inputs)\n</code></pre>"},{"location":"example_workflows/","title":"CausalIQ Workflow - Example Workflows","text":""},{"location":"example_workflows/#design-philosophy-inspired-by-ci-workflows","title":"Design Philosophy: Inspired by CI-Workflows","text":"<p>The CausalIQ Workflows are inspired by the concepts within  Continuous Integration (CI) workflows. They provide a subset of the features  provided by CI workflows and much of the functionality is simplified to ease the job of specifying CausalIQ workflows.</p> <p>Key concepts in CausalIQ workflows are:</p> <ul> <li>Workflows are a series of one or more sequential steps.</li> <li>Steps can be either an action or they can run shell commands.</li> <li>actions execute typical causal inference and evaluation activities like structure    learning, graph evaluation, causal inference etc., and actions:<ul> <li>are implemented in other CausalIQ packages such as causaliq-discovery,        causaliq-analysis etc. as specified in the uses: keyword of the step;</li> <li>take parameters values for the action - e.g., the algorithm to use - are specified using the with: keyword;</li> <li>actions can be implemented intelligently to perform their work efficiently and in parallel.</li> </ul> </li> <li>a matrix concept allows the set of steps to be repeated over multiple   combinations of values, for example over a set of networks, sample sizes and   algorithms. This is particularly valuable for large scale comparative experiments.</li> <li> <p>workflows may be run from the command line interface (CLI) using commands like:</p> <p><code>shell cwork example_workflow.yaml --network=child,property</code>   - CLI arguments can be used to override workflow and matrix parameters so     that the same workflow can be easily reused;   - the run: command can be used to run a workflow so that \"workflows of     workflows\" are supported.</p> </li> </ul> <p>The CausalIQ Workflow CLI has a --mode argument which controls its overall behaviour as follows:</p> <ul> <li>--mode=run: this actually executes the workflow. Note that, in this case  the actions in the workflow are performed conservatively - if the outputs of  actions are already present on the file system the action is not repeated. This  therefore faciltates restarting the workflow if it has previously been interrupted.</li> <li>--mode=dry-run: this is the default behaviour and reports what would be done if --mode=run were used, but does not perform the actual actions. In doing so,   it has the valuable side-effect of checking that the workflow definitions are   all valid.</li> <li>--mode=compare: all actions in the workflow will be re-run regardless of whether the output is present on the filesystem or not. If the output is present on the filesystem, the newly-generated output will replace it, and any differences with the previous version reported. This mode is therefore a very powerful form of functional testing.  </li> </ul>"},{"location":"example_workflows/#example-workflows-in-causal-discovery","title":"Example Workflows in Causal Discovery","text":""},{"location":"example_workflows/#simple-one-off-structure-learning-examples","title":"Simple One-Off Structure Learning Examples","text":"<p>The first simple example runs the Tabu-Stable structure learning algorithm on 1000 synthetic rows from the Asia network. Input and output files are generally specified in terms of their location in CausalIQ Workflows, and the filenames are fixed according to what kind of entity they are: a graph, metadata or trace for example.</p> <p>This will create a graph.xml and metadata.json file in the specified output folder. The debug switch also means that a trace.csv will be produced.</p> <pre><code># tabu_asia.yaml\ndescription: \"Tabu-stable learning Asia from 1K data with full trace\"\n\nsteps:\n  - name: \"Learn Structure\"\n    uses: \"causaliq-discovery\"\n    with:\n      algorithm: \"tabu-stable\"\n      sample_size: \"1k\"\n      dataset: \"/data/asia\"\n      output: \"/results/tabu-asia\"\n      debug: True\n</code></pre> <p>The same result could be achieved using the causaliq-discovery CLI command directly, but using this simple workflow avoids having to specify the dataset, algorithm etc. on the command line by using the following command:</p> <pre><code>cwork tabu_asia.yaml  # cwork is a synonym for causaliq-workflow\n</code></pre> <p>This \"shorthand\" capability becomes especially useful when plotting charts which  typically require a lot of parameter values to specify axis titles, chart type and colours etc., which become unmanageable to specify on the command line.</p> <p>Another simple workflow might be to download resources from Zenodo. This would download all the resources associated with my PhD thesis and unzip in the specified output folder.</p> <pre><code># download_paper.yaml\ndescription: \"Download assets for PhD Thesis\"\n\nsteps:\n  - name: \"Download paper\"\n    uses: \"causaliq-papers\"\n    with:\n      operation: \"download\"\n      doi: \"10.5072/zenodo.338579\"\n      output: \"/papers/2025KitsonThesis\"\n</code></pre>"},{"location":"example_workflows/#parameterised-structure-learning-example","title":"Parameterised Structure Learning Example","text":"<p>We can make the simple learning example more general by adding workflow-level variables to parameterise the workflow. In this example, the sample_size parameter has a default value of 1000 if not specified on the command line, whereas the None value for network indicates that this must be specified on the command line.  Note how the network and sample_size parameters are used in the dataset and output action parameters so that the correct input file and output folder are used.</p> <pre><code># tabu_learning.yaml\ndescription: \"Parameterised Tabu-stable learning\"\nsample_size: \"1K\"\nnetwork: None\n\nsteps:\n  - name: \"Learn Structure\"\n    uses: \"causaliq-discovery\"\n    with:\n      algorithm: \"tabu-stable\"\n      max_time: 10\n      sample_size: {{sample_size}}\n      dataset: \"/data/{{network}}\"\n      output: \"/results/{{network}}/{{sample_size}}\"\n</code></pre> <p>We can now run Tabu-Stable structure learning for any network and sample size using commands like:</p> <pre><code>cwork tabu_learning.yaml --network=child --sample_size=500\n</code></pre>"},{"location":"example_workflows/#multiple-step-examples","title":"Multiple Step Examples","text":"<p>We could enhance this by adding a step before structure learning where an LLM provides an initial graph. In this example, the dataset is provided so that the LLM has access to the values, and there is also some additional domain context provided in a json file. This could generate a graph.xml in the output folder and probably also some metadata and/or history of the prompts and responses  made to the LLM. Note that, this LLM generated graph would then be available to be re-used in other experiments, and the prompts/response trail provides the means to exactly replicate this result.</p> <pre><code># tabu_llm_learning.yaml\ndescription: \"Tabu-stable learning Asia with LLM initialisation\"\nsample_size: \"1K\"\nnetwork: None\n\nsteps:\n  - name: \"LLM Graph Initialisation\"\n    uses: \"causaliq-knowledge\"\n    with:\n      operation: \"propose-graph\"\n      dataset: \"/data/{{network}}\"\n      context: \"/knowledge/context/{{network}}\"\n      output: \"/knowledge/llm_graphs/{{network}}\"\n\n  - name: \"Learn Structure\"\n    uses: \"causaliq-discovery\"\n    with:\n      initial_graph: \"/knowledge/llm_graphs/asia\"\n      # etc ....\n</code></pre> <p>Alternatively, or additionally, we can add some steps after structure learning to compute some graph metrics and visualise the way that the graph was learnt.</p> <p>The \"Evaluate graph\" step requires the bic-score which needs the dataset to be specified, and F1 needs the true graph to be supplied - the relevant causaliq-analysis action is responsible for checking that these kind of dependencies are satisified. The results folder is specified where the learnt graph and learning metadata is, and so these results are appended to the metadata.</p> <pre><code># learn_and_evaluate.yaml\n\n# workflow variables ... \n\nsteps:\n\n  # step(s) to learn graph ...\n\n  - name: \"Evaluate graph\"\n    uses: \"causaliq-analysis\"\n    with:\n      operation: \"evaluate-graph\"\n      dataset: \"/data/{{network}}\"\n      true_graph: \"/knowledge/true_graph/{{network}}\"\n      metrics: [\"bic-score\", \"f1\", \"scaled-shd\"]\n      basis: [\"cpdag\"]\n      graph: \"/results/{{network}}/{{sample_size}}\"\n</code></pre>"},{"location":"example_workflows/#matrix-strategy-workflow","title":"Matrix Strategy Workflow","text":"<p>In many cases we will wish to run comparative experiments with a different result for each combinations of algorithm, network, and sample size and randomisation. Note how we structure the output folder path so that we ensure the result for each individual experiment is placed in its own folder. We also use an \"id\" workflow variablesto keep these results separate from those of other workflows.</p> <p>Internally, the action - in this case causal-discovery - may be implemented intelligently to maximise efficiency. For example, in this case, it may read the maximum number of rows from the filesystem dataset just once, and then adjust the effective sample size and randomisation internally for each individual experiment.</p> <pre><code># matrix_experiment.yaml\nid: \"stability001\"\ndescription: \"Algorithm stability comparison\"\nrandomise: [\"variable_order\", \"variable_name\"]\n\nmatrix:\n  network: [\"asia\", \"cancer\"]\n  algorithm: [\"pc\", \"ges\"]\n  sample_size: [\"100\", \"1K\"]\n  seed: [1, 25]\n\nsteps:\n  - name: \"Structure Learning\"\n    uses: \"causaliq-discovery\"\n    with:\n      algorithm: \"{{algorithm}}\"\n      sample_size: \"{{sample_size}}\"\n      dataset: \"/data/{{network}}\"\n      randomise: {{randomise}}\n      seed: {{seed}}\n      output: \"/results/{{id}}/{{algorithm}}/{{network}}/{{sample_size}}/{{seed}}\"\n</code></pre>"},{"location":"example_workflows/#workflow-of-workflows","title":"Workflow of workflows","text":"<p>The ability of one workflow to call other parameterised workflows facilitates the creation and testing of complex CausalIQ workflows, for instance to run all the experiments, analysis and asset generation for a research paper or thesis. CausalIQ Papers makes use of this to provide reproducibility of CausalIQ published papers.</p> <p>The example below shows a simple top level workflow which might reproduce the experiments and analysis behind a thesis. The lower level workflows such as create_chapter_4.yaml might then call other workflows to perform the structure learning, result analysis and asset generation for Chapter 4.</p> <pre><code># reproduce_thesis.yaml\n\nid: \"Kitson2025thesis\"\n\nmatrix:\n  chapter: [4, 5, 6]\n\nsteps:\n  name: \"Create chapter {{chapter}}\"\n  run: \"cwork create_chapter_{{chapter}}.yaml --id={{id}}\n</code></pre>"},{"location":"example_workflows/#parallel-jobs","title":"Parallel Jobs","text":"<p>CI workflows provide a jobs: keyword which allows multiple sequences of steps to run in parallel. We are not planning to implement this at the moment, instead relying on actions to provde parallelism using DAK tasks. This keeps CausalIQ Workflow functionality simple and reflects the fact that structure learning steps involving many individual structure learning experiments can keep even very powerful machines busy.</p>"},{"location":"example_workflows/#template-variables","title":"Template Variables","text":""},{"location":"example_workflows/#flexible-path-templating-pattern","title":"Flexible Path Templating Pattern","text":"<p>Our implementation supports flexible path templating using matrix variables:</p> <pre><code># Template variables can be used in action parameters:\n# {{id}} - workflow identifier\n# {{network}} - current matrix value for network\n# {{algorithm}} - current matrix value for algorithm  \n# {{sample_size}} - current matrix value for sample size\n\n# Example expansion for graphs using matrix above:\n# Job 1: dataset=\"/experiments/data/asia.csv\", result=\"/experiments/results/algo-comparison-001/pc/asia/100\"\n# Job 2: dataset=\"/experiments/data/asia.csv\", result=\"/experiments/results/algo-comparison-001/pc/asia/1K\"\n# Job 3: dataset=\"/experiments/data/asia.csv\", result=\"/experiments/results/algo-comparison-001/ges/graph_asia_0.01.xml\"\n# ... (8 total combinations)\n</code></pre>"},{"location":"example_workflows/#template-variable-validation","title":"Template Variable Validation","text":"<p>The workflow executor automatically validates that all template variables (<code>{{variable}}</code>) used in action parameters are available from either: - Workflow properties: <code>id</code>, <code>description</code>  - Matrix variables: Variables defined in the <code>matrix</code> section</p> <p>Valid Template Usage:</p> <pre><code>id: \"my-experiment-001\"\nmatrix:\n  dataset: [\"asia\", \"cancer\"]\n  algorithm: [\"pc\", \"ges\"]\n\nsteps:\n  - uses: \"dummy-structure-learner\"\n    with:\n      # These are all valid - variables exist in workflow context\n      output: \"/results/{{id}}/{{dataset}}_{{algorithm}}\"\n      description: \"Running {{algorithm}} on {{dataset}}\"\n</code></pre> <p>Invalid Template Usage (Validation Error):</p> <pre><code>id: \"my-experiment-001\"\nmatrix:\n  dataset: [\"asia\", \"cancer\"]\n\nsteps:\n  - uses: \"dummy-structure-learner\"\n    with:\n      # This will cause a WorkflowExecutionError\n      output: \"/results/{{unknown_variable}}/{{missing_param}}\"\n</code></pre> <p>Error Message Example:</p> <pre><code>WorkflowExecutionError: Unknown template variables: unknown_variable, missing_param\nAvailable context: id, description, dataset\n</code></pre>"},{"location":"example_workflows/#implemented-features","title":"Implemented Features","text":""},{"location":"example_workflows/#action-framework","title":"\u2705 Action Framework","text":"<ul> <li>Action Registration: Actions defined as Python classes</li> <li>Type Safety: Input/output specifications with type hints</li> <li>Error Handling: Comprehensive ActionExecutionError and ActionValidationError</li> <li>GraphML Output: Standardized format for causal graphs</li> </ul>"},{"location":"example_workflows/#workflow-execution-engine","title":"\u2705 Workflow Execution Engine","text":"<ul> <li>YAML Parsing: Parse and validate GitHub Actions-style workflow files</li> <li>Matrix Expansion: Convert matrix variables into individual job configurations</li> <li>Flexible Path Templating: User-controlled path generation with {{}} template variables</li> <li>Template Variable Validation: Automatic validation of {{variable}} patterns against available context</li> <li>Error Propagation: Comprehensive error handling with WorkflowExecutionError</li> </ul>"},{"location":"example_workflows/#schema-validation","title":"\u2705 Schema Validation","text":"<ul> <li>GitHub Actions Syntax: Familiar workflow patterns</li> <li>Matrix Variables: Full support for parameterized experiments  </li> <li>Flexible Action Parameters: Template variables in action <code>with:</code> blocks</li> <li>Action Parameters: with blocks for action configuration</li> </ul>"},{"location":"example_workflows/#test-coverage","title":"\u2705 Test Coverage","text":"<ul> <li>Functional Tests: Real filesystem operations with tracked test data</li> <li>Unit Tests: Mocked dependencies for action logic testing</li> <li>Schema Tests: Comprehensive validation of all workflow features</li> <li>100% Coverage: Complete test coverage including edge cases</li> </ul>"},{"location":"example_workflows/#example-action-implementation","title":"Example Action Implementation","text":"<pre><code>class DummyStructureLearnerAction(Action):\n    \"\"\"Reference action implementation.\"\"\"\n\n    name = \"dummy-structure-learner\"\n    version = \"1.0.0\"\n\n    inputs = {\n        \"data_path\": ActionInput(\n            name=\"data_path\",\n            description=\"Path to input CSV dataset (auto-constructed)\",\n            required=True,\n            type_hint=\"str\",\n        ),\n        \"output_dir\": ActionInput(\n            name=\"output_dir\", \n            description=\"Directory for output files (auto-constructed)\",\n            required=True,\n            type_hint=\"str\",\n        ),\n        \"dataset\": ActionInput(\n            name=\"dataset\",\n            description=\"Dataset name from matrix\",\n            required=True,\n            type_hint=\"str\",\n        ),\n        \"algorithm\": ActionInput(\n            name=\"algorithm\",\n            description=\"Algorithm name from matrix\",\n            required=True,\n            type_hint=\"str\",\n        ),\n    }\n\n    def run(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Create GraphML output file.\"\"\"\n        # Implementation creates valid GraphML file\n        # Returns graph_path, node_count, edge_count\n</code></pre>"},{"location":"example_workflows/#current-implementation-workflowexecutor","title":"Current Implementation: WorkflowExecutor","text":""},{"location":"example_workflows/#workflowexecutor-implementation-phase-1-complete","title":"WorkflowExecutor Implementation (Phase 1 Complete)","text":"<pre><code>from causaliq_workflow import WorkflowExecutor\n\n# Parse workflow and expand matrix\nexecutor = WorkflowExecutor()\nworkflow = executor.parse_workflow(\"experiment.yml\")\n\n# Matrix expansion example\nmatrix = {\"algorithm\": [\"pc\", \"ges\"], \"dataset\": [\"asia\", \"cancer\"], \"alpha\": [0.01, 0.05]}\njobs = executor.expand_matrix(matrix)  # Returns 8 job configurations\n\n# Example workflow with flexible paths\nworkflow_example = {\n    \"id\": \"experiment-001\",\n    \"description\": \"Flexible causal discovery experiment\",\n    \"matrix\": matrix,\n    \"steps\": [{\n        \"name\": \"Structure Learning\",\n        \"uses\": \"dummy-structure-learner\", \n        \"with\": {\n            \"dataset\": \"/experiments/data/{{dataset}}.csv\",\n            \"result\": \"/experiments/results/{{id}}/{{algorithm}}/graph_{{dataset}}_{{alpha}}.xml\",\n            \"alpha\": \"{{alpha}}\"\n        }\n    }]\n}\n\n# Each job contains expanded matrix variables for template substitution\nfor job in jobs:\n    print(f\"Job: {job}\")\n    # Example: {'algorithm': 'pc', 'dataset': 'asia', 'alpha': 0.01}\n</code></pre> <p>Implemented Features: - \u2705 Parse and validate YAML workflow files - \u2705 Expand matrix variables into job configurations - \u2705 Support flexible path templating with {{}} variables - \u2705 Comprehensive error handling and 100% test coverage</p> <p>Next Phase: Action execution engine with template variable substitution</p> <p>ges_series:     algorithm: \"ges\"      package: \"causaliq-discovery\"     datasets: [\"alarm\", \"asia\"]  # Reference same datasets     sample_sizes: [100, 500, 1000]     randomizations: 10     hyperparameters:       score_type: [\"bic\", \"aic\"]</p>"},{"location":"example_workflows/#resource-configuration","title":"Resource configuration","text":"<p>resources:   max_parallel_jobs: 8   memory_limit_per_job: \"4GB\"   runtime_limit_per_job: \"30m\"   total_runtime_limit: \"4h\"</p>"},{"location":"example_workflows/#analysis-configuration","title":"Analysis configuration","text":"<p>analysis:   compare_series: [\"pc_series\", \"ges_series\"]   metrics: [\"shd\", \"precision\", \"recall\", \"f1\"]   statistical_significance: 0.05   output_format: [\"csv\", \"json\"]</p>"},{"location":"example_workflows/#logging-and-monitoring","title":"Logging and monitoring","text":"<p>monitoring:   progress_updates: \"1m\"   log_level: \"INFO\"   save_intermediate: true</p> <pre><code>\n## Use Case 2: External Package Integration\n\n### Scenario\nUse R bnlearn package alongside Python algorithms for comprehensive comparison.\n\n### Multi-Package Workflow\n```yaml\n# multi_package_comparison.yaml\nmetadata:\n  name: \"python_r_algorithm_comparison\"\n  description: \"Compare Python and R causal discovery implementations\"\n\nseries:\n  python_pc:\n    algorithm: \"pc\"\n    package: \"causaliq-discovery\"\n    datasets: [\"alarm\"]\n    sample_sizes: [500, 1000]\n    randomizations: 5\n    hyperparameters:\n      alpha: 0.05\n\n  r_bnlearn_pc:\n    algorithm: \"pc.stable\"\n    package: \"r_bnlearn\"\n    datasets: [\"alarm\"]  # Same datasets for comparison\n    sample_sizes: [500, 1000] \n    randomizations: 5\n    hyperparameters:\n      alpha: 0.05\n      test: \"cor\"\n\n# External package requirements\nexternal_packages:\n  r_bnlearn:\n    check_installation: true\n    required_version: \"&gt;=4.7\"\n    install_if_missing: false  # Fail if not available\n\nresources:\n  max_parallel_jobs: 4  # Fewer for external packages\n  memory_limit_per_job: \"8GB\"\n\nvalidation:\n  dry_run: true  # Preview before execution\n  check_dependencies: true\n</code></pre>"},{"location":"example_workflows/#use-case-3-configuration-inheritance","title":"Use Case 3: Configuration Inheritance","text":""},{"location":"example_workflows/#scenario","title":"Scenario","text":"<p>Create specific experiments based on common base configuration.</p>"},{"location":"example_workflows/#base-configuration","title":"Base Configuration","text":"<pre><code># base_causal_experiment.yaml\nmetadata:\n  name: \"base_causal_discovery_template\"\n  description: \"Base template for causal discovery experiments\"\n\ndefaults:\n  datasets:\n    - name: \"alarm\" \n      zenodo_id: \"alarm_networks_v1\"\n    - name: \"asia\"\n      zenodo_id: \"asia_networks_v1\"\n  sample_sizes: [100, 500, 1000]\n  randomizations: 10\n\nresources:\n  max_parallel_jobs: 6\n  memory_limit_per_job: \"4GB\"\n  runtime_limit_per_job: \"1h\"\n\nmonitoring:\n  log_level: \"INFO\"\n  progress_updates: \"30s\"\n</code></pre>"},{"location":"example_workflows/#inherited-specific-experiment","title":"Inherited Specific Experiment","text":"<pre><code># pc_alpha_study.yaml\ninherits: \"base_causal_experiment.yaml\"\n\nmetadata:\n  name: \"pc_alpha_sensitivity_study\"\n  description: \"Study effect of alpha parameter on PC algorithm\"\n\noverrides:\n  series:\n    pc_alpha_study:\n      algorithm: \"pc\"\n      package: \"causaliq-discovery\" \n      hyperparameters:\n        alpha: [0.001, 0.01, 0.05, 0.1, 0.2]\n\n  analysis:\n    metrics: [\"shd\", \"precision\", \"recall\"]\n    focus_parameter: \"alpha\"\n    statistical_tests: true\n</code></pre>"},{"location":"example_workflows/#use-case-4-llm-integration-for-model-averaging","title":"Use Case 4: LLM Integration for Model Averaging","text":""},{"location":"example_workflows/#scenario_1","title":"Scenario","text":"<p>Use LLM to guide intelligent model averaging across multiple algorithm results.</p>"},{"location":"example_workflows/#llm-enhanced-workflow","title":"LLM-Enhanced Workflow","text":"<pre><code># llm_model_averaging.yaml\nmetadata:\n  name: \"llm_guided_model_averaging\"\n  description: \"Use LLM for intelligent model averaging\"\n\n# First run multiple algorithms\nseries:\n  pc_results:\n    algorithm: \"pc\"\n    package: \"causaliq-discovery\"\n    datasets: [\"healthcare_study\"]\n    sample_sizes: [1000]\n    randomizations: 5\n    hyperparameters:\n      alpha: 0.05\n\n  ges_results:\n    algorithm: \"ges\"\n    package: \"causaliq-discovery\"\n    datasets: [\"healthcare_study\"]\n    sample_sizes: [1000]\n    randomizations: 5\n    hyperparameters:\n      score_type: \"bic\"\n\n# LLM integration for analysis\nllm_analysis:\n  stage: \"post_discovery\"\n  package: \"causaliq-knowledge\"\n  tasks:\n    - task: \"analyze_algorithm_outputs\"\n      inputs: [\"pc_results\", \"ges_results\"]\n      domain_knowledge: \"healthcare/cardiology\"\n\n    - task: \"suggest_averaging_weights\"\n      based_on: \"algorithm_reliability_analysis\"\n\n    - task: \"validate_combined_graph\"\n      domain_expertise: true\n\n# Model averaging using LLM suggestions\nmodel_averaging:\n  package: \"causaliq-analysis\"\n  method: \"llm_weighted_average\"\n  inputs: [\"pc_results\", \"ges_results\", \"llm_suggestions\"]\n</code></pre>"},{"location":"example_workflows/#use-case-5-dataset-download-and-randomization","title":"Use Case 5: Dataset Download and Randomization","text":""},{"location":"example_workflows/#scenario_2","title":"Scenario","text":"<p>Automated dataset download from Zenodo with systematic randomization for robustness testing.</p>"},{"location":"example_workflows/#dataset-management-workflow","title":"Dataset Management Workflow","text":"<pre><code># dataset_robustness_study.yaml\nmetadata:\n  name: \"dataset_robustness_analysis\"\n  description: \"Study algorithm robustness to data variations\"\n\n# Dataset configuration with automatic download\ndatasets:\n  primary_dataset:\n    name: \"benchmark_networks\"\n    source: \"zenodo\"\n    zenodo_id: \"benchmark_causal_v2\"\n    cache_locally: true\n\n# Randomization strategies  \nrandomization:\n  strategies:\n    - type: \"subsample\"\n      fractions: [0.7, 0.8, 0.9, 1.0]\n\n    - type: \"variable_reorder\"\n      random_seeds: [42, 123, 456]\n\n    - type: \"noise_injection\"\n      noise_levels: [0.0, 0.05, 0.1]\n      noise_type: \"gaussian\"\n\n# Series using randomized datasets\nseries:\n  robustness_test:\n    algorithm: \"pc\"\n    package: \"causaliq-discovery\"\n    apply_all_randomizations: true\n    hyperparameters:\n      alpha: 0.05\n\n# Analysis focuses on robustness metrics\nanalysis:\n  robustness_metrics:\n    - \"stability_across_subsamples\"\n    - \"invariance_to_ordering\"\n    - \"noise_resistance\"\n  generate_robustness_report: true\n</code></pre>"},{"location":"example_workflows/#cli-usage-examples","title":"CLI Usage Examples","text":""},{"location":"example_workflows/#basic-execution","title":"Basic Execution","text":"<pre><code># Execute a series-based workflow\ncausaliq-workflow run pc_ges_comparison.yaml\n\n# Dry-run to preview execution plan\ncausaliq-workflow validate pc_ges_comparison.yaml --dry-run\n\n# Monitor running workflow\ncausaliq-workflow status workflow-abc123\n\n# Pause running workflow\ncausaliq-workflow pause workflow-abc123\n\n# Resume paused workflow  \ncausaliq-workflow resume workflow-abc123\n</code></pre>"},{"location":"example_workflows/#advanced-options","title":"Advanced Options","text":"<pre><code># Override resource limits\ncausaliq-workflow run experiment.yaml --max-jobs 16 --memory-per-job 8GB\n\n# Run specific series only\ncausaliq-workflow run experiment.yaml --series pc_series\n\n# Export results in specific format\ncausaliq-workflow results export workflow-abc123 --format csv --output results/\n</code></pre>"},{"location":"example_workflows/#python-api-examples","title":"Python API Examples","text":""},{"location":"example_workflows/#basic-workflow-execution","title":"Basic Workflow Execution","text":"<pre><code>from causaliq_workflow import WorkflowManager, ConfigurationManager\n\n# Load and validate configuration\nconfig_manager = ConfigurationManager()\nworkflow_config = config_manager.load_workflow(\"pc_ges_comparison.yaml\")\n\n# Execute workflow\nworkflow_manager = WorkflowManager()\nresult = workflow_manager.execute_workflow(workflow_config)\n\n# Access results by series\npc_results = result.get_series_results(\"pc_series\")\nges_results = result.get_series_results(\"ges_series\")\n</code></pre>"},{"location":"example_workflows/#series-analysis","title":"Series Analysis","text":"<pre><code>from causaliq_workflow.analysis import SeriesComparison\n\n# Compare algorithm performance across series\ncomparison = SeriesComparison()\ncomparison.add_series(\"PC Algorithm\", pc_results)\ncomparison.add_series(\"GES Algorithm\", ges_results)\n\n# Generate comparison metrics\nmetrics = comparison.compare_algorithms([\"shd\", \"precision\", \"recall\"])\nstatistical_significance = comparison.statistical_test(alpha=0.05)\n\n# Export results\ncomparison.export_results(\"algorithm_comparison.csv\")\n</code></pre>"},{"location":"example_workflows/#configuration-inheritance","title":"Configuration Inheritance","text":"<pre><code>from causaliq_workflow.config import ConfigurationInheritance\n\n# Create experiment based on template\ninheritance = ConfigurationInheritance()\nbase_config = inheritance.load_base(\"base_causal_experiment.yaml\")\n\n# Apply specific overrides\nspecific_config = inheritance.create_derived(\n    base_config,\n    overrides={\n        \"series\": {\n            \"custom_study\": {\n                \"algorithm\": \"fci\",\n                \"hyperparameters\": {\"alpha\": 0.01}\n            }\n        }\n    }\n)\n</code></pre> <p>This focused approach emphasizes the series concept and immediate implementation needs while providing practical examples for the three-month development phase.</p>"},{"location":"example_workflows/#use-case-2-production-causal-inference-workflow","title":"Use Case 2: Production Causal Inference Workflow","text":""},{"location":"example_workflows/#scenario_3","title":"Scenario","text":"<p>A business wants to continuously analyze the causal impact of marketing campaigns on customer behavior using streaming data.</p>"},{"location":"example_workflows/#workflow-configuration","title":"Workflow Configuration","text":"<pre><code># production_inference_workflow.yaml\nmetadata:\n  name: \"marketing_causal_inference\"\n  version: \"2.1\"\n  description: \"Real-time causal inference for marketing effectiveness\"\n  environment: \"production\"\n\nparameters:\n  data_stream: \"kafka://marketing-events\"\n  batch_size: 10000\n  update_frequency: \"1h\"\n  lookback_window: \"30d\"\n\nresources:\n  dask_cluster: \"kubernetes://marketing-cluster\"\n  min_workers: 3\n  max_workers: 20\n  auto_scale: true\n  memory_per_worker: \"4GB\"\n\nmonitoring:\n  metrics_endpoint: \"prometheus://metrics\"\n  alert_threshold: 0.1\n  slack_notifications: true\n\nsteps:\n  - name: \"stream_ingestion\"\n    package: \"causaliq-data\"\n    method: \"ingest_stream\"\n    parameters:\n      source: ${parameters.data_stream}\n      batch_size: ${parameters.batch_size}\n      schema_validation: true\n    outputs: [\"raw_events\"]\n\n  - name: \"real_time_preprocessing\"\n    package: \"causaliq-data\"\n    method: \"preprocess_streaming\"\n    depends_on: [\"stream_ingestion\"]\n    parameters:\n      window_size: ${parameters.lookback_window}\n      features: [\"campaign_type\", \"customer_segment\", \"channel\", \"timestamp\"]\n      target: \"conversion\"\n    inputs: [\"raw_events\"]\n    outputs: [\"processed_batch\"]\n\n  - name: \"causal_graph_update\"\n    package: \"causaliq-discovery\"\n    method: \"incremental_learning\"\n    depends_on: [\"real_time_preprocessing\"]\n    parameters:\n      existing_graph: \"models/marketing_graph.pkl\"\n      update_threshold: 1000\n      stability_check: true\n    inputs: [\"processed_batch\"]\n    outputs: [\"updated_graph\", \"graph_changes\"]\n\n  - name: \"intervention_effects\"\n    package: \"causaliq-inference\"\n    method: \"estimate_ate\"\n    depends_on: [\"causal_graph_update\"]\n    parameters:\n      treatment_vars: [\"campaign_type\", \"channel\"]\n      outcome_var: \"conversion\"\n      adjustment_sets: \"auto\"\n    inputs: [\"updated_graph\", \"processed_batch\"]\n    outputs: [\"treatment_effects\", \"confidence_intervals\"]\n\n  - name: \"anomaly_detection\"\n    package: \"causaliq-monitoring\"\n    method: \"detect_effect_anomalies\"\n    depends_on: [\"intervention_effects\"]\n    parameters:\n      baseline_window: \"7d\"\n      anomaly_threshold: 2.0\n    inputs: [\"treatment_effects\"]\n    outputs: [\"anomalies\", \"alerts\"]\n\n  - name: \"automated_insights\"\n    package: \"causaliq-workflow\"\n    method: \"llm_generate_insights\"\n    depends_on: [\"intervention_effects\", \"anomaly_detection\"]\n    parameters:\n      context: \"marketing_optimization\"\n      include_recommendations: true\n      business_constraints: \"config/business_rules.yaml\"\n    inputs: [\"treatment_effects\", \"anomalies\", \"graph_changes\"]\n    outputs: [\"insights\", \"recommendations\"]\n\n  - name: \"dashboard_update\"\n    package: \"causaliq-visualization\"\n    method: \"update_dashboard\"\n    depends_on: [\"automated_insights\"]\n    parameters:\n      dashboard_id: \"marketing_causal_dashboard\"\n      auto_refresh: true\n    inputs: [\"treatment_effects\", \"insights\", \"recommendations\"]\n    outputs: [\"dashboard_url\"]\n\ntriggers:\n  schedule: \"0 * * * *\"  # Every hour\n  data_threshold: 5000   # Trigger if 5k new events\n\nfailure_handling:\n  retry_attempts: 3\n  fallback_to_previous: true\n  alert_on_failure: true\n</code></pre>"},{"location":"example_workflows/#use-case-3-interactive-research-exploration","title":"Use Case 3: Interactive Research Exploration","text":""},{"location":"example_workflows/#scenario_4","title":"Scenario","text":"<p>An interactive session where researchers explore causal relationships with LLM assistance and iterative refinement.</p>"},{"location":"example_workflows/#workflow-configuration_1","title":"Workflow Configuration","text":"<pre><code># interactive_exploration_workflow.yaml\nmetadata:\n  name: \"interactive_causal_exploration\"\n  version: \"1.0\"\n  description: \"LLM-assisted interactive causal discovery\"\n  mode: \"interactive\"\n\nparameters:\n  data_source: \"research/climate_data.csv\"\n  domain: \"climate_science\"\n  interaction_mode: \"jupyter\"\n\nresources:\n  dask_cluster: \"local\"\n  workers: 2\n  memory_per_worker: \"6GB\"\n\ninteraction:\n  llm_model: \"gpt-4\"\n  enable_suggestions: true\n  save_conversation: true\n\nsteps:\n  - name: \"initial_analysis\"\n    package: \"causaliq-data\"\n    method: \"exploratory_analysis\"\n    parameters:\n      generate_summary: true\n      correlation_analysis: true\n    outputs: [\"data_summary\", \"correlations\"]\n\n  - name: \"llm_initial_consultation\"\n    package: \"causaliq-workflow\"\n    method: \"llm_domain_consultation\"\n    depends_on: [\"initial_analysis\"]\n    parameters:\n      domain: ${parameters.domain}\n      task: \"What causal relationships should we investigate in this climate dataset?\"\n      context_data: true\n    inputs: [\"data_summary\", \"correlations\"]\n    outputs: [\"domain_insights\", \"suggested_relationships\"]\n    interactive: true\n\n  - name: \"guided_discovery\"\n    package: \"causaliq-discovery\"\n    method: \"guided_search\"\n    depends_on: [\"llm_initial_consultation\"]\n    parameters:\n      prior_knowledge: \"domain_insights\"\n      search_strategy: \"hypothesis_driven\"\n    inputs: [\"data_summary\", \"suggested_relationships\"]\n    outputs: [\"candidate_graphs\"]\n    interactive: true\n\n  - name: \"iterative_refinement\"\n    type: \"interactive_loop\"\n    depends_on: [\"guided_discovery\"]\n    max_iterations: 10\n    steps:\n      - name: \"graph_evaluation\"\n        package: \"causaliq-validation\"\n        method: \"evaluate_graph\"\n        inputs: [\"candidate_graphs\"]\n        outputs: [\"evaluation_metrics\"]\n\n      - name: \"llm_feedback\"\n        package: \"causaliq-workflow\"\n        method: \"llm_evaluate_graph\"\n        parameters:\n          domain: ${parameters.domain}\n          include_suggestions: true\n        inputs: [\"candidate_graphs\", \"evaluation_metrics\"]\n        outputs: [\"llm_feedback\", \"improvement_suggestions\"]\n        interactive: true\n\n      - name: \"user_decision\"\n        type: \"user_input\"\n        prompt: \"Based on the LLM feedback, would you like to: (1) Accept graph (2) Refine further (3) Try different approach?\"\n        outputs: [\"user_choice\"]\n\n      - name: \"conditional_refinement\"\n        type: \"conditional\"\n        condition: \"user_choice == 'refine'\"\n        package: \"causaliq-discovery\"\n        method: \"refine_graph\"\n        inputs: [\"candidate_graphs\", \"improvement_suggestions\"]\n        outputs: [\"refined_graphs\"]\n\n  - name: \"final_interpretation\"\n    package: \"causaliq-workflow\"\n    method: \"llm_comprehensive_interpretation\"\n    depends_on: [\"iterative_refinement\"]\n    parameters:\n      domain: ${parameters.domain}\n      include_limitations: true\n      suggest_experiments: true\n    inputs: [\"candidate_graphs\", \"evaluation_metrics\"]\n    outputs: [\"final_interpretation\", \"experiment_suggestions\"]\n\nnotebook_integration:\n  auto_generate_cells: true\n  include_visualizations: true\n  save_checkpoints: true\n</code></pre>"},{"location":"example_workflows/#example-cli-usage","title":"Example CLI Usage","text":"<pre><code># Execute a workflow\ncausaliq-workflow run workflow.yaml --config production.yaml\n\n# Validate workflow before execution\ncausaliq-workflow validate workflow.yaml\n\n# Interactive mode\ncausaliq-workflow interactive --domain healthcare --data patient_data.csv\n\n# Monitor running workflow\ncausaliq-workflow status workflow-123\n\n# Generate workflow template\ncausaliq-workflow template --type discovery --domain finance\n\n# List available packages and methods\ncausaliq-workflow list-methods --package causaliq-discovery\n</code></pre>"},{"location":"example_workflows/#integration-examples","title":"Integration Examples","text":""},{"location":"example_workflows/#python-api-usage","title":"Python API Usage","text":"<pre><code>from causaliq_workflow import WorkflowEngine, DaskClusterManager\n\n# Set up DASK cluster\ncluster_manager = DaskClusterManager()\nclient = cluster_manager.create_local_cluster(workers=4)\n\n# Initialize workflow engine\nengine = WorkflowEngine(client=client)\n\n# Load and execute workflow\nresult = engine.execute_workflow(\"workflow.yaml\")\n\n# Access results\ncausal_graph = result.get_output(\"ensemble_graph\")\ninterpretation = result.get_output(\"interpretation\")\n\n# Interactive LLM consultation\nllm_analyzer = engine.get_llm_analyzer()\ninsights = llm_analyzer.interpret_results(result.all_outputs, domain=\"healthcare\")\n</code></pre>"},{"location":"example_workflows/#jupyter-notebook-integration","title":"Jupyter Notebook Integration","text":"<pre><code>%%causaliq_workflow\nmetadata:\n  name: \"notebook_workflow\"\n\nsteps:\n  - name: \"discovery\"\n    package: \"causaliq-discovery\"\n    method: \"pc\"\n    parameters:\n      alpha: 0.05\n\n# Results automatically displayed in notebook\n</code></pre>"},{"location":"roadmap/","title":"CausalIQ Workflow - Development Roadmap &amp; Progress","text":"<p>Single source of truth for all development planning and progress tracking</p> <p>Last updated: 2025-11-13</p>"},{"location":"roadmap/#current-status-phase-1-2-transition-action-framework-workflow-engine-90-complete","title":"Current Status: Phase 1-2 Transition - Action Framework + Workflow Engine [90% COMPLETE]","text":""},{"location":"roadmap/#major-achievement-complete-action-framework-workflowexecutor-implementation","title":"\ud83c\udfaf Major Achievement: Complete Action Framework + WorkflowExecutor Implementation","text":"<p>Key Breakthrough: We've successfully implemented both a robust action framework AND the core workflow execution engine. The framework provides type-safe action composition, comprehensive error handling, proven patterns for workflow orchestration, and now includes complete YAML workflow parsing with matrix expansion capabilities.</p> <p>Latest Achievement: WorkflowExecutor class with 99-line implementation featuring YAML workflow parsing, cartesian product matrix expansion, dynamic path construction, and comprehensive schema validation - all with 100% test coverage (65 tests total).</p> <p>Implementation Highlights: - \u2705 Action framework foundation - Abstract base classes with type-safe input/output specifications - \u2705 GraphML format adoption - Design decision for causal graph representation (DAGs, PDAGs, CPDAGs, MAGs, PAGs) - \u2705 Matrix variable architecture - Schema support for parameterized experiments - \u2705 GitHub Actions-inspired syntax - Familiar workflow patterns with schema validation</p>"},{"location":"roadmap/#phase-1-features-month-1-action-framework-foundation-100-complete","title":"Phase 1 Features (Month 1): Action Framework Foundation \u2705 100% Complete","text":""},{"location":"roadmap/#foundation-infrastructure-completed","title":"\u2705 Foundation Infrastructure [COMPLETED]","text":"<ul> <li>[x] Testing framework - Comprehensive pytest setup covering unit, functional, integration (47/47 tests passing)</li> <li>[x] CI/CD workflow - GitHub Actions workflow with linting, formatting, type checking</li> <li>[x] Code quality - Black, isort, flake8, MyPy integration with 100% compliance</li> <li>[x] Documentation structure - MkDocs integration for API documentation</li> <li>[x] Development environment - Complete workspace setup with proper tooling</li> <li>[x] Configuration foundation - JSON Schema-based workflow validation established</li> </ul>"},{"location":"roadmap/#action-framework-completed","title":"\u2705 Action Framework [COMPLETED]","text":"<ul> <li>[x] Action base classes - Abstract Action class with type-safe input/output specifications</li> <li>[x] Error handling - ActionExecutionError and ActionValidationError with comprehensive context</li> <li>[x] Input/output specification - ActionInput dataclass for type hints and validation</li> <li>[x] Reference implementation - DummyStructureLearnerAction demonstrating framework patterns</li> <li>[x] GraphML format decision - Adopted GraphML as standard for causal graph representation</li> <li>[x] Matrix variable support - Actions receive dataset, algorithm, and parameter inputs</li> </ul>"},{"location":"roadmap/#workflow-schema-integration-completed","title":"\u2705 Workflow Schema Integration [COMPLETED]","text":"<ul> <li>[x] GitHub Actions-inspired syntax - Familiar workflow patterns adapted for causal discovery</li> <li>[x] Matrix strategy support - Parameterized experiments with matrix variable expansion</li> <li>[x] Path construction fields - data_root, output_root, id fields for organizing experiment outputs  </li> <li>[x] Action parameters - with blocks for passing parameters to actions</li> <li>[x] Schema validation - JSON Schema validation with comprehensive error reporting</li> </ul>"},{"location":"roadmap/#phase-2-features-current-workflow-execution-engine-60-complete","title":"Phase 2 Features (Current): Workflow Execution Engine [60% Complete]","text":""},{"location":"roadmap/#ci-style-workflow-engine-completed","title":"\u2705 CI-Style Workflow Engine [COMPLETED]","text":"<ul> <li>[x] WorkflowExecutor class - Complete 99-line implementation with comprehensive testing (65 total tests, 100% coverage)</li> <li>[x] Workflow parser - Parse GitHub Actions-style YAML workflows with schema validation</li> <li>[x] Matrix expansion - Convert matrix variables into individual experiment jobs using cartesian product</li> <li>[x] Path construction - Dynamic file path generation from matrix variables with flexible templating</li> <li>[x] Schema validation - JSON Schema validation with corrected $schema/$id fields and required id/description</li> <li>[x] Error handling - Comprehensive validation and parsing error management</li> </ul>"},{"location":"roadmap/#research-reproducibility-platform-simplified-architecture-5-commits-to-working-workflow","title":"\ud83d\udd04 Research Reproducibility Platform [SIMPLIFIED ARCHITECTURE - 5 Commits to Working Workflow]","text":"<p>Architectural Focus: Start with simple sequential step execution and matrix expansion. Add complexity only when proven necessary.</p> <p>Commit 1: Template Variable Validation \u2705 COMPLETED - [x] Template extraction - Parse <code>{{variable}}</code> patterns from action parameters - [x] Context validation - Verify template variables exist in matrix + workflow properties - [x] Error reporting - Clear errors for unknown/malformed template variables - [x] Comprehensive tests - Cover valid, invalid, and malformed template scenarios</p> <p>Commit 2: Intelligent Action Framework &amp; Conservative Execution - [ ] Package-level actions - Actions like <code>causaliq-discovery</code> with algorithm parameters (no required versioning) - [ ] Conservative execution - Actions skip work if outputs exist, enabling safe workflow restarts - [ ] Mode-based operation - <code>--mode=dry-run|run|compare</code> for validation, execution, and functional testing - [ ] Smart action capabilities - Actions optimize internally (dataset loading, caching) transparently to users - [ ] Action-level validation - Actions validate their own inputs (integrated with dry-run mode) - [ ] Standardized output format - Fixed filenames by type (graph.xml, metadata.json, trace.csv)</p> <p>Commit 3: CLI &amp; Workflow Composition - [ ] CLI implementation - <code>cwork</code> command with parameter injection: <code>cwork workflow.yml --network=asia</code> - [ ] Workflow calling capability - <code>cwork</code> commands in workflow steps for composition - [ ] Template parameter flow - CLI and caller parameters available as <code>{{parameter}}</code> in workflows - [ ] Flexible parameter model - No rigid workflow input schemas, actions decide what they need</p> <p>Commit 4: Action Registry &amp; Step Execution Engine - [ ] ActionRegistry class - Centralized registry for action discovery and instantiation - [ ] Step executor - Execute <code>uses:</code> action steps via ActionRegistry - [ ] Shell command support - Handle <code>run:</code> command execution and <code>cwork</code> workflow calling - [ ] Parameter mapping - Map workflow <code>with:</code> blocks to action inputs with CLI parameter injection</p> <p>Commit 5: Production CLI &amp; Real Actions - [ ] Enhanced CLI - Full <code>cwork</code> implementation with mode support: <code>cwork workflow.yml --network=asia --mode=run</code> - [ ] Real algorithm actions - PC, GES, FCI structure learning implementations via causaliq-discovery - [ ] CausalIQ package integration - causaliq-discovery, causaliq-analysis, causaliq-knowledge, causaliq-papers - [ ] Hierarchical output organization - Standardized folder structures for experiment results</p> <p>Milestone Achievement: After these 5 commits, CausalIQ Workflow will support: - \u2705 Sequential step execution with matrix expansion - \u2705 Intelligent action optimization with dry-run and caching - \u2705 Workflow composition via CLI parameter passing and shell commands - \u2705 Research reproducibility platform foundation for causaliq-papers integration</p>"},{"location":"roadmap/#future-enhancements-when-proven-necessary","title":"\ud83d\udd2e Future Enhancements [When Proven Necessary]","text":"<p>Parallel Jobs Support: Add <code>jobs:</code> syntax and parallel execution when performance demands require it DASK Integration: Step-level parallelization for computationally intensive actions Formal Parameter Schemas: Optional workflow input definitions for enhanced validation when workflows become complex - \u2705 DASK-powered task parallelization within steps - \u2705 Workflow composition via calling with parameters - \u2705 Intelligent action optimization with dry-run and caching - \u2705 Research reproducibility platform foundation for causaliq-papers integration - [ ] Data file handling - Read actual CSV datasets and produce results - [ ] Algorithm parameters - Support real algorithm configuration options</p> <p>Milestone Achievement: After these 8 commits, CausalIQ Workflow will support: - \u2705 Parallel job execution with dependency management - \u2705 DASK-powered task parallelization within steps - \u2705 Workflow composition via calling with parameters - \u2705 Intelligent action optimization with dry-run and caching - \u2705 Research reproducibility platform foundation for causaliq-papers integration</p>"},{"location":"roadmap/#research-reproducibility-ecosystem-future-integration-with-causaliq-papers","title":"\ud83d\udd2e Research Reproducibility Ecosystem [FUTURE - Integration with causaliq-papers]","text":"<p>Vision: CausalIQ Workflow serves as the execution engine for a comprehensive research reproducibility platform.</p> <p>causaliq-papers Integration Architecture:</p> <pre><code># High-level research reproducibility workflow\ncausaliq-papers replicate peters2023causal --target=figure3\n\n# causaliq-papers processes paper dependencies and generates:\n\u251c\u2500\u2500 workflow-dependencies.yml    # Analyzes what's needed for figure3\n\u251c\u2500\u2500 optimized-reproduction.yml   # Generates minimal workflow-of-workflows\n\u2514\u2500\u2500 execution-plan.json         # Dependency graph for execution\n\n# Then calls causaliq-workflow to execute:\ncausaliq-workflow run optimized-reproduction.yml --target=figure3\n</code></pre> <p>Workflow-of-Workflows Pattern: - Paper reproduction = Top-level workflow calling component workflows - Dependency resolution = causaliq-papers analyzes workflow graph to minimize execution - Asset targeting = Generate only requested paper assets (tables, figures, results) - Intelligence integration = Actions optimize across the entire workflow graph</p>"},{"location":"roadmap/#algorithm-integration-future-after-working-workflow","title":"\u23f8\ufe0f Algorithm Integration [FUTURE - After Working Workflow]","text":"<ul> <li>[ ] Advanced algorithms - Additional causal discovery algorithms beyond PC/GES</li> <li>[ ] Package plugins - bnlearn (R), Tetrad (Java), causal-learn (Python) integration</li> <li>[ ] Cross-language bridges - rpy2, py4j integration for R/Java algorithm access</li> <li>[ ] Algorithm benchmarking - Systematic comparison across algorithm implementations</li> </ul>"},{"location":"roadmap/#success-metrics-phase-1-phase-2-partial","title":"Success Metrics - Phase 1 \u2705 + Phase 2 Partial \u2705","text":"<ul> <li>\u2705 Framework Foundation: Action framework with type-safe interfaces implemented</li> <li>\u2705 Schema Architecture: GitHub Actions-inspired workflow syntax with matrix support  </li> <li>\u2705 Reference Implementation: DummyStructureLearnerAction proving framework viability</li> <li>\u2705 Format Decision: GraphML adopted as standard for causal graph representation</li> <li>\u2705 Workflow Parsing: Complete WorkflowExecutor with YAML parsing and matrix expansion</li> <li>\u2705 Path Construction: Dynamic file path generation from matrix variables</li> <li>\u2705 Schema Validation: Corrected JSON Schema with proper $id field and field requirements</li> <li>\u2705 Test Coverage: 100% coverage maintained across 65 comprehensive tests</li> </ul>"},{"location":"roadmap/#next-milestone-functional-causal-discovery-workflow","title":"Next Milestone: Functional Causal Discovery Workflow","text":"<p>Target: Complete working workflow capable of executing real causal discovery experiments Success Criteria:  - Execute complete workflows from command line - Support real structure learning algorithms (PC, GES) - Handle matrix expansion with parallel step execution - Generate organized experimental outputs with GraphML graphs - Maintain 100% test coverage and CI compliance</p> <p>Timeline: 4 focused commits transitioning from framework to working research tool   - \u2705 Required/optional section validation per pattern   - \u2705 Hierarchical field validation with detailed error reporting   - \u2705 Flexible validation schemas defined in external YAML - [x] Flexible workflow patterns - 5 patterns supporting diverse research needs   - \u2705 Series pattern for comparative research (algorithm comparison across datasets/parameters)   - \u2705 Task pattern for sequential operations (preprocessing \u2192 algorithm \u2192 analysis)   - \u2705 Mixed pattern combining multiple approaches   - \u2705 Workflow pattern for DAG-based workflows with dependencies   - \u2705 Longitudinal_research pattern for temporal causal discovery studies - [ ] Configuration inheritance - Create workflows based on templates with overrides</p>"},{"location":"roadmap/#ci-style-workflow-engine-completed_1","title":"\u2705 CI-Style Workflow Engine [COMPLETED]","text":"<ul> <li>[x] Workflow parser - Parse GitHub Actions-style YAML workflows</li> <li>[x] Matrix expansion - Convert matrix variables into individual experiment jobs  </li> <li>[x] Path construction - Dynamic file path generation from matrix variables</li> <li>[x] Schema validation - JSON Schema validation with required id/description fields</li> <li>[x] WorkflowExecutor class - Complete 99-line implementation with comprehensive testing</li> <li>[ ] Step execution - Execute workflow steps with action-based architecture</li> <li>[ ] Environment management - Handle workflow environment variables and context</li> <li>[ ] Conditional execution - Support <code>if:</code> conditions in workflow steps</li> <li>[ ] Artifact handling - Manage inputs/outputs between workflow steps</li> </ul>"},{"location":"roadmap/#dask-task-graph-integration-pending","title":"\u23f8\ufe0f DASK Task Graph Integration [PENDING]","text":"<ul> <li>[ ] Matrix job expansion - Convert matrix configs into DASK task graphs</li> <li>[ ] Dependency management - Handle job dependencies with DASK</li> <li>[ ] Local cluster management - Setup and manage local DASK clusters</li> <li>[ ] Progress monitoring - Track workflow execution with real-time updates</li> <li>[ ] Resource estimation - Estimate compute requirements for planning</li> </ul>"},{"location":"roadmap/#configuration-migration-pending","title":"\u23f8\ufe0f Configuration Migration [PENDING]","text":"<ul> <li>[ ] CI workflow validation - Ensure CI workflows validate correctly</li> <li>[ ] Documentation update - Update all docs to reflect CI workflow approach</li> </ul>"},{"location":"roadmap/#phase-2-features-month-2-research-integration-not-started","title":"Phase 2 Features (Month 2): Research Integration [NOT STARTED]","text":""},{"location":"roadmap/#algorithm-package-integration","title":"\u23f8\ufe0f Algorithm Package Integration","text":"<ul> <li>[ ] R bnlearn integration - Execute R bnlearn algorithms via rpy2</li> <li>Matrix-driven algorithm selection: <code>algorithm: [\"pc\", \"iamb\", \"gs\"]</code></li> <li>[ ] Java Tetrad integration - Integration with Java-based Tetrad via py4j</li> <li>Cross-language workflow steps with data serialization</li> <li>[ ] Python causal-learn - Direct integration with Python algorithms</li> <li>Native Python execution within workflow steps</li> <li>[ ] Package discovery - Automatic detection of available packages</li> <li>[ ] Dependency validation - Check required packages before workflow execution</li> </ul>"},{"location":"roadmap/#dataset-management-with-ci-patterns","title":"\u23f8\ufe0f Dataset Management with CI Patterns","text":"<ul> <li>[ ] Zenodo integration - Dataset download as workflow action</li> <li><code>uses: zenodo-download@v1</code> action pattern</li> <li>[ ] Dataset caching - Local storage and reuse with cache actions</li> <li>[ ] Matrix dataset expansion - Multiple datasets in workflow matrix</li> <li><code>matrix: {dataset: [\"asia\", \"sachs\"], sample_size: [100, 1000]}</code></li> <li>[ ] Dataset transformations - Preprocessing steps as workflow actions</li> </ul>"},{"location":"roadmap/#advanced-matrix-workflows","title":"\u23f8\ufe0f Advanced Matrix Workflows","text":"<ul> <li>[ ] Cross-product expansion - Full matrix combinations with intelligent batching</li> <li>[ ] Conditional matrices - Include/exclude matrix combinations based on conditions</li> <li>[ ] Matrix job dependencies - Sequential and parallel matrix job orchestration</li> <li>[ ] Result aggregation - Collect and combine results across matrix jobs</li> </ul>"},{"location":"roadmap/#llm-integration-as-actions","title":"\u23f8\ufe0f LLM Integration as Actions","text":"<ul> <li>[ ] Model averaging action - LLM-guided model averaging as reusable action</li> <li>[ ] Hypothesis generation - LLM analysis steps in workflow</li> <li>[ ] Result interpretation - LLM post-processing actions</li> <li>[ ] Research workflow templates - Pre-built workflows for common research patterns</li> </ul>"},{"location":"roadmap/#phase-3-features-month-3-production-ci-features-not-started","title":"Phase 3 Features (Month 3): Production CI Features [NOT STARTED]","text":""},{"location":"roadmap/#advanced-workflow-management","title":"\u23f8\ufe0f Advanced Workflow Management","text":"<ul> <li>[ ] Workflow queuing - Manage multiple concurrent workflows like CI runners</li> <li>[ ] Pause/resume - Interrupt and restart workflows with state preservation</li> <li>[ ] Workflow artifacts - Persistent storage and retrieval of workflow outputs</li> <li>[ ] Workflow caching - Cache intermediate results for faster re-runs</li> <li>[ ] Branch/PR workflows - Different workflows for different experiment branches</li> </ul>"},{"location":"roadmap/#enterprise-ci-features","title":"\u23f8\ufe0f Enterprise CI Features","text":"<ul> <li>[ ] Secrets management - Secure handling of API keys and credentials</li> <li>[ ] Environment isolation - Containerized execution environments</li> <li>[ ] Resource limits - CPU, memory, and time limits per workflow/job</li> <li>[ ] Approval workflows - Human approval steps for expensive experiments</li> <li>[ ] Scheduled workflows - Cron-style scheduled execution</li> </ul>"},{"location":"roadmap/#monitoring-and-observability","title":"\u23f8\ufe0f Monitoring and Observability","text":"<ul> <li>[ ] Workflow status dashboard - Real-time workflow execution monitoring</li> <li>[ ] Job logs and traces - Detailed logging with searchable history</li> <li>[ ] Performance metrics - Resource usage, timing, and efficiency tracking</li> <li>[ ] Alert integration - Notifications for workflow success/failure</li> <li>[ ] Audit trail - Complete execution history for reproducibility</li> </ul>"},{"location":"roadmap/#results-and-artifacts","title":"\u23f8\ufe0f Results and Artifacts","text":"<ul> <li>[ ] Standardized outputs - Replace pickle files with structured formats</li> <li>[ ] Version tracking - Track algorithm versions and parameter changes</li> <li>[ ] Result comparison - Compare outputs across workflow runs</li> <li>[ ] Export capabilities - Multiple output formats (CSV, JSON, HDF5)</li> <li>[ ] Reproducibility metadata - Complete metadata for result reproduction</li> </ul>"},{"location":"roadmap/#success-criteria-by-phase","title":"Success Criteria by Phase","text":""},{"location":"roadmap/#phase-1-success-metrics","title":"Phase 1 Success Metrics","text":"<ul> <li>[ ] Execute GitHub Actions-style YAML workflows locally</li> <li>[ ] Matrix expansion generates individual causal discovery jobs</li> <li>[ ] Package-level algorithm integration (bnlearn, Tetrad, causal-learn)</li> <li>[ ] DASK task graph execution with progress monitoring</li> <li>[ ] Jinja2 template processing for workflow variables</li> </ul>"},{"location":"roadmap/#phase-2-success-metrics","title":"Phase 2 Success Metrics","text":"<ul> <li>[ ] Multi-language workflows (R, Java, Python) in single configuration</li> <li>[ ] Automatic dataset download and matrix expansion across datasets</li> <li>[ ] LLM integration actions for model averaging and analysis</li> <li>[ ] Advanced matrix workflows with conditional execution</li> <li>[ ] Research workflow templates for common causal discovery patterns</li> </ul>"},{"location":"roadmap/#phase-3-success-metrics","title":"Phase 3 Success Metrics","text":"<ul> <li>[ ] Production-grade workflow queue management</li> <li>[ ] Enterprise features: secrets, isolation, limits, approvals</li> <li>[ ] Comprehensive monitoring dashboard with real-time status</li> <li>[ ] Standardized result formats with complete reproducibility metadata</li> <li>[ ] Foundation ready for large-scale research deployment</li> </ul>"},{"location":"roadmap/#post-three-month-features-research-phase","title":"Post Three-Month Features (Research Phase)","text":""},{"location":"roadmap/#q2-2026-advanced-research-features","title":"Q2 2026: Advanced Research Features","text":"<ul> <li>Workflow marketplace - Sharing and discovering research workflow templates</li> <li>Interactive notebooks - Jupyter integration with workflow execution</li> <li>Publication workflows - Generate reproducible research outputs automatically</li> <li>Domain knowledge integration - Expert knowledge as workflow conditions</li> </ul>"},{"location":"roadmap/#q3-q4-2026-migration-and-scale","title":"Q3-Q4 2026: Migration and Scale","text":"<ul> <li>Multi-machine execution - Distributed workflows across compute clusters</li> <li>Cloud provider integration - AWS, GCP, Azure workflow runners</li> <li>GPU acceleration - Support for GPU-accelerated algorithms</li> <li>Web interface - Browser-based workflow designer and monitor</li> </ul>"},{"location":"roadmap/#beyond-2026-advanced-capabilities","title":"Beyond 2026: Advanced Capabilities","text":"<ul> <li>Workflow orchestration - Complex multi-stage research workflows</li> <li>Real-time collaboration - Multiple researchers on shared workflows</li> <li>AI-assisted optimization - Automated hyperparameter and workflow tuning</li> <li>Integration ecosystem - Plugins for major research tools and platforms</li> </ul> <p>This roadmap leverages the familiar GitHub Actions paradigm while building a powerful platform specifically designed for causal discovery research workflows.</p>"},{"location":"technical_architecture/","title":"CausalIQ Workflow - Technical Architecture","text":""},{"location":"technical_architecture/#architectural-vision-scalable-research-reproducibility-platform","title":"Architectural Vision: Scalable Research Reproducibility Platform","text":""},{"location":"technical_architecture/#core-architecture-principles","title":"Core Architecture Principles","text":"<p>CausalIQ Workflow is designed as a workflow orchestration engine that enables reproducible causal discovery research through:</p> <ol> <li>Sequential Steps with Matrix Expansion: Simple, predictable workflow execution with powerful parameterization</li> <li>Conservative Execution: Actions skip work if outputs already exist, enabling safe workflow restart and efficient re-runs</li> <li>Mode-Based Operation: <code>--mode=dry-run|run|compare</code> provides validation, execution, and functional testing capabilities</li> <li>Intelligent Actions: Actions optimize internally (e.g., loading datasets once at maximum sample size) while hiding complexity from users  </li> <li>Implicit Parameter Passing: CLI parameters flow through workflows without formal definitions</li> <li>Action-Level Validation: Each action validates its own inputs (integrated with dry-run capability)</li> <li>Workflow Composition: Workflows can call other workflows via <code>cwork</code> commands, enabling complex research workflows</li> <li>Standardized Output: Fixed filenames by type (<code>graph.xml</code>, <code>metadata.json</code>, <code>trace.csv</code>) with hierarchical organization</li> </ol>"},{"location":"technical_architecture/#research-reproducibility-pattern","title":"Research Reproducibility Pattern","text":"<p>Paper Reproduction = Workflow-of-Workflows where: - Top-level workflow defines paper reproduction strategy - Component workflows handle specific analyses (structure learning, visualization, etc.) - causaliq-papers processes workflow dependencies to generate targeted execution plans - causaliq-workflow executes the optimized workflow graph</p>"},{"location":"technical_architecture/#example-simplified-workflow-architecture","title":"Example: Simplified Workflow Architecture","text":"<pre><code># paper-reproduction.yml (top-level workflow)\nid: \"peters2023causal-reproduction\"\nmatrix:\n  network: [\"asia\", \"cancer\"]\n  algorithm: [\"pc\", \"ges\", \"fci\"]\n\nsteps:\n  - name: \"Structure Learning\"\n    uses: \"causaliq-discovery\"\n    with:\n      algorithm: \"{{algorithm}}\"\n      dataset: \"/data/{{network}}\"\n      output: \"/results/{{id}}/{{algorithm}}/{{network}}\"\n\n  - name: \"Analysis\"\n    uses: \"causaliq-analysis\"\n    with:\n      operation: \"evaluate-graph\"\n      graph: \"/results/{{id}}/{{algorithm}}/{{network}}\"\n\n  - name: \"Generate Figures\"\n    uses: \"causaliq-visualization\"\n    with:\n      input: \"/results/{{id}}/{{algorithm}}/{{network}}\"\n      output: \"/results/{{id}}/figures\"\n\n# Workflow composition via CLI calls\nsteps:\n  - name: \"Run Structure Learning\"\n    run: |\n      cwork structure-learning.yml \\\n        --network=\"{{network}}\" --algorithm=\"{{algorithm}}\"\n</code></pre>"},{"location":"technical_architecture/#conservative-execution-mode-control","title":"Conservative Execution &amp; Mode Control","text":"<pre><code># CLI execution modes\ncwork workflow.yml --mode=dry-run    # Default: validate and preview (no execution)\ncwork workflow.yml --mode=run        # Execute workflow (skip if outputs exist)\ncwork workflow.yml --mode=compare    # Re-execute and compare with existing outputs\n</code></pre>"},{"location":"technical_architecture/#action-intelligence-efficiency","title":"Action Intelligence &amp; Efficiency","text":"<pre><code># Actions support robust execution patterns with validation\naction.run(inputs, dry_run=True)    # (a) Validate and preview execution\naction.run(inputs, force=False)     # (b) Skip if output file already exists (conservative)\naction.compare(inputs)              # (c) Regenerate and compare with filesystem\n\n# Implicit parameter passing - no formal definitions needed\nclass StructureLearnerAction(Action):\n    def run(self, inputs, matrix_job, dry_run=False):\n        # Action handles its own validation\n        self.validate_inputs(inputs)\n\n        # Conservative execution: skip if outputs exist\n        if not inputs.force and self.outputs_exist(inputs):\n            return self.load_existing_outputs(inputs)\n\n        if dry_run:\n            return self.simulate_execution(inputs)\n        return self.learn_structure(inputs)\n</code></pre>"},{"location":"technical_architecture/#system-overview","title":"System Overview","text":"<p>The causaliq-workflow serves as the orchestration layer within the CausalIQ ecosystem, coordinating causal discovery experiments through GitHub Actions-inspired YAML workflows. This architecture models causal discovery experiments as familiar CI/CD workflows, providing unprecedented flexibility while leveraging proven workflow patterns.</p>"},{"location":"technical_architecture/#current-implementation-status","title":"Current Implementation Status","text":""},{"location":"technical_architecture/#implemented-components-v010","title":"\u2705 Implemented Components (v0.1.0)","text":"<ol> <li>Action Framework (<code>causaliq_workflow.action</code>)</li> <li>Abstract base class for workflow actions</li> <li>Type-safe input/output specifications</li> <li>Comprehensive error handling with <code>ActionExecutionError</code> and <code>ActionValidationError</code></li> <li> <p>100% test coverage with unit and functional tests</p> </li> <li> <p>Schema Validation (<code>causaliq_workflow.schema</code>)</p> </li> <li>JSON Schema-based workflow validation</li> <li>Support for GitHub Actions-style syntax</li> <li>Matrix variables, with parameters, data_root/output_root validation</li> <li> <p>Comprehensive error reporting with schema path context</p> </li> <li> <p>Workflow Execution Engine (<code>causaliq_workflow.workflow</code>)</p> </li> <li><code>WorkflowExecutor</code> class for parsing YAML workflows</li> <li>Matrix expansion with cartesian product generation</li> <li>Path construction from matrix variables and workflow configuration</li> <li> <p>Integration with existing schema validation</p> </li> <li> <p>Dummy Structure Learner Action (<code>causaliq_workflow.actions.dummy_structure_learner</code>)</p> </li> <li>Reference implementation demonstrating action framework</li> <li>GraphML output format for causal graph representation</li> <li>Matrix variable support (dataset, algorithm parameters)</li> <li> <p>Real filesystem operations with proper path construction</p> </li> <li> <p>Workflow Schema (<code>causaliq_workflow.schemas.causaliq-workflow.json</code>)</p> </li> <li>GitHub Actions-inspired syntax with causal discovery extensions</li> <li>Matrix strategy support for parameterized experiments</li> <li>Path construction with <code>data_root</code>, <code>output_root</code>, and <code>id</code> fields</li> <li>Action parameters via <code>with</code> blocks</li> </ol>"},{"location":"technical_architecture/#core-architectural-decisions","title":"Core Architectural Decisions","text":""},{"location":"technical_architecture/#github-actions-foundation","title":"GitHub Actions Foundation","text":"<p>The architecture is built on GitHub Actions workflow patterns, adapted for causal discovery:</p> <pre><code>name: \"Causal Discovery Experiment\"\nid: \"asia-comparison-001\"\ndata_root: \"/data\"\noutput_root: \"/results\"\n\nmatrix:\n  dataset: [\"asia\", \"cancer\"]  \n  algorithm: [\"pc\", \"ges\"]\n\nsteps:\n  - name: \"Structure Learning\"\n    uses: \"dummy-structure-learner\"\n    with:\n      alpha: 0.05\n      max_iter: 1000\n</code></pre>"},{"location":"technical_architecture/#action-based-components","title":"Action-Based Components","text":"<p>Actions are reusable workflow components with semantic versioning:</p> <pre><code>class Action(ABC):\n    \"\"\"Abstract base class for all workflow actions.\"\"\"\n\n    name: str                    # Action identifier\n    version: str                 # Semantic version\n    description: str             # Human description  \n    inputs: Dict[str, ActionInput]   # Type-safe inputs\n    outputs: Dict[str, str]      # Output descriptions\n\n    @abstractmethod\n    def run(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Execute the action with given inputs.\"\"\"\n</code></pre>"},{"location":"technical_architecture/#core-architecture-components","title":"Core Architecture Components","text":""},{"location":"technical_architecture/#1-workflow-execution-engine-causaliq_workflowworkflow","title":"1. Workflow Execution Engine (<code>causaliq_workflow.workflow</code>)","text":"<pre><code>class WorkflowExecutor:\n    \"\"\"Parse and execute GitHub Actions-style workflows with matrix expansion.\"\"\"\n\n    def parse_workflow(self, workflow_path: Union[str, Path]) -&gt; Dict[str, Any]:\n        \"\"\"Parse workflow YAML file with validation and template variable validation.\"\"\"\n\n    def expand_matrix(self, matrix: Dict[str, List[Any]]) -&gt; List[Dict[str, Any]]:\n        \"\"\"Expand matrix variables into individual job configurations.\"\"\"\n\n    def _extract_template_variables(self, text: Any) -&gt; Set[str]:\n        \"\"\"Extract {{variable}} patterns from strings using regex.\"\"\"\n\n    def _validate_template_variables(self, variables: Set[str], context: Set[str]) -&gt; None:\n        \"\"\"Validate template variables against available context.\"\"\"\n\n    def _collect_template_variables(self, obj: Any) -&gt; Set[str]:\n        \"\"\"Recursively collect template variables from workflow configuration.\"\"\"\n</code></pre> <p>Current Implementation Status: \u2705 Phase 1 Complete - Parse and validate YAML workflow files - Matrix expansion with cartesian product generation - Template variable validation with context checking - Path construction from matrix variables - 100% test coverage with comprehensive error handling</p> <p>Next Phase: Step execution, environment management, conditional execution</p>"},{"location":"technical_architecture/#2-package-level-algorithm-registry-causaliq_workflowalgorithms","title":"2. Package-Level Algorithm Registry (<code>causaliq_workflow.algorithms</code>)","text":"<pre><code>class AlgorithmRegistry:\n    \"\"\"Manage package-level algorithm plugins.\"\"\"\n\n    def discover_packages(self) -&gt; List[AlgorithmPackage]:\n        \"\"\"Auto-discover bnlearn, Tetrad, causal-learn packages.\"\"\"\n\n    def execute_algorithm(self, package: str, algorithm: str, \n                         data: Dataset, params: Dict) -&gt; Result:\n        \"\"\"Execute algorithm with cross-language bridge handling.\"\"\"\n\nclass BnlearnPackage:\n    \"\"\"R bnlearn package integration via rpy2.\"\"\"\n\nclass TetradPackage:\n    \"\"\"Java Tetrad package integration via py4j.\"\"\"\n\nclass CausalLearnPackage:  \n    \"\"\"Python causal-learn direct integration.\"\"\"\n</code></pre>"},{"location":"technical_architecture/#3-action-based-component-library-causaliq_workflowactions","title":"3. Action-Based Component Library (<code>causaliq_workflow.actions</code>)","text":"<pre><code>class ActionRegistry:\n    \"\"\"Manage reusable workflow actions.\"\"\"\n\n    def register_action(self, name: str, version: str, action: Action) -&gt; None:\n        \"\"\"Register versioned action: load-network@v1.\"\"\"\n\n    def execute_action(self, action_ref: str, inputs: Dict) -&gt; ActionResult:\n        \"\"\"Execute action with input validation and output handling.\"\"\"\n\nclass LoadNetworkAction(Action):\n    \"\"\"Action: load-network@v1 - Load causal network dataset.\"\"\"\n\nclass CausalDiscoveryAction(Action):\n    \"\"\"Action: causal-discovery@v1 - Run causal discovery algorithm.\"\"\"\n\nclass EvaluateGraphAction(Action):\n    \"\"\"Action: evaluate-graph@v1 - Evaluate learned graph against true graph.\"\"\"\n</code></pre>"},{"location":"technical_architecture/#ci-workflow-syntax-examples","title":"CI Workflow Syntax Examples","text":""},{"location":"technical_architecture/#basic-matrix-workflow","title":"Basic Matrix Workflow","text":"<pre><code>name: \"Algorithm Comparison\"\n\nstrategy:\n  matrix:\n    algorithm: [\"PC\", \"GES\", \"LINGAM\"]\n    network: [\"asia\", \"sachs\", \"alarm\"]\n    sample_size: [100, 500, 1000]\n\n  exclude:\n    - algorithm: \"LINGAM\"\n      network: \"alarm\"  # LINGAM doesn't work with discrete data\n\nsteps:\n  - name: \"Load Network Data\"\n    uses: \"load-network@v1\"\n    with:\n      network_name: \"${{ matrix.network }}\"\n      sample_size: \"${{ matrix.sample_size }}\"\n\n  - name: \"Run Causal Discovery\"\n    uses: \"causal-discovery@v1\"\n    with:\n      algorithm: \"${{ matrix.algorithm }}\"\n      package: \"auto-detect\"\n      data: \"${{ steps.load_network.outputs.dataset }}\"\n\n  - name: \"Evaluate Results\"\n    uses: \"evaluate-graph@v1\"\n    with:\n      learned_graph: \"${{ steps.causal_discovery.outputs.graph }}\"\n      true_graph: \"${{ steps.load_network.outputs.true_graph }}\"\n</code></pre>"},{"location":"technical_architecture/#conditional-execution","title":"Conditional Execution","text":"<pre><code>steps:\n  - name: \"Run PC Algorithm\"\n    uses: \"causal-discovery@v1\"\n    with:\n      algorithm: \"pc\"\n      package: \"bnlearn\"\n\n  - name: \"Run GES Algorithm\"\n    if: \"${{ matrix.sample_size &gt;= 500 }}\"  # Only for larger samples\n    uses: \"causal-discovery@v1\"\n    with:\n      algorithm: \"ges\"\n      package: \"causal-learn\"\n</code></pre>"},{"location":"technical_architecture/#dask-integration-architecture","title":"DASK Integration Architecture","text":""},{"location":"technical_architecture/#ci-workflow-dask-task-graph-conversion","title":"CI Workflow \u2192 DASK Task Graph Conversion","text":"<pre><code>class DaskTaskGraphBuilder:\n    \"\"\"Convert CI workflows into DASK task graphs.\"\"\"\n\n    def build_workflow_graph(self, workflow: WorkflowDefinition) -&gt; Dict:\n        \"\"\"Convert CI workflow steps into DASK computation graph.\"\"\"\n\n    def handle_matrix_strategy(self, strategy: MatrixStrategy) -&gt; List[TaskDefinition]:\n        \"\"\"Convert matrix strategy into parallel DASK tasks.\"\"\"\n\n    def manage_cross_language_bridges(self, action: Action) -&gt; TaskWrapper:\n        \"\"\"Manage R/Java bridges with proper lifecycle and cleanup.\"\"\"\n</code></pre>"},{"location":"technical_architecture/#resource-management","title":"Resource Management","text":"<p>CI workflow features map to DASK execution controls:</p> <ul> <li><code>max_parallel</code>: DASK worker pool size limitation</li> <li><code>timeout_minutes</code>: Per-task timeout enforcement</li> <li><code>runs_on</code>: DASK cluster specification (local/remote)</li> <li><code>fail_fast</code>: Task failure propagation strategy</li> </ul>"},{"location":"technical_architecture/#cross-language-integration","title":"Cross-Language Integration","text":""},{"location":"technical_architecture/#r-bnlearn-integration","title":"R bnlearn Integration","text":"<ul> <li>Bridge: rpy2 with automatic data conversion</li> <li>Lifecycle: Package-level R session management</li> <li>Error Handling: Graceful R exception translation</li> </ul>"},{"location":"technical_architecture/#java-tetrad-integration","title":"Java Tetrad Integration","text":"<ul> <li>Bridge: py4j with JVM lifecycle management</li> <li>Data Conversion: Pandas \u2194 Tetrad data structures</li> <li>Resource Management: Proper JVM cleanup</li> </ul>"},{"location":"technical_architecture/#python-causal-learn-integration","title":"Python causal-learn Integration","text":"<ul> <li>Direct Integration: Native Python execution</li> <li>Optimised Paths: No cross-language overhead</li> <li>Data Handling: Efficient NumPy/Pandas operations</li> </ul>"},{"location":"technical_architecture/#template-processing","title":"Template Processing","text":""},{"location":"technical_architecture/#github-actions-style-variables","title":"GitHub Actions-Style Variables","text":"<pre><code>env:\n  RANDOM_SEED: 42\n  DATA_DIR: \"${{ github.workspace }}/data\"\n\nsteps:\n  - name: \"Process ${{ matrix.network }} with ${{ matrix.algorithm }}\"\n    with:\n      output_path: \"${{ env.DATA_DIR }}/results/${{ matrix.network }}_${{ matrix.algorithm }}.json\"\n</code></pre>"},{"location":"technical_architecture/#jinja2-implementation","title":"Jinja2 Implementation","text":"<ul> <li>Syntax: <code>${{ variable.property }}</code> exactly matching GitHub Actions</li> <li>Context: Matrix variables, environment variables, step outputs</li> <li>Security: Safe template processing with variable validation</li> </ul>"},{"location":"technical_architecture/#integration-with-causaliq-ecosystem","title":"Integration with CausalIQ Ecosystem","text":""},{"location":"technical_architecture/#package-coordination","title":"Package Coordination","text":"<ul> <li>causaliq-discovery: Core algorithms integrated as package plugins</li> <li>causaliq-knowledge: Knowledge provision via action-based architecture</li> <li>causaliq-analysis: Statistical analysis actions and workflow post-processing</li> <li>causaliq-experiments: Configuration and result storage with CI workflow metadata</li> </ul>"},{"location":"technical_architecture/#development-standards","title":"Development Standards","text":"<ul> <li>GitHub Actions schema compliance: Official JSON schema for validation</li> <li>Action versioning: Semantic versioning for all reusable actions</li> <li>CausalIQ integration standards: Plugin architecture, result standardisation</li> <li>79-character line limit: All code adheres to CausalIQ formatting standards</li> <li>Type safety: Full MyPy type checking with strict configuration</li> </ul>"},{"location":"technical_architecture/#design-patterns","title":"Design Patterns","text":""},{"location":"technical_architecture/#yaml-first-configuration","title":"YAML-First Configuration","text":"<ul> <li>All workflow functionality expressible through YAML</li> <li>External workflow definitions without code changes</li> <li>Clear, actionable error messages for configuration issues</li> </ul>"},{"location":"technical_architecture/#package-level-plugin-architecture","title":"Package-Level Plugin Architecture","text":"<ul> <li>Dynamic discovery and registration of algorithm packages</li> <li>Cross-language bridge management at package level</li> <li>Preference resolution for algorithm conflicts</li> </ul>"},{"location":"technical_architecture/#action-based-composability","title":"Action-Based Composability","text":"<ul> <li>Reusable, versioned workflow components</li> <li>Standardised input/output interfaces</li> <li>Community potential for shared actions</li> </ul> <p>This architecture transforms causal discovery workflow definition from domain-specific patterns into familiar CI/CD workflows, dramatically reducing the learning curve while providing enterprise-grade features for research.</p>"},{"location":"design/action_architecture_design/","title":"Action-Based Component Architecture","text":""},{"location":"design/action_architecture_design/#overview","title":"Overview","text":"<p>The action-based architecture provides reusable, version-controlled workflow components following GitHub Actions patterns. Actions encapsulate common causal discovery operations with standardised inputs, outputs, and error handling.</p>"},{"location":"design/action_architecture_design/#core-action-framework","title":"Core Action Framework","text":""},{"location":"design/action_architecture_design/#base-action-interface","title":"Base Action Interface","text":"<pre><code>from abc import ABC, abstractmethod\nfrom typing import Dict, Any, Optional\nfrom dataclasses import dataclass\nimport semantic_version\n\n@dataclass\nclass ActionInput:\n    \"\"\"Define action input specification.\"\"\"\n    name: str\n    description: str\n    required: bool = False\n    default: Any = None\n    type_hint: str = \"Any\"\n\n@dataclass\nclass ActionOutput:\n    \"\"\"Define action output specification.\"\"\"\n    name: str\n    description: str\n    value: Any\n\nclass Action(ABC):\n    \"\"\"Base class for all workflow actions.\"\"\"\n\n    # Action metadata\n    name: str = \"\"\n    version: str = \"1.0.0\"\n    description: str = \"\"\n    author: str = \"\"\n\n    # Input/output specifications\n    inputs: Dict[str, ActionInput] = {}\n    outputs: Dict[str, str] = {}  # name -&gt; description mapping\n\n    @abstractmethod\n    def run(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Execute action with validated inputs, return outputs.\"\"\"\n        pass\n\n    def validate_inputs(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Validate and process input values.\"\"\"\n        validated = {}\n\n        for input_name, input_spec in self.inputs.items():\n            if input_spec.required and input_name not in inputs:\n                raise ValueError(f\"Required input '{input_name}' missing for action {self.name}\")\n\n            value = inputs.get(input_name, input_spec.default)\n            validated[input_name] = value\n\n        return validated\n\n    def format_outputs(self, raw_outputs: Dict[str, Any]) -&gt; Dict[str, ActionOutput]:\n        \"\"\"Format raw outputs with metadata.\"\"\"\n        formatted = {}\n\n        for name, value in raw_outputs.items():\n            description = self.outputs.get(name, f\"Output from {self.name}\")\n            formatted[name] = ActionOutput(\n                name=name,\n                description=description,\n                value=value\n            )\n\n        return formatted\n</code></pre>"},{"location":"design/action_architecture_design/#action-registry-and-versioning","title":"Action Registry and Versioning","text":"<pre><code>import importlib\nimport pkg_resources\nfrom typing import Dict, List, Optional\n\nclass ActionRegistry:\n    \"\"\"Manage registration and execution of workflow actions.\"\"\"\n\n    def __init__(self):\n        self._actions: Dict[str, Dict[str, Action]] = {}  # name -&gt; version -&gt; action\n        self._load_builtin_actions()\n        self._discover_plugin_actions()\n\n    def register_action(self, action: Action) -&gt; None:\n        \"\"\"Register action with version management.\"\"\"\n        name = action.name\n        version = action.version\n\n        if name not in self._actions:\n            self._actions[name] = {}\n\n        self._actions[name][version] = action\n\n        # Validate semantic versioning\n        try:\n            semantic_version.Version(version)\n        except ValueError as e:\n            raise ValueError(f\"Invalid semantic version '{version}' for action '{name}': {e}\")\n\n    def get_action(self, action_ref: str) -&gt; Action:\n        \"\"\"\n        Get action by reference: 'action-name@v1.2.3' or 'action-name@latest'\n\n        Examples:\n        - load-network@v1.0.0\n        - load-network@latest  \n        - causal-discovery@v2.1.0\n        \"\"\"\n        if \"@\" in action_ref:\n            name, version_spec = action_ref.split(\"@\", 1)\n        else:\n            name, version_spec = action_ref, \"latest\"\n\n        if name not in self._actions:\n            raise ValueError(f\"Unknown action: {name}\")\n\n        available_versions = self._actions[name]\n\n        if version_spec == \"latest\":\n            # Get highest semantic version\n            versions = [semantic_version.Version(v) for v in available_versions.keys()]\n            latest_version = str(max(versions))\n            return available_versions[latest_version]\n\n        if version_spec in available_versions:\n            return available_versions[version_spec]\n\n        # Try semantic version matching (e.g., ^1.0.0 matches 1.x.x)\n        spec = semantic_version.Spec(version_spec)\n        compatible_versions = []\n\n        for version_str in available_versions.keys():\n            version = semantic_version.Version(version_str)\n            if spec.match(version):\n                compatible_versions.append((version, version_str))\n\n        if compatible_versions:\n            # Get highest compatible version\n            latest_compatible = max(compatible_versions)[1]\n            return available_versions[latest_compatible]\n\n        raise ValueError(f\"No compatible version found for {action_ref}\")\n\n    def execute_action(self, action_ref: str, inputs: Dict[str, Any]) -&gt; Dict[str, ActionOutput]:\n        \"\"\"Execute action with input validation and output formatting.\"\"\"\n        action = self.get_action(action_ref)\n\n        # Validate inputs\n        validated_inputs = action.validate_inputs(inputs)\n\n        try:\n            # Execute action\n            raw_outputs = action.run(validated_inputs)\n\n            # Format outputs\n            return action.format_outputs(raw_outputs)\n\n        except Exception as e:\n            raise RuntimeError(f\"Action {action_ref} failed: {str(e)}\") from e\n</code></pre>"},{"location":"design/action_architecture_design/#core-causal-discovery-actions","title":"Core Causal Discovery Actions","text":""},{"location":"design/action_architecture_design/#data-loading-actions","title":"Data Loading Actions","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\nclass LoadNetworkAction(Action):\n    \"\"\"Load causal network dataset from various sources.\"\"\"\n\n    name = \"load-network\"\n    version = \"1.0.0\"\n    description = \"Load causal network dataset with optional transformations\"\n\n    inputs = {\n        \"network_name\": ActionInput(\"network_name\", \"Name of the network to load\", required=True),\n        \"source\": ActionInput(\"source\", \"Data source\", default=\"zenodo\"),\n        \"sample_size\": ActionInput(\"sample_size\", \"Number of samples to generate\", type_hint=\"int\"),\n        \"random_seed\": ActionInput(\"random_seed\", \"Random seed for reproducibility\", default=42),\n        \"add_noise\": ActionInput(\"add_noise\", \"Add Gaussian noise to continuous variables\", default=False),\n        \"noise_level\": ActionInput(\"noise_level\", \"Standard deviation of noise\", default=0.1)\n    }\n\n    outputs = {\n        \"dataset\": \"Pandas DataFrame with network data\",\n        \"true_graph\": \"NetworkX DiGraph representing true causal structure\",\n        \"metadata\": \"Dataset metadata including source, transformations applied\"\n    }\n\n    def run(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Load network data with optional transformations.\"\"\"\n        network_name = inputs[\"network_name\"]\n        source = inputs[\"source\"]\n        sample_size = inputs.get(\"sample_size\")\n        random_seed = inputs[\"random_seed\"]\n\n        # Set random seed for reproducibility\n        np.random.seed(random_seed)\n\n        # Load network from source\n        if source == \"zenodo\":\n            dataset, true_graph = self._load_from_zenodo(network_name)\n        elif source == \"bnlearn\":\n            dataset, true_graph = self._load_from_bnlearn(network_name)\n        elif source == \"local\":\n            dataset, true_graph = self._load_from_local(network_name)\n        else:\n            raise ValueError(f\"Unsupported data source: {source}\")\n\n        # Apply transformations\n        if sample_size and sample_size != len(dataset):\n            dataset = self._resample_dataset(dataset, sample_size)\n\n        if inputs.get(\"add_noise\", False):\n            dataset = self._add_noise(dataset, inputs[\"noise_level\"])\n\n        metadata = {\n            \"network_name\": network_name,\n            \"source\": source,\n            \"original_size\": len(dataset),\n            \"sample_size\": sample_size or len(dataset),\n            \"noise_added\": inputs.get(\"add_noise\", False),\n            \"random_seed\": random_seed\n        }\n\n        return {\n            \"dataset\": dataset,\n            \"true_graph\": true_graph,\n            \"metadata\": metadata\n        }\n\n    def _load_from_zenodo(self, network_name: str) -&gt; tuple:\n        \"\"\"Load network from Zenodo repository.\"\"\"\n        # Implementation would use zenodo API or cached files\n        # For now, placeholder that loads from local cache\n        return self._load_from_local(network_name)\n\n    def _load_from_bnlearn(self, network_name: str) -&gt; tuple:\n        \"\"\"Load network using R bnlearn package.\"\"\"\n        try:\n            import rpy2.robjects as ro\n            from rpy2.robjects import pandas2ri\n            pandas2ri.activate()\n\n            # Load bnlearn\n            ro.r('library(bnlearn)')\n\n            # Load network structure and data\n            ro.r(f'data({network_name})')\n            data_r = ro.r(network_name)\n            dataset = pandas2ri.rpy2py(data_r)\n\n            # Load true graph if available\n            try:\n                ro.r(f'net &lt;- {network_name}.net')\n                # Convert bnlearn network to NetworkX\n                true_graph = self._convert_bnlearn_to_networkx(ro.r('net'))\n            except:\n                true_graph = None\n\n            return dataset, true_graph\n\n        except ImportError:\n            raise RuntimeError(\"rpy2 not available - cannot load from bnlearn\")\n\n    def _load_from_local(self, network_name: str) -&gt; tuple:\n        \"\"\"Load network from local files.\"\"\"\n        # Implementation would load from data directory\n        # Placeholder for now\n        raise NotImplementedError(f\"Local loading for {network_name} not implemented\")\n\nclass RandomiseDataAction(Action):\n    \"\"\"Apply randomisation strategies to dataset.\"\"\"\n\n    name = \"randomise-data\"\n    version = \"1.0.0\"\n    description = \"Apply various randomisation strategies to causal data\"\n\n    inputs = {\n        \"dataset\": ActionInput(\"dataset\", \"Input dataset\", required=True),\n        \"strategy\": ActionInput(\"strategy\", \"Randomisation strategy\", required=True),\n        \"random_seed\": ActionInput(\"random_seed\", \"Random seed\", default=42)\n    }\n\n    outputs = {\n        \"randomised_dataset\": \"Dataset with randomisation applied\",\n        \"transformation_log\": \"Log of transformations applied\"\n    }\n\n    def run(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Apply randomisation strategy.\"\"\"\n        dataset = inputs[\"dataset\"]\n        strategy = inputs[\"strategy\"]\n        np.random.seed(inputs[\"random_seed\"])\n\n        transformation_log = []\n\n        if strategy == \"row_shuffle\":\n            randomised = dataset.sample(frac=1.0).reset_index(drop=True)\n            transformation_log.append(\"Shuffled row order\")\n\n        elif strategy == \"column_shuffle\":\n            columns = dataset.columns.tolist()\n            np.random.shuffle(columns)\n            randomised = dataset[columns]\n            transformation_log.append(f\"Shuffled column order: {' -&gt; '.join(columns)}\")\n\n        elif strategy == \"subsample\":\n            subsample_size = min(len(dataset) // 2, 1000)\n            randomised = dataset.sample(n=subsample_size).reset_index(drop=True)\n            transformation_log.append(f\"Subsampled to {subsample_size} rows\")\n\n        elif strategy == \"bootstrap\":\n            randomised = dataset.sample(n=len(dataset), replace=True).reset_index(drop=True)\n            transformation_log.append(\"Bootstrap resampling applied\")\n\n        else:\n            raise ValueError(f\"Unknown randomisation strategy: {strategy}\")\n\n        return {\n            \"randomised_dataset\": randomised,\n            \"transformation_log\": transformation_log\n        }\n</code></pre>"},{"location":"design/action_architecture_design/#algorithm-execution-actions","title":"Algorithm Execution Actions","text":"<pre><code>import networkx as nx\n\nclass CausalDiscoveryAction(Action):\n    \"\"\"Execute causal discovery algorithm from various packages.\"\"\"\n\n    name = \"causal-discovery\"\n    version = \"1.0.0\"\n    description = \"Run causal discovery algorithm with automatic package detection\"\n\n    inputs = {\n        \"algorithm\": ActionInput(\"algorithm\", \"Algorithm name (pc, ges, lingam, etc.)\", required=True),\n        \"package\": ActionInput(\"package\", \"Algorithm package (bnlearn, tetrad, causal-learn, auto)\", default=\"auto\"),\n        \"data\": ActionInput(\"data\", \"Input dataset\", required=True),\n        \"parameters\": ActionInput(\"parameters\", \"Algorithm-specific parameters\", default={})\n    }\n\n    outputs = {\n        \"learned_graph\": \"Learned causal graph as NetworkX DiGraph\",\n        \"algorithm_info\": \"Information about algorithm execution\",\n        \"performance_metrics\": \"Execution time, memory usage, convergence info\"\n    }\n\n    def run(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Execute causal discovery algorithm.\"\"\"\n        algorithm = inputs[\"algorithm\"].lower()\n        package = inputs[\"package\"]\n        data = inputs[\"data\"]\n        parameters = inputs[\"parameters\"]\n\n        # Auto-detect package if needed\n        if package == \"auto\":\n            package = self._detect_best_package(algorithm)\n\n        # Execute algorithm\n        start_time = time.time()\n\n        if package == \"bnlearn\":\n            learned_graph, algo_info = self._execute_bnlearn(algorithm, data, parameters)\n        elif package == \"tetrad\":\n            learned_graph, algo_info = self._execute_tetrad(algorithm, data, parameters)\n        elif package == \"causal-learn\":\n            learned_graph, algo_info = self._execute_causal_learn(algorithm, data, parameters)\n        else:\n            raise ValueError(f\"Unsupported package: {package}\")\n\n        execution_time = time.time() - start_time\n\n        performance_metrics = {\n            \"execution_time_seconds\": execution_time,\n            \"algorithm\": algorithm,\n            \"package\": package,\n            \"num_variables\": len(data.columns),\n            \"num_samples\": len(data),\n            \"num_edges\": learned_graph.number_of_edges()\n        }\n\n        return {\n            \"learned_graph\": learned_graph,\n            \"algorithm_info\": algo_info,\n            \"performance_metrics\": performance_metrics\n        }\n\n    def _detect_best_package(self, algorithm: str) -&gt; str:\n        \"\"\"Detect best available package for algorithm.\"\"\"\n        algorithm_packages = {\n            \"pc\": [\"bnlearn\", \"causal-learn\", \"tetrad\"],\n            \"ges\": [\"causal-learn\", \"tetrad\"], \n            \"lingam\": [\"causal-learn\"],\n            \"iamb\": [\"bnlearn\"],\n            \"gs\": [\"bnlearn\"]\n        }\n\n        preferred_packages = algorithm_packages.get(algorithm, [\"causal-learn\"])\n\n        # Check availability and return first available\n        for package in preferred_packages:\n            if self._is_package_available(package):\n                return package\n\n        raise RuntimeError(f\"No available package found for algorithm: {algorithm}\")\n\n    def _execute_bnlearn(self, algorithm: str, data: pd.DataFrame, \n                        parameters: Dict) -&gt; tuple:\n        \"\"\"Execute algorithm using R bnlearn.\"\"\"\n        try:\n            import rpy2.robjects as ro\n            from rpy2.robjects import pandas2ri\n            pandas2ri.activate()\n\n            # Transfer data to R\n            ro.r('library(bnlearn)')\n            ro.globalenv['data'] = data\n\n            # Build parameter string\n            param_str = self._build_bnlearn_params(parameters)\n\n            # Execute algorithm\n            cmd = f\"learned_net &lt;- {algorithm}(data{param_str})\"\n            ro.r(cmd)\n\n            # Convert result to NetworkX\n            learned_graph = self._convert_bnlearn_to_networkx(ro.r('learned_net'))\n\n            algo_info = {\n                \"package\": \"bnlearn\",\n                \"algorithm\": algorithm,\n                \"parameters\": parameters,\n                \"command\": cmd\n            }\n\n            return learned_graph, algo_info\n\n        except Exception as e:\n            raise RuntimeError(f\"bnlearn execution failed: {str(e)}\") from e\n</code></pre>"},{"location":"design/action_architecture_design/#evaluation-and-comparison-actions","title":"Evaluation and Comparison Actions","text":"<pre><code>class EvaluateGraphAction(Action):\n    \"\"\"Evaluate learned graph against true graph.\"\"\"\n\n    name = \"evaluate-graph\"\n    version = \"1.0.0\"\n    description = \"Compute accuracy metrics for learned causal graph\"\n\n    inputs = {\n        \"learned_graph\": ActionInput(\"learned_graph\", \"Learned causal graph\", required=True),\n        \"true_graph\": ActionInput(\"true_graph\", \"True causal graph\", required=True),\n        \"metrics\": ActionInput(\"metrics\", \"Metrics to compute\", default=[\"shd\", \"precision\", \"recall\", \"f1\"])\n    }\n\n    outputs = {\n        \"metrics\": \"Dictionary of computed metrics\",\n        \"confusion_matrix\": \"Edge-level confusion matrix\",\n        \"edge_analysis\": \"Detailed edge-by-edge comparison\"\n    }\n\n    def run(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Compute evaluation metrics.\"\"\"\n        learned = inputs[\"learned_graph\"]\n        true = inputs[\"true_graph\"]\n        metric_names = inputs[\"metrics\"]\n\n        # Compute confusion matrix at edge level\n        confusion = self._compute_edge_confusion_matrix(learned, true)\n\n        # Compute requested metrics\n        metrics = {}\n\n        if \"shd\" in metric_names:\n            metrics[\"shd\"] = self._compute_shd(learned, true)\n\n        if \"precision\" in metric_names:\n            metrics[\"precision\"] = confusion[\"tp\"] / (confusion[\"tp\"] + confusion[\"fp\"]) if confusion[\"tp\"] + confusion[\"fp\"] &gt; 0 else 0.0\n\n        if \"recall\" in metric_names:\n            metrics[\"recall\"] = confusion[\"tp\"] / (confusion[\"tp\"] + confusion[\"fn\"]) if confusion[\"tp\"] + confusion[\"fn\"] &gt; 0 else 0.0\n\n        if \"f1\" in metric_names:\n            p, r = metrics.get(\"precision\", 0), metrics.get(\"recall\", 0)\n            metrics[\"f1\"] = 2 * p * r / (p + r) if p + r &gt; 0 else 0.0\n\n        # Detailed edge analysis\n        edge_analysis = self._analyse_edge_differences(learned, true)\n\n        return {\n            \"metrics\": metrics,\n            \"confusion_matrix\": confusion,\n            \"edge_analysis\": edge_analysis\n        }\n</code></pre> <p>This action-based architecture provides a flexible, version-controlled foundation for building complex causal discovery workflows with reusable components and standardised interfaces.</p>"},{"location":"design/algorithm_registry_design/","title":"Algorithm Registry Design","text":""},{"location":"design/algorithm_registry_design/#overview","title":"Overview","text":"<p>The algorithm registry implements a package-level plugin architecture rather than individual algorithm plugins. This approach provides better maintainability, cross-language bridge management, and automatic algorithm discovery from major causal discovery packages.</p>"},{"location":"design/algorithm_registry_design/#package-level-plugin-architecture","title":"Package-Level Plugin Architecture","text":""},{"location":"design/algorithm_registry_design/#core-design-principle","title":"Core Design Principle","text":"<p>Instead of creating individual plugins for each algorithm (PC, GES, LINGAM, etc.), we create plugins for algorithm packages (bnlearn, Tetrad, causal-learn). This design provides several advantages:</p> <ul> <li>Simplified maintenance: One plugin per package instead of dozens per algorithm</li> <li>Automatic algorithm discovery: Packages expose their available algorithms dynamically</li> <li>Better resource management: Package-level initialisation and cleanup</li> <li>Cross-language bridge efficiency: Shared R/Java connections across algorithms</li> </ul>"},{"location":"design/algorithm_registry_design/#algorithm-registry-architecture","title":"Algorithm Registry Architecture","text":"<pre><code>from abc import ABC, abstractmethod\nfrom typing import Dict, List, Any, Optional\nimport pandas as pd\nimport networkx as nx\n\nclass AlgorithmPackage(ABC):\n    \"\"\"Base class for algorithm package plugins.\"\"\"\n\n    @property\n    @abstractmethod\n    def package_name(self) -&gt; str:\n        \"\"\"Return package name (e.g., 'bnlearn', 'tetrad', 'causal-learn').\"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def supported_algorithms(self) -&gt; List[str]:\n        \"\"\"Return list of supported algorithm names.\"\"\"\n        pass\n\n    @abstractmethod\n    def is_available(self) -&gt; bool:\n        \"\"\"Check if package is available and properly configured.\"\"\"\n        pass\n\n    @abstractmethod\n    def execute_algorithm(self, algorithm: str, data: pd.DataFrame, \n                         parameters: Dict[str, Any]) -&gt; nx.DiGraph:\n        \"\"\"Execute specified algorithm with given data and parameters.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_algorithm_info(self, algorithm: str) -&gt; Dict[str, Any]:\n        \"\"\"Get metadata about specific algorithm (parameters, constraints, etc.).\"\"\"\n        pass\n\n    def validate_data(self, data: pd.DataFrame, algorithm: str) -&gt; bool:\n        \"\"\"Validate that data is compatible with algorithm.\"\"\"\n        return True  # Default: accept all data\n\n    def preprocess_data(self, data: pd.DataFrame, algorithm: str) -&gt; pd.DataFrame:\n        \"\"\"Package-specific data preprocessing.\"\"\"\n        return data  # Default: no preprocessing\n\nclass AlgorithmRegistry:\n    \"\"\"Central registry for algorithm packages.\"\"\"\n\n    def __init__(self):\n        self._packages: Dict[str, AlgorithmPackage] = {}\n        self._algorithm_map: Dict[str, str] = {}  # algorithm -&gt; package_name\n        self._discover_packages()\n\n    def register_package(self, package: AlgorithmPackage) -&gt; None:\n        \"\"\"Register an algorithm package.\"\"\"\n        package_name = package.package_name\n\n        if not package.is_available():\n            raise RuntimeError(f\"Package {package_name} is not available\")\n\n        self._packages[package_name] = package\n\n        # Update algorithm mapping\n        for algorithm in package.supported_algorithms:\n            if algorithm in self._algorithm_map:\n                # Handle conflicts - prefer certain packages\n                existing_package = self._algorithm_map[algorithm]\n                preferred = self._resolve_package_preference(algorithm, existing_package, package_name)\n                self._algorithm_map[algorithm] = preferred\n            else:\n                self._algorithm_map[algorithm] = package_name\n\n    def execute_algorithm(self, algorithm: str, data: pd.DataFrame, \n                         parameters: Dict[str, Any] = None, \n                         preferred_package: str = None) -&gt; AlgorithmResult:\n        \"\"\"Execute algorithm using best available package.\"\"\"\n        parameters = parameters or {}\n\n        # Determine which package to use\n        if preferred_package and preferred_package in self._packages:\n            package_name = preferred_package\n        elif algorithm in self._algorithm_map:\n            package_name = self._algorithm_map[algorithm]\n        else:\n            raise ValueError(f\"Algorithm '{algorithm}' not available in any registered package\")\n\n        package = self._packages[package_name]\n\n        # Validate data compatibility\n        if not package.validate_data(data, algorithm):\n            raise ValueError(f\"Data incompatible with {algorithm} in {package_name}\")\n\n        # Preprocess data\n        processed_data = package.preprocess_data(data, algorithm)\n\n        # Execute algorithm\n        start_time = time.time()\n        try:\n            learned_graph = package.execute_algorithm(algorithm, processed_data, parameters)\n            execution_time = time.time() - start_time\n\n            return AlgorithmResult(\n                algorithm=algorithm,\n                package=package_name,\n                learned_graph=learned_graph,\n                execution_time=execution_time,\n                parameters=parameters,\n                data_info={\n                    \"num_variables\": len(data.columns),\n                    \"num_samples\": len(data),\n                    \"variable_names\": list(data.columns)\n                }\n            )\n\n        except Exception as e:\n            raise RuntimeError(f\"Algorithm {algorithm} failed in {package_name}: {str(e)}\") from e\n\n    def get_available_algorithms(self) -&gt; Dict[str, List[str]]:\n        \"\"\"Get all available algorithms grouped by package.\"\"\"\n        result = {}\n        for package_name, package in self._packages.items():\n            result[package_name] = package.supported_algorithms\n        return result\n\n    def _discover_packages(self) -&gt; None:\n        \"\"\"Automatically discover and register available packages.\"\"\"\n        # Try to register each known package\n        package_classes = [\n            BnlearnPackage,\n            TetradPackage,\n            CausalLearnPackage,\n            PcalgorithmPackage  # R pcalgorithm package\n        ]\n\n        for package_class in package_classes:\n            try:\n                package = package_class()\n                if package.is_available():\n                    self.register_package(package)\n            except Exception:\n                # Package not available or failed to initialise\n                continue\n</code></pre>"},{"location":"design/algorithm_registry_design/#specific-package-implementations","title":"Specific Package Implementations","text":""},{"location":"design/algorithm_registry_design/#r-bnlearn-package","title":"R bnlearn Package","text":"<pre><code>import warnings\nfrom typing import Dict, List, Any\nimport pandas as pd\nimport networkx as nx\n\nclass BnlearnPackage(AlgorithmPackage):\n    \"\"\"R bnlearn package integration via rpy2.\"\"\"\n\n    def __init__(self):\n        self._r_session = None\n        self._initialise_r()\n\n    @property\n    def package_name(self) -&gt; str:\n        return \"bnlearn\"\n\n    @property\n    def supported_algorithms(self) -&gt; List[str]:\n        return [\n            \"pc\", \"iamb\", \"fast.iamb\", \"inter.iamb\", \n            \"gs\", \"mmpc\", \"si.hiton.pc\", \"hpc\",\n            \"chow.liu\", \"aracne\", \"hc\", \"tabu\"\n        ]\n\n    def is_available(self) -&gt; bool:\n        \"\"\"Check R and bnlearn availability.\"\"\"\n        try:\n            import rpy2.robjects as ro\n            ro.r('library(bnlearn)')\n            return True\n        except (ImportError, Exception):\n            return False\n\n    def execute_algorithm(self, algorithm: str, data: pd.DataFrame, \n                         parameters: Dict[str, Any]) -&gt; nx.DiGraph:\n        \"\"\"Execute bnlearn algorithm.\"\"\"\n        import rpy2.robjects as ro\n        from rpy2.robjects import pandas2ri\n        pandas2ri.activate()\n\n        # Transfer data to R\n        ro.globalenv['causal_data'] = data\n\n        # Build parameter string for R function call\n        param_str = self._build_r_parameters(parameters)\n\n        # Execute algorithm\n        if algorithm in [\"pc\", \"iamb\", \"fast.iamb\", \"inter.iamb\", \"gs\", \"mmpc\"]:\n            # Constraint-based algorithms\n            ro.r(f\"learned_net &lt;- {algorithm}(causal_data{param_str})\")\n        elif algorithm in [\"hc\", \"tabu\"]:\n            # Score-based algorithms\n            ro.r(f\"learned_net &lt;- {algorithm}(causal_data{param_str})\")\n        else:\n            raise ValueError(f\"Unknown bnlearn algorithm: {algorithm}\")\n\n        # Convert bnlearn network to NetworkX\n        return self._convert_bnlearn_to_networkx(ro.r('learned_net'))\n\n    def get_algorithm_info(self, algorithm: str) -&gt; Dict[str, Any]:\n        \"\"\"Get bnlearn algorithm information.\"\"\"\n        algorithm_info = {\n            \"pc\": {\n                \"type\": \"constraint-based\",\n                \"parameters\": [\"alpha\", \"test\"],\n                \"data_types\": [\"mixed\", \"continuous\", \"discrete\"],\n                \"default_alpha\": 0.05\n            },\n            \"ges\": {\n                \"type\": \"score-based\", \n                \"parameters\": [\"score\", \"lambda\"],\n                \"data_types\": [\"continuous\"],\n                \"default_score\": \"bic-g\"\n            },\n            \"hc\": {\n                \"type\": \"score-based\",\n                \"parameters\": [\"score\", \"restart\", \"perturb\"],\n                \"data_types\": [\"mixed\", \"continuous\", \"discrete\"],\n                \"default_score\": \"bic\"\n            }\n        }\n\n        return algorithm_info.get(algorithm, {\"type\": \"unknown\"})\n\n    def validate_data(self, data: pd.DataFrame, algorithm: str) -&gt; bool:\n        \"\"\"Validate data for bnlearn algorithms.\"\"\"\n        # Check for missing values\n        if data.isnull().any().any():\n            return False\n\n        # Algorithm-specific validation\n        if algorithm == \"ges\":\n            # GES requires continuous data\n            return all(pd.api.types.is_numeric_dtype(data[col]) for col in data.columns)\n\n        return True\n\n    def preprocess_data(self, data: pd.DataFrame, algorithm: str) -&gt; pd.DataFrame:\n        \"\"\"Preprocess data for bnlearn.\"\"\"\n        processed = data.copy()\n\n        # Convert categorical strings to factors for R\n        for col in processed.columns:\n            if processed[col].dtype == 'object':\n                processed[col] = processed[col].astype('category')\n\n        return processed\n\n    def _initialise_r(self) -&gt; None:\n        \"\"\"Initialise R session.\"\"\"\n        try:\n            import rpy2.robjects as ro\n            ro.r('library(bnlearn)')\n            # Set up any global R settings\n            ro.r('options(warn=-1)')  # Suppress R warnings\n        except Exception as e:\n            raise RuntimeError(f\"Failed to initialise R session: {e}\")\n\n    def _build_r_parameters(self, parameters: Dict[str, Any]) -&gt; str:\n        \"\"\"Build R function parameter string.\"\"\"\n        if not parameters:\n            return \"\"\n\n        param_parts = []\n        for key, value in parameters.items():\n            if isinstance(value, str):\n                param_parts.append(f'{key}=\"{value}\"')\n            else:\n                param_parts.append(f'{key}={value}')\n\n        return \", \" + \", \".join(param_parts)\n\n    def _convert_bnlearn_to_networkx(self, bnlearn_net) -&gt; nx.DiGraph:\n        \"\"\"Convert bnlearn network to NetworkX DiGraph.\"\"\"\n        import rpy2.robjects as ro\n\n        # Get arc matrix from bnlearn\n        ro.r('arc_matrix &lt;- arcs(learned_net)')\n        arc_matrix = ro.r('arc_matrix')\n\n        # Convert to NetworkX\n        graph = nx.DiGraph()\n\n        # Add nodes (get node names from bnlearn network)\n        ro.r('node_names &lt;- names(learned_net$nodes)')\n        nodes = list(ro.r('node_names'))\n        graph.add_nodes_from(nodes)\n\n        # Add edges from arc matrix\n        if arc_matrix is not None:\n            for i in range(arc_matrix.nrow):\n                from_node = str(arc_matrix.rx(i+1, 1)[0])\n                to_node = str(arc_matrix.rx(i+1, 2)[0])\n                graph.add_edge(from_node, to_node)\n\n        return graph\n</code></pre>"},{"location":"design/algorithm_registry_design/#python-causal-learn-package","title":"Python causal-learn Package","text":"<pre><code>class CausalLearnPackage(AlgorithmPackage):\n    \"\"\"Python causal-learn package direct integration.\"\"\"\n\n    @property\n    def package_name(self) -&gt; str:\n        return \"causal-learn\"\n\n    @property  \n    def supported_algorithms(self) -&gt; List[str]:\n        return [\n            \"pc\", \"ges\", \"lingam\", \"direct-lingam\", \"ica-lingam\",\n            \"fci\", \"rfci\", \"gfci\", \"cdnod\", \"gds\"\n        ]\n\n    def is_available(self) -&gt; bool:\n        \"\"\"Check causal-learn availability.\"\"\"\n        try:\n            import causallearn\n            return True\n        except ImportError:\n            return False\n\n    def execute_algorithm(self, algorithm: str, data: pd.DataFrame, \n                         parameters: Dict[str, Any]) -&gt; nx.DiGraph:\n        \"\"\"Execute causal-learn algorithm.\"\"\"\n        import numpy as np\n\n        # Convert to numpy for causal-learn\n        data_array = data.values\n\n        if algorithm == \"pc\":\n            from causallearn.search.ConstraintBased.PC import pc\n            alpha = parameters.get(\"alpha\", 0.05)\n            cg = pc(data_array, alpha=alpha)\n            return self._convert_causallearn_to_networkx(cg.G, data.columns)\n\n        elif algorithm == \"ges\":\n            from causallearn.search.ScoreBased.GES import ges\n            score_func = parameters.get(\"score_func\", \"local_score_BIC\")\n            record = ges(data_array, score_func=score_func)\n            return self._convert_causallearn_to_networkx(record['G'], data.columns)\n\n        elif algorithm == \"lingam\":\n            from causallearn.search.FCMBased import lingam\n            model = lingam.DirectLiNGAM()\n            model.fit(data_array)\n            return self._convert_adjacency_to_networkx(model.adjacency_matrix_, data.columns)\n\n        else:\n            raise ValueError(f\"Algorithm {algorithm} not implemented in causal-learn package\")\n\n    def validate_data(self, data: pd.DataFrame, algorithm: str) -&gt; bool:\n        \"\"\"Validate data for causal-learn algorithms.\"\"\"\n        # Check for missing values\n        if data.isnull().any().any():\n            return False\n\n        # Most causal-learn algorithms require numeric data\n        if not all(pd.api.types.is_numeric_dtype(data[col]) for col in data.columns):\n            return False\n\n        return True\n\n    def _convert_causallearn_to_networkx(self, causal_matrix, variable_names) -&gt; nx.DiGraph:\n        \"\"\"Convert causal-learn result to NetworkX.\"\"\"\n        graph = nx.DiGraph()\n        graph.add_nodes_from(variable_names)\n\n        # causal-learn uses different matrix formats\n        for i in range(len(variable_names)):\n            for j in range(len(variable_names)):\n                if causal_matrix[i, j] == 1:  # Directed edge\n                    graph.add_edge(variable_names[i], variable_names[j])\n\n        return graph\n</code></pre>"},{"location":"design/algorithm_registry_design/#java-tetrad-package","title":"Java Tetrad Package","text":"<pre><code>class TetradPackage(AlgorithmPackage):\n    \"\"\"Java Tetrad package integration via py4j.\"\"\"\n\n    def __init__(self):\n        self._gateway = None\n        self._initialise_java()\n\n    @property\n    def package_name(self) -&gt; str:\n        return \"tetrad\"\n\n    @property\n    def supported_algorithms(self) -&gt; List[str]:\n        return [\n            \"pc\", \"cpc\", \"ges\", \"fges\", \"gfci\", \n            \"fas\", \"rfci\", \"cfci\", \"svarfci\"\n        ]\n\n    def is_available(self) -&gt; bool:\n        \"\"\"Check Java and Tetrad availability.\"\"\"\n        try:\n            import subprocess\n            result = subprocess.run(['java', '-version'], \n                                  capture_output=True, text=True)\n            return result.returncode == 0\n        except FileNotFoundError:\n            return False\n\n    def execute_algorithm(self, algorithm: str, data: pd.DataFrame, \n                         parameters: Dict[str, Any]) -&gt; nx.DiGraph:\n        \"\"\"Execute Tetrad algorithm via Java bridge.\"\"\"\n        if self._gateway is None:\n            raise RuntimeError(\"Java gateway not initialised\")\n\n        # Convert pandas to Java data structure\n        tetrad_data = self._convert_to_tetrad_data(data)\n\n        # Execute algorithm\n        if algorithm == \"pc\":\n            alpha = parameters.get(\"alpha\", 0.05)\n            search = self._gateway.jvm.edu.cmu.tetrad.search.Pc(tetrad_data)\n            search.setAlpha(alpha)\n            result_graph = search.search()\n\n        elif algorithm == \"ges\":\n            score = self._gateway.jvm.edu.cmu.tetrad.search.score.BicScore(tetrad_data)\n            search = self._gateway.jvm.edu.cmu.tetrad.search.Ges(score)\n            result_graph = search.search()\n\n        else:\n            raise ValueError(f\"Algorithm {algorithm} not implemented in Tetrad package\")\n\n        # Convert result back to NetworkX\n        return self._convert_tetrad_to_networkx(result_graph, data.columns)\n\n    def _initialise_java(self) -&gt; None:\n        \"\"\"Initialise Java gateway for Tetrad.\"\"\"\n        try:\n            from py4j.java_gateway import JavaGateway\n            self._gateway = JavaGateway()\n            # Test connection\n            self._gateway.jvm.System.currentTimeMillis()\n        except Exception as e:\n            raise RuntimeError(f\"Failed to initialise Java gateway: {e}\")\n</code></pre>"},{"location":"design/algorithm_registry_design/#algorithm-resolution-strategy","title":"Algorithm Resolution Strategy","text":""},{"location":"design/algorithm_registry_design/#package-preference-order","title":"Package Preference Order","text":"<p>When multiple packages support the same algorithm, the registry uses preference rules:</p> <pre><code>def _resolve_package_preference(self, algorithm: str, existing_package: str, new_package: str) -&gt; str:\n    \"\"\"Resolve conflicts when multiple packages support same algorithm.\"\"\"\n\n    # Define preference order for common algorithms\n    preferences = {\n        \"pc\": [\"bnlearn\", \"causal-learn\", \"tetrad\"],  # Prefer bnlearn for PC\n        \"ges\": [\"causal-learn\", \"tetrad\", \"bnlearn\"], # Prefer causal-learn for GES\n        \"lingam\": [\"causal-learn\"],                   # Only causal-learn supports LiNGAM\n        \"hc\": [\"bnlearn\"],                            # Only bnlearn supports HC\n    }\n\n    if algorithm in preferences:\n        preference_order = preferences[algorithm]\n\n        existing_idx = preference_order.index(existing_package) if existing_package in preference_order else 999\n        new_idx = preference_order.index(new_package) if new_package in preference_order else 999\n\n        return existing_package if existing_idx &lt; new_idx else new_package\n\n    # Default: keep existing\n    return existing_package\n</code></pre> <p>This package-level approach provides a clean, maintainable foundation for supporting the diverse ecosystem of causal discovery algorithms while handling cross-language integration complexities efficiently.</p>"},{"location":"design/matrix_expansion_design/","title":"CI Workflow Matrix Strategy Implementation","text":""},{"location":"design/matrix_expansion_design/#overview","title":"Overview","text":"<p>The matrix strategy implementation provides the GitHub Actions-style <code>strategy.matrix</code> functionality within the unified CI workflow engine. This is not a separate matrix expansion system, but rather a core component of the CI workflow architecture that handles the complex logic of experiment combination generation.</p>"},{"location":"design/matrix_expansion_design/#github-actions-matrix-strategy","title":"GitHub Actions Matrix Strategy","text":""},{"location":"design/matrix_expansion_design/#core-matrix-features","title":"Core Matrix Features","text":"<p>The implementation supports all GitHub Actions matrix strategy features:</p> <pre><code>strategy:\n  matrix:\n    algorithm: [\"PC\", \"GES\", \"LINGAM\"]\n    network: [\"asia\", \"sachs\"]\n    sample_size: [100, 500]\n\n  exclude:\n    - algorithm: \"LINGAM\"\n      network: \"alarm\"  # Remove specific combinations\n\n  include:\n    - algorithm: \"PC\"\n      network: \"asia\"\n      alpha: 0.01      # Add combinations with extra parameters\n\n  fail_fast: false      # Continue other jobs if one fails\n  max_parallel: 8       # Limit concurrent execution\n</code></pre>"},{"location":"design/matrix_expansion_design/#matrix-strategy-implementation","title":"Matrix Strategy Implementation","text":"<pre><code>import itertools\nfrom typing import Dict, List, Any\n\nclass CIMatrixStrategy:\n    \"\"\"Implement GitHub Actions matrix strategy within CI workflow engine.\"\"\"\n\n    def expand_matrix_strategy(self, strategy_config: Dict) -&gt; List[Dict]:\n        \"\"\"\n        Generate all job combinations from strategy.matrix configuration.\n\n        This is part of the CI workflow engine, not a separate expansion system.\n\n        Input:\n          strategy:\n            matrix:\n              algorithm: [\"PC\", \"GES\", \"LINGAM\"]\n              network: [\"asia\", \"sachs\"]\n              sample_size: [100, 500]\n\n        Output:\n          12 jobs with all combinations of algorithm \u00d7 network \u00d7 sample_size\n        \"\"\"\n        matrix_config = strategy_config.get(\"matrix\", {})\n        exclude_rules = strategy_config.get(\"exclude\", [])\n        include_rules = strategy_config.get(\"include\", [])\n\n        # Generate base cartesian product\n        base_jobs = self._generate_base_combinations(matrix_config)\n\n        # Apply exclude rules\n        filtered_jobs = self._apply_exclude_rules(base_jobs, exclude_rules)\n\n        # Apply include rules  \n        final_jobs = self._apply_include_rules(filtered_jobs, include_rules)\n\n        return final_jobs\n</code></pre>"},{"location":"design/matrix_expansion_design/#advanced-matrix-features","title":"Advanced Matrix Features","text":"<pre><code>def apply_exclude_rules(self, jobs: List[Dict], exclude_rules: List[Dict]) -&gt; List[Dict]:\n    \"\"\"\n    Remove jobs matching exclude patterns.\n\n    Example:\n      exclude:\n        - algorithm: \"LINGAM\"\n          network: \"alarm\"  # Remove LINGAM + alarm combination\n        - sample_size: 100\n          algorithm: \"GES\"   # Remove GES + 100 sample size combination\n    \"\"\"\n    filtered_jobs = []\n\n    for job in jobs:\n        should_exclude = False\n\n        for exclude_rule in exclude_rules:\n            if self._job_matches_pattern(job, exclude_rule):\n                should_exclude = True\n                break\n\n        if not should_exclude:\n            filtered_jobs.append(job)\n\n    return filtered_jobs\n\ndef apply_include_rules(self, jobs: List[Dict], include_rules: List[Dict]) -&gt; List[Dict]:\n    \"\"\"\n    Add specific job combinations even if not in matrix.\n\n    Example:\n      include:\n        - algorithm: \"PC\"\n          network: \"asia\"\n          sample_size: 50    # Add PC + asia + 50 even though 50 not in matrix\n          alpha: 0.01        # Add extra parameter for this combination\n    \"\"\"\n    extended_jobs = jobs.copy()\n\n    for include_rule in include_rules:\n        # Check if this combination already exists\n        if not any(self._job_matches_pattern(job, include_rule) for job in jobs):\n            extended_jobs.append(include_rule.copy())\n\n    return extended_jobs\n\ndef _job_matches_pattern(self, job: Dict, pattern: Dict) -&gt; bool:\n    \"\"\"Check if job matches the given pattern (all pattern keys must match).\"\"\"\n    for key, value in pattern.items():\n        if key not in job or job[key] != value:\n            return False\n    return True\n</code></pre>"},{"location":"design/matrix_expansion_design/#template-substitution-engine","title":"Template Substitution Engine","text":""},{"location":"design/matrix_expansion_design/#jinja2-integration-for-variable-substitution","title":"Jinja2 Integration for Variable Substitution","text":"<pre><code>import jinja2\nfrom typing import Dict, Any\n\nclass TemplateProcessor:\n    \"\"\"Handle GitHub Actions-style template substitution.\"\"\"\n\n    def __init__(self):\n        # Configure Jinja2 with GitHub Actions syntax\n        self.env = jinja2.Environment(\n            variable_start_string=\"${{\",\n            variable_end_string=\"}}\",\n            trim_blocks=True,\n            lstrip_blocks=True\n        )\n\n    def substitute_job_variables(self, template: str, job_context: Dict) -&gt; str:\n        \"\"\"\n        Process template with job-specific variables.\n\n        Example:\n          template: \"network_${{ matrix.network }}_${{ matrix.sample_size }}.csv\"\n          job_context: {\"matrix\": {\"network\": \"asia\", \"sample_size\": 100}}\n          result: \"network_asia_100.csv\"\n        \"\"\"\n        jinja_template = self.env.from_string(template)\n        return jinja_template.render(**job_context)\n\n    def build_job_context(self, job: Dict, workflow_env: Dict, \n                         step_outputs: Dict = None) -&gt; Dict:\n        \"\"\"\n        Build complete context for template substitution.\n\n        Available variables:\n        - matrix.*: Matrix variables for current job\n        - env.*: Environment variables\n        - steps.*: Outputs from previous steps\n        - github.*: GitHub Actions compatibility variables\n        \"\"\"\n        context = {\n            \"matrix\": job,\n            \"env\": workflow_env,\n            \"github\": {\n                \"workspace\": \"/tmp/causaliq-workspace\",  # Configurable\n                \"run_id\": \"12345\",  # Generated\n                \"run_number\": \"1\"\n            }\n        }\n\n        if step_outputs:\n            context[\"steps\"] = step_outputs\n\n        return context\n</code></pre>"},{"location":"design/matrix_expansion_design/#integration-with-dask-task-graphs","title":"Integration with DASK Task Graphs","text":""},{"location":"design/matrix_expansion_design/#job-dask-task-conversion","title":"Job \u2192 DASK Task Conversion","text":"<pre><code>import dask\nfrom typing import Dict, List, Callable\n\nclass DaskTaskGraphBuilder:\n    \"\"\"Convert matrix jobs into DASK computation graph.\"\"\"\n\n    def build_workflow_graph(self, jobs: List[Dict], \n                            workflow_definition: Dict) -&gt; Dict:\n        \"\"\"\n        Convert expanded matrix jobs into DASK task graph.\n\n        Each job becomes a series of DASK tasks corresponding to workflow steps.\n        Dependencies between steps are handled through DASK delayed mechanism.\n        \"\"\"\n        graph = {}\n\n        for job_idx, job in enumerate(jobs):\n            job_id = f\"job_{job_idx}\"\n            step_results = {}\n\n            # Process each step in the workflow\n            for step_idx, step in enumerate(workflow_definition[\"steps\"]):\n                step_id = f\"{job_id}_step_{step_idx}\"\n                step_name = step.get(\"name\", f\"step_{step_idx}\")\n\n                # Build step task with dependencies\n                step_task = self._build_step_task(\n                    step=step,\n                    job_context=job,\n                    previous_results=step_results,\n                    workflow_env=workflow_definition.get(\"env\", {})\n                )\n\n                graph[step_id] = step_task\n                step_results[step_name] = step_id\n\n        return graph\n\n    def _build_step_task(self, step: Dict, job_context: Dict, \n                        previous_results: Dict, workflow_env: Dict) -&gt; tuple:\n        \"\"\"\n        Build individual DASK task for workflow step.\n\n        Returns tuple: (function, *args) suitable for DASK graph\n        \"\"\"\n        action_name = step[\"uses\"]\n        action_inputs = step.get(\"with\", {})\n\n        # Substitute templates in action inputs\n        template_processor = TemplateProcessor()\n        full_context = template_processor.build_job_context(\n            job=job_context,\n            workflow_env=workflow_env,\n            step_outputs=previous_results\n        )\n\n        substituted_inputs = {}\n        for key, value in action_inputs.items():\n            if isinstance(value, str):\n                substituted_inputs[key] = template_processor.substitute_job_variables(\n                    value, full_context\n                )\n            else:\n                substituted_inputs[key] = value\n\n        # Return DASK task tuple\n        return (\n            self._execute_action,\n            action_name,\n            substituted_inputs,\n            job_context,\n            previous_results\n        )\n\n    def _execute_action(self, action_name: str, inputs: Dict, \n                       job_context: Dict, previous_results: Dict) -&gt; Any:\n        \"\"\"Execute workflow action with proper error handling.\"\"\"\n        from causaliq_workflow.actions import ActionRegistry\n\n        registry = ActionRegistry()\n        return registry.execute_action(action_name, inputs)\n</code></pre>"},{"location":"design/matrix_expansion_design/#performance-optimisations","title":"Performance Optimisations","text":""},{"location":"design/matrix_expansion_design/#intelligent-job-batching","title":"Intelligent Job Batching","text":"<pre><code>class MatrixOptimiser:\n    \"\"\"Optimise matrix job execution for performance.\"\"\"\n\n    def batch_similar_jobs(self, jobs: List[Dict], max_batch_size: int = 10) -&gt; List[List[Dict]]:\n        \"\"\"\n        Group jobs by similarity for efficient resource usage.\n\n        Jobs using same algorithm/package can share initialisation costs.\n        Jobs using same dataset can share data loading costs.\n        \"\"\"\n        batches = []\n\n        # Group by algorithm package for shared initialisation\n        algorithm_groups = {}\n        for job in jobs:\n            algorithm = job.get(\"algorithm\", \"unknown\")\n            if algorithm not in algorithm_groups:\n                algorithm_groups[algorithm] = []\n            algorithm_groups[algorithm].append(job)\n\n        # Further group by dataset within each algorithm\n        for algorithm, algo_jobs in algorithm_groups.items():\n            dataset_groups = {}\n            for job in algo_jobs:\n                dataset = job.get(\"network\", \"unknown\")\n                if dataset not in dataset_groups:\n                    dataset_groups[dataset] = []\n                dataset_groups[dataset].append(job)\n\n            # Create batches within each dataset group\n            for dataset, dataset_jobs in dataset_groups.items():\n                for i in range(0, len(dataset_jobs), max_batch_size):\n                    batch = dataset_jobs[i:i + max_batch_size]\n                    batches.append(batch)\n\n        return batches\n\n    def estimate_job_resources(self, job: Dict) -&gt; Dict[str, Any]:\n        \"\"\"\n        Estimate resource requirements for job.\n\n        Used for intelligent DASK worker allocation.\n        \"\"\"\n        algorithm = job.get(\"algorithm\", \"\")\n        sample_size = job.get(\"sample_size\", 100)\n        network_size = self._estimate_network_complexity(job.get(\"network\", \"\"))\n\n        # Simple heuristics - can be refined with benchmarking data\n        memory_mb = max(500, sample_size * network_size * 0.1)\n        cpu_time_minutes = max(1, (sample_size * network_size) / 10000)\n\n        return {\n            \"memory_mb\": memory_mb,\n            \"cpu_time_minutes\": cpu_time_minutes,\n            \"io_intensive\": algorithm.lower() in [\"ges\", \"gies\"],  # Score-based algorithms\n            \"cross_language\": self._requires_cross_language_bridge(algorithm)\n        }\n</code></pre>"},{"location":"design/matrix_expansion_design/#integration-with-ci-workflow-engine","title":"Integration with CI Workflow Engine","text":""},{"location":"design/matrix_expansion_design/#unified-architecture","title":"Unified Architecture","text":"<p>The matrix strategy implementation is a core component of the unified CI workflow engine, not a separate system:</p> <pre><code>class CIWorkflowEngine:\n    \"\"\"Unified CI workflow engine with integrated matrix strategy support.\"\"\"\n\n    def __init__(self):\n        self.matrix_strategy = CIMatrixStrategy()\n        self.template_processor = TemplateProcessor()\n        self.action_registry = ActionRegistry()\n        self.dask_builder = DaskTaskGraphBuilder()\n\n    def execute_workflow(self, workflow_yaml: str) -&gt; WorkflowResult:\n        \"\"\"Execute complete CI workflow with matrix strategy expansion.\"\"\"\n        workflow_def = self.parse_workflow(workflow_yaml)\n\n        # Expand matrix strategy if present\n        if \"strategy\" in workflow_def:\n            jobs = self.matrix_strategy.expand_matrix_strategy(workflow_def[\"strategy\"])\n        else:\n            jobs = [{}]  # Single job with empty context\n\n        # Build DASK task graph for all jobs\n        task_graph = self.dask_builder.build_workflow_graph(jobs, workflow_def)\n\n        # Execute with DASK\n        return self._execute_dask_graph(task_graph)\n</code></pre>"},{"location":"design/matrix_expansion_design/#key-design-principles","title":"Key Design Principles","text":"<ol> <li>Unified System: Matrix strategy is part of CI workflow engine, not separate component</li> <li>GitHub Actions Compatibility: Full compatibility with GitHub Actions matrix syntax</li> <li>DASK Integration: Matrix jobs convert directly to DASK task graph</li> <li>Template Support: Full Jinja2 template substitution with <code>${{ matrix.variable }}</code> syntax</li> <li>Resource Management: Matrix features like <code>max_parallel</code> map to DASK execution controls</li> </ol> <p>This implementation provides the sophisticated matrix capabilities of GitHub Actions while maintaining the unified architecture of the CI workflow system for causal discovery research.</p>"},{"location":"design/workflow_executor_design/","title":"WorkflowExecutor Implementation Design","text":""},{"location":"design/workflow_executor_design/#overview","title":"Overview","text":"<p>The <code>WorkflowExecutor</code> class provides the foundation for parsing and executing GitHub Actions-style YAML workflows with matrix expansion support. This implementation focuses on the parsing and preparation phase, establishing the infrastructure for workflow execution.</p>"},{"location":"design/workflow_executor_design/#implementation-architecture","title":"Implementation Architecture","text":""},{"location":"design/workflow_executor_design/#class-structure","title":"Class Structure","text":"<pre><code>class WorkflowExecutor:\n    \"\"\"Parse and execute GitHub Actions-style workflows with matrix expansion.\"\"\"\n\n    def parse_workflow(self, workflow_path: Union[str, Path]) -&gt; Dict[str, Any]\n    def expand_matrix(self, matrix: Dict[str, List[Any]]) -&gt; List[Dict[str, Any]]\n    def construct_paths(self, job: Dict[str, Any], data_root: str, \n                       output_root: str, workflow_id: str) -&gt; Dict[str, str]\n</code></pre>"},{"location":"design/workflow_executor_design/#key-design-decisions","title":"Key Design Decisions","text":""},{"location":"design/workflow_executor_design/#1-integration-with-existing-schema-validation","title":"1. Integration with Existing Schema Validation","text":"<p>The <code>WorkflowExecutor</code> leverages the existing <code>causaliq_workflow.schema</code> module for workflow validation:</p> <pre><code>def parse_workflow(self, workflow_path):\n    workflow = load_workflow_file(workflow_path)  # Existing function\n    validate_workflow(workflow)                   # Existing function\n    return workflow\n</code></pre> <p>Rationale: Reuse proven validation logic, maintain single source of truth for workflow structure validation.</p>"},{"location":"design/workflow_executor_design/#2-matrix-expansion-strategy","title":"2. Matrix Expansion Strategy","text":"<p>Matrix expansion uses cartesian product generation:</p> <pre><code>def expand_matrix(self, matrix):\n    variables = list(matrix.keys())\n    value_lists = list(matrix.values())\n    combinations = list(itertools.product(*value_lists))\n\n    jobs = []\n    for combination in combinations:\n        job = dict(zip(variables, combination))\n        jobs.append(job)\n    return jobs\n</code></pre> <p>Rationale:  - Simple, predictable algorithm matching GitHub Actions behaviour - Easy to understand and debug - Supports arbitrary matrix dimensions - Deterministic ordering for reproducible results</p>"},{"location":"design/workflow_executor_design/#3-path-construction-pattern","title":"3. Path Construction Pattern","text":"<p>Paths follow the established pattern from the examples:</p> <pre><code># Input: {data_root}/{dataset}/input.csv\n# Output: {output_root}/{workflow_id}/{dataset}_{algorithm}/\n</code></pre> <p>Rationale: - Consistent with documentation examples - Organises outputs by workflow and experiment parameters - Supports hierarchical result organisation - Compatible with existing action framework expectations</p>"},{"location":"design/workflow_executor_design/#4-error-handling-strategy","title":"4. Error Handling Strategy","text":"<p>All methods use consistent error propagation:</p> <pre><code>try:\n    # Core logic\nexcept Exception as e:\n    raise WorkflowExecutionError(f\"Operation failed: {e}\") from e\n</code></pre> <p>Rationale: - Maintains error chain for debugging - Provides consistent error interface - Follows established patterns in the codebase</p>"},{"location":"design/workflow_executor_design/#implementation-phases","title":"Implementation Phases","text":""},{"location":"design/workflow_executor_design/#phase-1-parsing-foundation-complete","title":"Phase 1: Parsing Foundation (Complete \u2705)","text":"<p>Scope: Basic workflow parsing and matrix expansion - Parse and validate YAML workflow files - Expand matrix variables into job configurations - Construct file paths from matrix variables - Comprehensive error handling</p> <p>Test Coverage: 100% with edge case coverage - Unit tests with mocked dependencies - Functional tests with real YAML files - Exception handling verification</p>"},{"location":"design/workflow_executor_design/#phase-2-execution-engine-future","title":"Phase 2: Execution Engine (Future)","text":"<p>Scope: Step execution and orchestration - Execute workflow steps with action coordination - Environment variable management - Conditional execution (<code>if:</code> conditions) - Step output handling and dependencies</p>"},{"location":"design/workflow_executor_design/#phase-3-advanced-features-future","title":"Phase 3: Advanced Features (Future)","text":"<p>Scope: Enterprise workflow features - DASK task graph integration - Progress monitoring and status reporting - Resource management and limits - Workflow queue management</p>"},{"location":"design/workflow_executor_design/#integration-points","title":"Integration Points","text":""},{"location":"design/workflow_executor_design/#with-action-framework","title":"With Action Framework","text":"<pre><code># Future integration pattern\nfor step in workflow[\"steps\"]:\n    action = ActionRegistry.get_action(step[\"uses\"])\n    inputs = construct_action_inputs(step[\"with\"], job_context)\n    result = action.run(inputs)\n</code></pre>"},{"location":"design/workflow_executor_design/#with-schema-validation","title":"With Schema Validation","text":"<pre><code># Current integration\nworkflow = load_workflow_file(path)     # schema.py\nvalidate_workflow(workflow)             # schema.py\njobs = executor.expand_matrix(workflow[\"matrix\"])  # workflow.py\n</code></pre>"},{"location":"design/workflow_executor_design/#with-dask-future","title":"With DASK (Future)","text":"<pre><code># Planned integration\njobs = executor.expand_matrix(matrix)\ntask_graph = DaskBuilder.build_workflow_graph(jobs, workflow)\nresults = executor.execute_dask_graph(task_graph)\n</code></pre>"},{"location":"design/workflow_executor_design/#testing-strategy","title":"Testing Strategy","text":""},{"location":"design/workflow_executor_design/#unit-tests-isolated-logic","title":"Unit Tests (Isolated Logic)","text":"<ul> <li>Matrix expansion algorithm verification</li> <li>Path construction with various inputs</li> <li>Exception handling edge cases</li> <li>Mocked dependencies for isolation</li> </ul>"},{"location":"design/workflow_executor_design/#functional-tests-real-operations","title":"Functional Tests (Real Operations)","text":"<ul> <li>YAML file parsing with real workflow files</li> <li>Filesystem operations for temporary test files</li> <li>Integration with schema validation</li> <li>End-to-end workflow parsing scenarios</li> </ul>"},{"location":"design/workflow_executor_design/#edge-case-coverage","title":"Edge Case Coverage","text":"<ul> <li>Exception handling in matrix expansion (<code>itertools.product</code> failures)</li> <li>Empty matrices and default value handling</li> <li>Invalid workflow file scenarios</li> <li>Missing matrix variable handling</li> </ul>"},{"location":"design/workflow_executor_design/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"design/workflow_executor_design/#matrix-expansion-scaling","title":"Matrix Expansion Scaling","text":"<ul> <li>Time Complexity: O(n\u2081 \u00d7 n\u2082 \u00d7 ... \u00d7 n\u2096) where n\u1d62 is the size of each matrix dimension</li> <li>Space Complexity: O(jobs) linear in the number of generated job combinations</li> <li>Practical Limits: Reasonable for typical causal discovery experiments (&lt; 1000 jobs)</li> </ul>"},{"location":"design/workflow_executor_design/#memory-usage","title":"Memory Usage","text":"<ul> <li>Workflow parsing: Linear in YAML file size</li> <li>Matrix expansion: Linear in number of generated jobs</li> <li>Path construction: Constant per job</li> </ul>"},{"location":"design/workflow_executor_design/#future-optimisations","title":"Future Optimisations","text":"<ul> <li>Lazy matrix expansion for large matrices</li> <li>Streaming job processing for memory efficiency</li> <li>Job batching for DASK execution</li> </ul>"},{"location":"design/workflow_executor_design/#alignment-with-causaliq-standards","title":"Alignment with CausalIQ Standards","text":""},{"location":"design/workflow_executor_design/#development-guidelines-compliance","title":"Development Guidelines Compliance","text":"<ul> <li>\u2705 Small incremental changes: 99-line focused implementation</li> <li>\u2705 100% test coverage: Comprehensive unit and functional tests  </li> <li>\u2705 CI compliance: All formatting, linting, and type checking standards met</li> <li>\u2705 British English: Documentation and code comments</li> <li>\u2705 Type safety: Complete type annotations with mypy validation</li> </ul>"},{"location":"design/workflow_executor_design/#architecture-principles","title":"Architecture Principles","text":"<ul> <li>\u2705 GitHub Actions foundation: Consistent with CI/CD workflow patterns</li> <li>\u2705 Action-based components: Integration with existing action framework</li> <li>\u2705 Schema-first design: Leverage existing validation infrastructure</li> <li>\u2705 Incremental functionality: Foundation for future execution features</li> </ul> <p>This design provides a solid foundation for workflow execution while maintaining the incremental development approach and high quality standards established in the CausalIQ Workflow project.</p>"}]}