{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"CausalIQ Workflow","text":""},{"location":"#welcome","title":"Welcome","text":"<p>Welcome to the documentation for CausalIQ Workflow - a comprehensive solution for designing, executing, and reproducing causal discovery and inference experiments at scale. It is part of the CausalIQ ecosystem for intelligent causal discovery.</p> <p>causaliq-workflow builds on causaliq-core for its action framework and caching infrastructure, ensuring consistency across the CausalIQ ecosystem.</p>"},{"location":"#overview","title":"Overview","text":"<p>This site provides detailed documentation, including:</p> <ul> <li>Development roadmap</li> <li>User guide</li> <li>Architectural vision</li> <li>Design notes</li> <li>API reference for users and contributors</li> </ul>"},{"location":"#quickstart-installation","title":"Quickstart &amp; Installation","text":"<p>For a quickstart guide and installation instructions, see the README on GitHub.</p>"},{"location":"#documentation-contents","title":"Documentation Contents","text":"<ul> <li>Development Roadmap: Roadmap of upcoming features</li> <li>[User Guide]: Comprehensive user guide (coming soon)</li> <li>Architecture: Overall architecture and design notes</li> <li>API Reference: Complete reference for Python code</li> <li>Development Guidelines: CausalIQ guidelines for developers</li> <li>Changelog</li> <li>License</li> </ul>"},{"location":"#support-community","title":"Support &amp; Community","text":"<ul> <li>GitHub Issues: Report bugs or request features.</li> <li>GitHub Discussions: Ask questions and join the community.</li> </ul> <p>Tip: Use the navigation sidebar to explore the documentation. For the latest code and releases, visit the causaliq-workflow GitHub repository.</p> <p>Supported Python Versions: 3.9, 3.10, 3.11, 3.12, 3.13 Default Python Version: 3.11</p>"},{"location":"example_workflows/","title":"CausalIQ Workflow - Example Workflows","text":"<p>!!! info \"Feature Implementation Status\"     This documentation includes both currently implemented features and planned features. Planned features are marked with warning boxes. Currently implemented:</p> <pre><code>- \u2705 Workflow parsing and validation\n- \u2705 Matrix expansion into job configurations\n- \u2705 Template variable substitution (`{{variable}}`)\n- \u2705 Action registry and discovery\n- \u2705 Schema validation (strings, numbers, booleans in `with:` blocks)\n\nPlanned features (not yet implemented):\n\n- \u23f3 Array values in `with:` blocks\n- \u23f3 Series-based workflows\n- \u23f3 Resource configuration\n- \u23f3 LLM integration\n- \u23f3 Advanced randomization strategies\n</code></pre>"},{"location":"example_workflows/#design-philosophy-inspired-by-ci-workflows","title":"Design Philosophy: Inspired by CI-Workflows","text":"<p>The CausalIQ Workflows are inspired by the concepts within  Continuous Integration (CI) workflows. They provide a subset of the features  provided by CI workflows and much of the functionality is simplified to ease the job of specifying CausalIQ workflows.</p> <p>Key concepts in CausalIQ workflows are:</p> <ul> <li>Workflows are a series of one or more sequential steps.</li> <li>Steps can be either an action or they can run shell commands.</li> <li>actions execute typical causal inference and evaluation activities like structure    learning, graph evaluation, causal inference etc., and actions:<ul> <li>are implemented in other CausalIQ packages such as causaliq-discovery,        causaliq-analysis etc. as specified in the uses: keyword of the step;</li> <li>take parameters values for the action - e.g., the algorithm to use - are specified using the with: keyword;</li> <li>actions can be implemented intelligently to perform their work efficiently and in parallel.</li> </ul> </li> <li>a matrix concept allows the set of steps to be repeated over multiple   combinations of values, for example over a set of networks, sample sizes and   algorithms. This is particularly valuable for large scale comparative experiments.</li> <li> <p>workflows may be run from the command line interface (CLI) using commands like:</p> <p><code>shell cqflow example_workflow.yaml --model=child,property</code>   - CLI arguments can be used to override workflow and matrix parameters so     that the same workflow can be easily reused;   - the run: command can be used to run a workflow so that \"workflows of     workflows\" are supported.</p> </li> </ul> <p>The CausalIQ Workflow CLI has a --mode argument which controls its overall behaviour as follows:</p> <ul> <li>--mode=run: this actually executes the workflow. Note that, in this case  the actions in the workflow are performed conservatively - if the outputs of  actions are already present on the file system the action is not repeated. This  therefore faciltates restarting the workflow if it has previously been interrupted.</li> <li>--mode=dry-run: this is the default behaviour and reports what would be done if --mode=run were used, but does not perform the actual actions. In doing so,   it has the valuable side-effect of checking that the workflow definitions are   all valid.</li> <li>--mode=compare: all actions in the workflow will be re-run regardless of whether the output is present on the filesystem or not. If the output is present on the filesystem, the newly-generated output will replace it, and any differences with the previous version reported. This mode is therefore a very powerful form of functional testing.  </li> </ul>"},{"location":"example_workflows/#example-workflows-in-causal-discovery","title":"Example Workflows in Causal Discovery","text":""},{"location":"example_workflows/#unparameterised-single-step-workflows","title":"Unparameterised single-step workflows","text":"<p>The first simple example is a minimal workflow that asks a LLM to propose a causal network for covid</p> <pre><code># llm_covid.yaml\nsteps:\n  - name: \"Llama3 proposes a covid graph\"\n    uses: \"causaliq-knowledge\"\n    with:\n      action: \"generate_graph\"\n      context: \"models/covid/covid_llm_context.json\"\n      result: \"results/covid.db\"\n</code></pre> <p>This is a single-step workflow with a minimal set of values defined for the  single step:</p> <ul> <li>name - each step must be given a neame which is hopefully meaningful</li> <li>uses - the step must define which CausalIQ package it uses</li> </ul> <p>and then has a variable number of parameters following the with: statment that defines the processing to be undertaken by the package through its ActionProvider interface. In this example, these are:</p> <ul> <li>action - this parameter is always present and defines what particular action the package is required to do</li> <li>context - defines a JSON file providing details of the problem domain and its variables which will serve as input to the LLM for this action</li> <li>result - defines a results (sql-lite) database where results will be placed</li> </ul> <p>The same result could be achieved using the causaliq-knowledge CLI command directly, but using this simple workflow avoids having to specify the model and result locations on the command line by using the following command:</p> <pre><code>cqflow llm_covid.yaml  # cqflow is a synonym for causaliq-workflow\n</code></pre> <p>Using <code>.yaml</code> workflow files becomes increasingly convenenient as additional parameters are specified, for example, as in the following example:</p> <pre><code># llm_covid.yaml\ndescription: \"Using gpt-4 to propose a causal graph using rich context\"\nroot_dir: \"c:/dev/causaliq/causaliq-research\"\n\nsteps:\n  - name: \"GPT-4 proposes a covid graph\"\n    uses: \"causaliq-knowledge\"\n    with:\n      action: \"generate_graph\"\n      llm: \"openai/gpt-4o-mini\"\n      context_level: \"rich\"\n      context: \"models/covid/covid_llm_context.json\"\n      result: \"results/covid.db\"\n</code></pre> <p>Additional workflow-level parameters here are: </p> <ul> <li>description - a human friendly description of the workflow</li> <li>root_dir - a base file location for all paths used in the workflow (by default the current directory is used)</li> </ul> <p>Additional parameters for the <code>generate_graph</code> step are:</p> <ul> <li>llm - explicit specification of LLM model to use instead of using the default LLM</li> <li>context_level - specify that a rich level of context is to be provided to the LLM</li> </ul> <p>The next simple example runs the Tabu-Stable structure learning algorithm on 1000 synthetic rows from the Asia network. Specific parameters for structure learning are:</p> <ul> <li>action - specifices the structure learning action</li> <li>algorithm - the structure learning algorithm</li> <li>sample_size - how many rows of data to use</li> <li>dataset - file containing synthetically generated rows</li> <li>debug - is set true so that a detailed iteration-by-iteratuon analysis of the learning process is included in the result</li> <li>results - the results database, where in this case an entry will be  the learnt graph, an iteration trace, and structure learning metadata</li> </ul> <pre><code># tabu_asia.yaml\nid: \"tabu-stable_asia_1k\"\ndescription: \"Tabu-stable learning Asia from 1K data with full trace\"\n\nsteps:\n  - name: \"Learn Structure\"\n    uses: \"causaliq-discovery\"\n    with:\n      action: \"learn_structure\"\n      algorithm: \"tabu-stable\"\n      sample_size: \"1k\"\n      dataset: \"models/asia/asia_10k.data.gz\"\n      debug: True\n      results: \"results/tabu_stable.db\"\n</code></pre> <p>The example below updates the metadata about a learned graph to evaluate structural accuracy and some inference scores for all graphs learnt by asia.</p> <p>!!! warning \"Planned Feature\"     Array values in <code>with:</code> blocks (e.g., <code>dag_structure: [\"f1\", \"shd\"]</code> and <code>score: [\"bic\", \"bde\"]</code>) shown in this example are planned features. The current schema only supports string, number, and boolean values in action parameters.</p> <pre><code># evaluate_graph.yaml\ndescription: \"Evaluate structure and score of learned graphs\"\nmodel: \"asia\"\n\nsteps:\n  - name: \"Evaluate Graph\"\n    uses: \"causaliq-analysis\"\n    with:\n      action: \"evaluate_graph\"\n      model: {{model}}\n      true_graph: \"models/{{model}}/{{model}}.xdsl\"\n      dag_structure: [\"f1\",shd\"]\n      cpdag_structure: \"shd\"\n      score: [\"bic\", \"bde\"]\n      results: \"results/tabu_stable.db\"\n</code></pre> <p>Another simple workflow might be to download resources from Zenodo. This would download all the resources associated with my PhD thesis and unzip in the specified output folder.</p> <pre><code># download_paper.yaml\ndescription: \"Download assets for PhD Thesis\"\n\nsteps:\n  - name: \"Download paper\"\n    uses: \"causaliq-papers\"\n    with:\n      operation: \"download\"\n      doi: \"10.5072/zenodo.338579\"\n      output: \"/papers/2025KitsonThesis\"\n</code></pre>"},{"location":"example_workflows/#parameterised-structure-learning-example","title":"Parameterised Structure Learning Example","text":"<p>We can make the simple learning example more general by adding workflow-level variables to parameterise the workflow. In this example, the sample_size parameter has a default value of 1000 if not specified on the command line, whereas the None value for model (i.e. network) indicates that this must be specified on the command line.  Note how the model and sample_size parameters are used in the dataset and output action parameters so that the correct input file and output folder are used.</p> <pre><code># tabu_learning.yaml\ndescription: \"Parameterised Tabu-stable learning\"\nsample_size: \"1K\"\nmodel: None\n\nsteps:\n  - name: \"Learn Structure\"\n    uses: \"causaliq-discovery\"\n    with:\n      action: \"learn_structure\"\n      algorithm: \"tabu-stable\"\n      max_time: 10\n      sample_size: {{sample_size}}\n      dataset: \"/models/{{model}}/{{network}}_10k.data.gz\"\n      output: \"/results/tabu_stable.db\n</code></pre> <p>We can now run Tabu-Stable structure learning for any network and sample size using commands like:</p> <pre><code>cqflow tabu_learning.yaml --network=child --sample_size=500\n</code></pre>"},{"location":"example_workflows/#multiple-step-examples","title":"Multiple Step Examples","text":"<p>We could enhance this by adding a step before structure learning where an LLM provides an initial graph. In this example, the dataset is provided so that the LLM has access to the values, and there is also some additional domain context provided in a json file. This could generate a graph.xml in the output folder and probably also some metadata and/or history of the prompts and responses  made to the LLM. Note that, this LLM generated graph would then be available to be re-used in other experiments, and the prompts/response trail provides the means to exactly replicate this result.</p> <pre><code># tabu_llm_learning.yaml\ndescription: \"Tabu-stable learning Asia with LLM initialisation\"\nsample_size: \"1K\"\nmodel: None\n\nsteps:\n  - name: \"LLM Graph Initialisation\"\n    uses: \"causaliq-knowledge\"\n    with:\n      action: \"generate_graph\"\n      context: \"models/{{model}}/{{model}}_llm_context.json\"\n      result: \"results/llm_graphs.db\"\n\n  - name: \"Learn Structure\"\n    uses: \"causaliq-discovery\"\n    with:\n      action: \"learn_structure\"\n      initial_graph: \"results/llm_graphs.db\"\n      # etc ....\n</code></pre>"},{"location":"example_workflows/#matrix-strategy-workflow","title":"Matrix Strategy Workflow","text":"<p>In many cases we will wish to run comparative experiments with a different result for each combinations of algorithm, network, and sample size and randomisation. Note how we structure the output folder path so that we ensure the result for each individual experiment is placed in its own folder. We also use an \"id\" workflow variablesto keep these results separate from those of other workflows.</p> <p>Internally, the action - in this case causal-discovery - may be implemented intelligently to maximise efficiency. For example, in this case, it may read the maximum number of rows from the filesystem dataset just once, and then adjust the effective sample size and randomisation internally for each individual experiment.</p> <p>!!! warning \"Planned Feature\"     The <code>randomise</code> workflow-level variable and array values in <code>with:</code> blocks (e.g., <code>randomise: {{randomise}}</code>) shown in this example are planned features. The current schema only supports string, number, and boolean values in action parameters.</p> <pre><code># matrix_experiment.yaml\ndescription: \"Algorithm stability comparison\"\nrandomise: [\"variable_order\", \"variable_name\"]\n\nmatrix:\n  model: [\"asia\", \"cancer\"]\n  algorithm: [\"pc\", \"ges\"]\n  sample_size: [\"100\", \"1K\"]\n  seed: [1, 25]\n\nsteps:\n  - name: \"Structure Learning\"\n    uses: \"causaliq-discovery\"\n    with:\n      action: \"learn_structure\"\n      algorithm: \"{{algorithm}}\"\n      sample_size: \"{{sample_size}}\"\n      dataset: \"models/{{model}}/{{model}}_10k.data.gz\"\n      randomise: {{randomise}}\n      seed: {{seed}}\n      output: \"results/algo_stability.db\"\n</code></pre>"},{"location":"example_workflows/#workflow-of-workflows","title":"Workflow of workflows","text":"<p>The ability of one workflow to call other parameterised workflows facilitates the creation and testing of complex CausalIQ workflows, for instance to run all the experiments, analysis and asset generation for a research paper or thesis. CausalIQ Papers makes use of this to provide reproducibility of CausalIQ published papers.</p> <p>The example below shows a simple top level workflow which might reproduce the experiments and analysis behind a thesis. The lower level workflows such as create_chapter_4.yaml might then call other workflows to perform the structure learning, result analysis and asset generation for Chapter 4.</p> <pre><code># reproduce_thesis.yaml\n\nid: \"Kitson2025thesis\"\n\nmatrix:\n  chapter: [4, 5, 6]\n\nsteps:\n  name: \"Create chapter {{chapter}}\"\n  run: \"cqflow create_chapter_{{chapter}}.yaml --id={{id}}\n</code></pre>"},{"location":"example_workflows/#parallel-jobs","title":"Parallel Jobs","text":"<p>CI workflows provide a jobs: keyword which allows multiple sequences of steps to run in parallel. We are not planning to implement this at the moment, instead relying on actions to provde parallelism using DAK tasks. This keeps CausalIQ Workflow functionality simple and reflects the fact that structure learning steps involving many individual structure learning experiments can keep even very powerful machines busy.</p>"},{"location":"example_workflows/#template-variables","title":"Template Variables","text":""},{"location":"example_workflows/#flexible-path-templating-pattern","title":"Flexible Path Templating Pattern","text":"<p>Our implementation supports flexible path templating using matrix variables:</p> <pre><code># Template variables can be used in action parameters:\n# {{id}} - workflow identifier\n# {{network}} - current matrix value for network\n# {{algorithm}} - current matrix value for algorithm  \n# {{sample_size}} - current matrix value for sample size\n\n# Example expansion for graphs using matrix above:\n# Job 1: dataset=\"/experiments/data/asia.csv\", result=\"/experiments/results/algo-comparison-001/pc/asia/100\"\n# Job 2: dataset=\"/experiments/data/asia.csv\", result=\"/experiments/results/algo-comparison-001/pc/asia/1K\"\n# Job 3: dataset=\"/experiments/data/asia.csv\", result=\"/experiments/results/algo-comparison-001/ges/graph_asia_0.01.xml\"\n# ... (8 total combinations)\n</code></pre>"},{"location":"example_workflows/#template-variable-validation","title":"Template Variable Validation","text":"<p>The workflow executor automatically validates that all template variables (<code>{{variable}}</code>) used in action parameters are available from either: - Workflow properties: <code>id</code>, <code>description</code>  - Matrix variables: Variables defined in the <code>matrix</code> section</p> <p>Valid Template Usage:</p> <pre><code>id: \"my-experiment-001\"\nmatrix:\n  dataset: [\"asia\", \"cancer\"]\n  algorithm: [\"pc\", \"ges\"]\n\nsteps:\n  - uses: \"dummy-structure-learner\"\n    with:\n      # These are all valid - variables exist in workflow context\n      output: \"/results/{{id}}/{{dataset}}_{{algorithm}}\"\n      description: \"Running {{algorithm}} on {{dataset}}\"\n</code></pre> <p>Invalid Template Usage (Validation Error):</p> <pre><code>id: \"my-experiment-001\"\nmatrix:\n  dataset: [\"asia\", \"cancer\"]\n\nsteps:\n  - uses: \"dummy-structure-learner\"\n    with:\n      # This will cause a WorkflowExecutionError\n      output: \"/results/{{unknown_variable}}/{{missing_param}}\"\n</code></pre> <p>Error Message Example:</p> <pre><code>WorkflowExecutionError: Unknown template variables: unknown_variable, missing_param\nAvailable context: id, description, dataset\n</code></pre>"},{"location":"example_workflows/#implemented-features","title":"Implemented Features","text":""},{"location":"example_workflows/#action-framework","title":"\u2705 Action Framework","text":"<ul> <li>Action Registration: Actions defined as Python classes</li> <li>Type Safety: Input/output specifications with type hints</li> <li>Error Handling: Comprehensive ActionExecutionError and ActionValidationError</li> <li>GraphML Output: Standardized format for causal graphs</li> </ul>"},{"location":"example_workflows/#workflow-execution-engine","title":"\u2705 Workflow Execution Engine","text":"<ul> <li>YAML Parsing: Parse and validate GitHub Actions-style workflow files</li> <li>Matrix Expansion: Convert matrix variables into individual job configurations</li> <li>Flexible Path Templating: User-controlled path generation with {{}} template variables</li> <li>Template Variable Validation: Automatic validation of {{variable}} patterns against available context</li> <li>Error Propagation: Comprehensive error handling with WorkflowExecutionError</li> </ul>"},{"location":"example_workflows/#schema-validation","title":"\u2705 Schema Validation","text":"<ul> <li>GitHub Actions Syntax: Familiar workflow patterns</li> <li>Matrix Variables: Full support for parameterized experiments  </li> <li>Flexible Action Parameters: Template variables in action <code>with:</code> blocks</li> <li>Action Parameters: with blocks for action configuration</li> </ul>"},{"location":"example_workflows/#test-coverage","title":"\u2705 Test Coverage","text":"<ul> <li>Functional Tests: Real filesystem operations with tracked test data</li> <li>Unit Tests: Mocked dependencies for action logic testing</li> <li>Schema Tests: Comprehensive validation of all workflow features</li> <li>100% Coverage: Complete test coverage including edge cases</li> </ul>"},{"location":"example_workflows/#example-action-implementation","title":"Example Action Implementation","text":"<pre><code>class DummyStructureLearnerAction(BaseActionProvider):\n    \"\"\"Reference action implementation.\"\"\"\n\n    name = \"dummy-structure-learner\"\n    version = \"1.0.0\"\n\n    inputs = {\n        \"data_path\": ActionInput(\n            name=\"data_path\",\n            description=\"Path to input CSV dataset (auto-constructed)\",\n            required=True,\n            type_hint=\"str\",\n        ),\n        \"output_dir\": ActionInput(\n            name=\"output_dir\", \n            description=\"Directory for output files (auto-constructed)\",\n            required=True,\n            type_hint=\"str\",\n        ),\n        \"dataset\": ActionInput(\n            name=\"dataset\",\n            description=\"Dataset name from matrix\",\n            required=True,\n            type_hint=\"str\",\n        ),\n        \"algorithm\": ActionInput(\n            name=\"algorithm\",\n            description=\"Algorithm name from matrix\",\n            required=True,\n            type_hint=\"str\",\n        ),\n    }\n\n    def run(\n        self,\n        action: str,\n        parameters: Dict[str, Any],\n        mode: str = \"dry-run\",\n        context: Optional[\"WorkflowContext\"] = None,\n        logger: Optional[\"WorkflowLogger\"] = None,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Create GraphML output file.\"\"\"\n        # Implementation creates valid GraphML file\n        # Returns graph_path, node_count, edge_count\n</code></pre>"},{"location":"example_workflows/#current-implementation-workflowexecutor","title":"Current Implementation: WorkflowExecutor","text":""},{"location":"example_workflows/#workflowexecutor-implementation-phase-1-complete","title":"WorkflowExecutor Implementation (Phase 1 Complete)","text":"<pre><code>from causaliq_workflow import WorkflowExecutor\n\n# Parse workflow and expand matrix\nexecutor = WorkflowExecutor()\nworkflow = executor.parse_workflow(\"experiment.yml\")\n\n# Matrix expansion example\nmatrix = {\"algorithm\": [\"pc\", \"ges\"], \"dataset\": [\"asia\", \"cancer\"], \"alpha\": [0.01, 0.05]}\njobs = executor.expand_matrix(matrix)  # Returns 8 job configurations\n\n# Example workflow with flexible paths\nworkflow_example = {\n    \"id\": \"experiment-001\",\n    \"description\": \"Flexible causal discovery experiment\",\n    \"matrix\": matrix,\n    \"steps\": [{\n        \"name\": \"Structure Learning\",\n        \"uses\": \"dummy-structure-learner\", \n        \"with\": {\n            \"dataset\": \"/experiments/data/{{dataset}}.csv\",\n            \"result\": \"/experiments/results/{{id}}/{{algorithm}}/graph_{{dataset}}_{{alpha}}.xml\",\n            \"alpha\": \"{{alpha}}\"\n        }\n    }]\n}\n\n# Each job contains expanded matrix variables for template substitution\nfor job in jobs:\n    print(f\"Job: {job}\")\n    # Example: {'algorithm': 'pc', 'dataset': 'asia', 'alpha': 0.01}\n</code></pre> <p>Implemented Features: - \u2705 Parse and validate YAML workflow files - \u2705 Expand matrix variables into job configurations - \u2705 Support flexible path templating with {{}} variables - \u2705 Comprehensive error handling and 100% test coverage</p> <p>Next Phase: Action execution engine with template variable substitution</p> <p>!!! warning \"Planned Feature\"     The <code>series</code>, <code>resources</code>, <code>analysis</code>, and <code>monitoring</code> workflow sections shown below are planned features and not yet implemented in the current schema.</p> <pre><code># series_workflow_example.yaml (Planned Feature)\n  ges_series:\n    algorithm: \"ges\" \n    package: \"causaliq-discovery\"\n    datasets: [\"alarm\", \"asia\"]  # Reference same datasets\n    sample_sizes: [100, 500, 1000]\n    randomizations: 10\n    hyperparameters:\n      score_type: [\"bic\", \"aic\"]\n\n# Resource configuration\nresources:\n  max_parallel_jobs: 8\n  memory_limit_per_job: \"4GB\"\n  runtime_limit_per_job: \"30m\"\n  total_runtime_limit: \"4h\"\n\n# Analysis configuration\nanalysis:\n  compare_series: [\"pc_series\", \"ges_series\"]\n  metrics: [\"shd\", \"precision\", \"recall\", \"f1\"]\n  statistical_significance: 0.05\n  output_format: [\"csv\", \"json\"]\n\n# Logging and monitoring\nmonitoring:\n  progress_updates: \"1m\"\n  log_level: \"INFO\"\n  save_intermediate: true\n</code></pre>"},{"location":"example_workflows/#use-case-2-external-package-integration","title":"Use Case 2: External Package Integration","text":"<p>!!! warning \"Planned Feature\"     The <code>series</code>, <code>external_packages</code>, <code>resources</code>, and <code>validation</code> workflow sections shown in this example are planned features and not yet implemented in the current schema.</p>"},{"location":"example_workflows/#scenario","title":"Scenario","text":"<p>Use R bnlearn package alongside Python algorithms for comprehensive comparison.</p>"},{"location":"example_workflows/#multi-package-workflow","title":"Multi-Package Workflow","text":"<pre><code># multi_package_comparison.yaml\nmetadata:\n  name: \"python_r_algorithm_comparison\"\n  description: \"Compare Python and R causal discovery implementations\"\n\nseries:\n  python_pc:\n    algorithm: \"pc\"\n    package: \"causaliq-discovery\"\n    datasets: [\"alarm\"]\n    sample_sizes: [500, 1000]\n    randomizations: 5\n    hyperparameters:\n      alpha: 0.05\n\n  r_bnlearn_pc:\n    algorithm: \"pc.stable\"\n    package: \"r_bnlearn\"\n    datasets: [\"alarm\"]  # Same datasets for comparison\n    sample_sizes: [500, 1000] \n    randomizations: 5\n    hyperparameters:\n      alpha: 0.05\n      test: \"cor\"\n\n# External package requirements\nexternal_packages:\n  r_bnlearn:\n    check_installation: true\n    required_version: \"&gt;=4.7\"\n    install_if_missing: false  # Fail if not available\n\nresources:\n  max_parallel_jobs: 4  # Fewer for external packages\n  memory_limit_per_job: \"8GB\"\n\nvalidation:\n  dry_run: true  # Preview before execution\n  check_dependencies: true\n</code></pre>"},{"location":"example_workflows/#use-case-3-configuration-inheritance","title":"Use Case 3: Configuration Inheritance","text":"<p>!!! warning \"Planned Feature\"     The <code>defaults</code>, <code>inherits</code>, and inheritance-based workflow configuration shown in this example are planned features and not yet implemented in the current schema.</p>"},{"location":"example_workflows/#scenario_1","title":"Scenario","text":"<p>Create specific experiments based on common base configuration.</p>"},{"location":"example_workflows/#base-configuration","title":"Base Configuration","text":"<pre><code># base_causal_experiment.yaml\nmetadata:\n  name: \"base_causal_discovery_template\"\n  description: \"Base template for causal discovery experiments\"\n\ndefaults:\n  datasets:\n    - name: \"alarm\" \n      zenodo_id: \"alarm_networks_v1\"\n    - name: \"asia\"\n      zenodo_id: \"asia_networks_v1\"\n  sample_sizes: [100, 500, 1000]\n  randomizations: 10\n\nresources:\n  max_parallel_jobs: 6\n  memory_limit_per_job: \"4GB\"\n  runtime_limit_per_job: \"1h\"\n\nmonitoring:\n  log_level: \"INFO\"\n  progress_updates: \"30s\"\n</code></pre>"},{"location":"example_workflows/#inherited-specific-experiment","title":"Inherited Specific Experiment","text":"<pre><code># pc_alpha_study.yaml\ninherits: \"base_causal_experiment.yaml\"\n\nmetadata:\n  name: \"pc_alpha_sensitivity_study\"\n  description: \"Study effect of alpha parameter on PC algorithm\"\n\noverrides:\n  series:\n    pc_alpha_study:\n      algorithm: \"pc\"\n      package: \"causaliq-discovery\" \n      hyperparameters:\n        alpha: [0.001, 0.01, 0.05, 0.1, 0.2]\n\n  analysis:\n    metrics: [\"shd\", \"precision\", \"recall\"]\n    focus_parameter: \"alpha\"\n    statistical_tests: true\n</code></pre>"},{"location":"example_workflows/#use-case-4-llm-integration-for-model-averaging","title":"Use Case 4: LLM Integration for Model Averaging","text":"<p>!!! warning \"Planned Feature\"     The <code>series</code>, <code>llm_analysis</code>, and <code>model_averaging</code> workflow sections shown in this example are planned features and not yet implemented in the current schema.</p>"},{"location":"example_workflows/#scenario_2","title":"Scenario","text":"<p>Use LLM to guide intelligent model averaging across multiple algorithm results.</p>"},{"location":"example_workflows/#llm-enhanced-workflow","title":"LLM-Enhanced Workflow","text":"<pre><code># llm_model_averaging.yaml\nmetadata:\n  name: \"llm_guided_model_averaging\"\n  description: \"Use LLM for intelligent model averaging\"\n\n# First run multiple algorithms\nseries:\n  pc_results:\n    algorithm: \"pc\"\n    package: \"causaliq-discovery\"\n    datasets: [\"healthcare_study\"]\n    sample_sizes: [1000]\n    randomizations: 5\n    hyperparameters:\n      alpha: 0.05\n\n  ges_results:\n    algorithm: \"ges\"\n    package: \"causaliq-discovery\"\n    datasets: [\"healthcare_study\"]\n    sample_sizes: [1000]\n    randomizations: 5\n    hyperparameters:\n      score_type: \"bic\"\n\n# LLM integration for analysis\nllm_analysis:\n  stage: \"post_discovery\"\n  package: \"causaliq-knowledge\"\n  tasks:\n    - task: \"analyze_algorithm_outputs\"\n      inputs: [\"pc_results\", \"ges_results\"]\n      domain_knowledge: \"healthcare/cardiology\"\n\n    - task: \"suggest_averaging_weights\"\n      based_on: \"algorithm_reliability_analysis\"\n\n    - task: \"validate_combined_graph\"\n      domain_expertise: true\n\n# Model averaging using LLM suggestions\nmodel_averaging:\n  package: \"causaliq-analysis\"\n  method: \"llm_weighted_average\"\n  inputs: [\"pc_results\", \"ges_results\", \"llm_suggestions\"]\n</code></pre>"},{"location":"example_workflows/#use-case-5-dataset-download-and-randomization","title":"Use Case 5: Dataset Download and Randomization","text":"<p>!!! warning \"Planned Feature\"     The <code>datasets</code>, <code>randomization</code>, <code>series</code>, and <code>analysis</code> workflow sections shown in this example are planned features and not yet implemented in the current schema.</p>"},{"location":"example_workflows/#scenario_3","title":"Scenario","text":"<p>Automated dataset download from Zenodo with systematic randomization for robustness testing.</p>"},{"location":"example_workflows/#dataset-management-workflow","title":"Dataset Management Workflow","text":"<pre><code># dataset_robustness_study.yaml\nmetadata:\n  name: \"dataset_robustness_analysis\"\n  description: \"Study algorithm robustness to data variations\"\n\n# Dataset configuration with automatic download\ndatasets:\n  primary_dataset:\n    name: \"benchmark_networks\"\n    source: \"zenodo\"\n    zenodo_id: \"benchmark_causal_v2\"\n    cache_locally: true\n\n# Randomization strategies  \nrandomization:\n  strategies:\n    - type: \"subsample\"\n      fractions: [0.7, 0.8, 0.9, 1.0]\n\n    - type: \"variable_reorder\"\n      random_seeds: [42, 123, 456]\n\n    - type: \"noise_injection\"\n      noise_levels: [0.0, 0.05, 0.1]\n      noise_type: \"gaussian\"\n\n# Series using randomized datasets\nseries:\n  robustness_test:\n    algorithm: \"pc\"\n    package: \"causaliq-discovery\"\n    apply_all_randomizations: true\n    hyperparameters:\n      alpha: 0.05\n\n# Analysis focuses on robustness metrics\nanalysis:\n  robustness_metrics:\n    - \"stability_across_subsamples\"\n    - \"invariance_to_ordering\"\n    - \"noise_resistance\"\n  generate_robustness_report: true\n</code></pre>"},{"location":"example_workflows/#cli-usage-examples","title":"CLI Usage Examples","text":""},{"location":"example_workflows/#basic-execution","title":"Basic Execution","text":"<pre><code># Execute a series-based workflow\ncausaliq-workflow run pc_ges_comparison.yaml\n\n# Dry-run to preview execution plan\ncausaliq-workflow validate pc_ges_comparison.yaml --dry-run\n\n# Monitor running workflow\ncausaliq-workflow status workflow-abc123\n\n# Pause running workflow\ncausaliq-workflow pause workflow-abc123\n\n# Resume paused workflow  \ncausaliq-workflow resume workflow-abc123\n</code></pre>"},{"location":"example_workflows/#advanced-options","title":"Advanced Options","text":"<pre><code># Override resource limits\ncausaliq-workflow run experiment.yaml --max-jobs 16 --memory-per-job 8GB\n\n# Run specific series only\ncausaliq-workflow run experiment.yaml --series pc_series\n\n# Export results in specific format\ncausaliq-workflow results export workflow-abc123 --format csv --output results/\n</code></pre>"},{"location":"example_workflows/#python-api-examples","title":"Python API Examples","text":""},{"location":"example_workflows/#basic-workflow-execution","title":"Basic Workflow Execution","text":"<pre><code>from causaliq_workflow import WorkflowManager, ConfigurationManager\n\n# Load and validate configuration\nconfig_manager = ConfigurationManager()\nworkflow_config = config_manager.load_workflow(\"pc_ges_comparison.yaml\")\n\n# Execute workflow\nworkflow_manager = WorkflowManager()\nresult = workflow_manager.execute_workflow(workflow_config)\n\n# Access results by series\npc_results = result.get_series_results(\"pc_series\")\nges_results = result.get_series_results(\"ges_series\")\n</code></pre>"},{"location":"example_workflows/#series-analysis","title":"Series Analysis","text":"<pre><code>from causaliq_workflow.analysis import SeriesComparison\n\n# Compare algorithm performance across series\ncomparison = SeriesComparison()\ncomparison.add_series(\"PC Algorithm\", pc_results)\ncomparison.add_series(\"GES Algorithm\", ges_results)\n\n# Generate comparison metrics\nmetrics = comparison.compare_algorithms([\"shd\", \"precision\", \"recall\"])\nstatistical_significance = comparison.statistical_test(alpha=0.05)\n\n# Export results\ncomparison.export_results(\"algorithm_comparison.csv\")\n</code></pre>"},{"location":"example_workflows/#configuration-inheritance","title":"Configuration Inheritance","text":"<pre><code>from causaliq_workflow.config import ConfigurationInheritance\n\n# Create experiment based on template\ninheritance = ConfigurationInheritance()\nbase_config = inheritance.load_base(\"base_causal_experiment.yaml\")\n\n# Apply specific overrides\nspecific_config = inheritance.create_derived(\n    base_config,\n    overrides={\n        \"series\": {\n            \"custom_study\": {\n                \"algorithm\": \"fci\",\n                \"hyperparameters\": {\"alpha\": 0.01}\n            }\n        }\n    }\n)\n</code></pre> <p>This focused approach emphasizes the series concept and immediate implementation needs while providing practical examples for the three-month development phase.</p>"},{"location":"example_workflows/#use-case-2-production-causal-inference-workflow","title":"Use Case 2: Production Causal Inference Workflow","text":"<p>!!! warning \"Planned Feature\"     This production workflow configuration demonstrates planned features including <code>parameters</code>, <code>resources</code>, <code>monitoring</code>, <code>depends_on</code>, <code>inputs</code>, <code>outputs</code>, and streaming data integration. These are not yet implemented in the current schema.</p>"},{"location":"example_workflows/#scenario_4","title":"Scenario","text":"<p>A business wants to continuously analyze the causal impact of marketing campaigns on customer behavior using streaming data.</p>"},{"location":"example_workflows/#workflow-configuration","title":"Workflow Configuration","text":"<pre><code># production_inference_workflow.yaml\nmetadata:\n  name: \"marketing_causal_inference\"\n  version: \"2.1\"\n  description: \"Real-time causal inference for marketing effectiveness\"\n  environment: \"production\"\n\nparameters:\n  data_stream: \"kafka://marketing-events\"\n  batch_size: 10000\n  update_frequency: \"1h\"\n  lookback_window: \"30d\"\n\nresources:\n  dask_cluster: \"kubernetes://marketing-cluster\"\n  min_workers: 3\n  max_workers: 20\n  auto_scale: true\n  memory_per_worker: \"4GB\"\n\nmonitoring:\n  metrics_endpoint: \"prometheus://metrics\"\n  alert_threshold: 0.1\n  slack_notifications: true\n\nsteps:\n  - name: \"stream_ingestion\"\n    package: \"causaliq-data\"\n    method: \"ingest_stream\"\n    parameters:\n      source: ${parameters.data_stream}\n      batch_size: ${parameters.batch_size}\n      schema_validation: true\n    outputs: [\"raw_events\"]\n\n  - name: \"real_time_preprocessing\"\n    package: \"causaliq-data\"\n    method: \"preprocess_streaming\"\n    depends_on: [\"stream_ingestion\"]\n    parameters:\n      window_size: ${parameters.lookback_window}\n      features: [\"campaign_type\", \"customer_segment\", \"channel\", \"timestamp\"]\n      target: \"conversion\"\n    inputs: [\"raw_events\"]\n    outputs: [\"processed_batch\"]\n\n  - name: \"causal_graph_update\"\n    package: \"causaliq-discovery\"\n    method: \"incremental_learning\"\n    depends_on: [\"real_time_preprocessing\"]\n    parameters:\n      existing_graph: \"models/marketing_graph.pkl\"\n      update_threshold: 1000\n      stability_check: true\n    inputs: [\"processed_batch\"]\n    outputs: [\"updated_graph\", \"graph_changes\"]\n\n  - name: \"intervention_effects\"\n    package: \"causaliq-inference\"\n    method: \"estimate_ate\"\n    depends_on: [\"causal_graph_update\"]\n    parameters:\n      treatment_vars: [\"campaign_type\", \"channel\"]\n      outcome_var: \"conversion\"\n      adjustment_sets: \"auto\"\n    inputs: [\"updated_graph\", \"processed_batch\"]\n    outputs: [\"treatment_effects\", \"confidence_intervals\"]\n\n  - name: \"anomaly_detection\"\n    package: \"causaliq-monitoring\"\n    method: \"detect_effect_anomalies\"\n    depends_on: [\"intervention_effects\"]\n    parameters:\n      baseline_window: \"7d\"\n      anomaly_threshold: 2.0\n    inputs: [\"treatment_effects\"]\n    outputs: [\"anomalies\", \"alerts\"]\n\n  - name: \"automated_insights\"\n    package: \"causaliq-workflow\"\n    method: \"llm_generate_insights\"\n    depends_on: [\"intervention_effects\", \"anomaly_detection\"]\n    parameters:\n      context: \"marketing_optimization\"\n      include_recommendations: true\n      business_constraints: \"config/business_rules.yaml\"\n    inputs: [\"treatment_effects\", \"anomalies\", \"graph_changes\"]\n    outputs: [\"insights\", \"recommendations\"]\n\n  - name: \"dashboard_update\"\n    package: \"causaliq-visualization\"\n    method: \"update_dashboard\"\n    depends_on: [\"automated_insights\"]\n    parameters:\n      dashboard_id: \"marketing_causal_dashboard\"\n      auto_refresh: true\n    inputs: [\"treatment_effects\", \"insights\", \"recommendations\"]\n    outputs: [\"dashboard_url\"]\n\ntriggers:\n  schedule: \"0 * * * *\"  # Every hour\n  data_threshold: 5000   # Trigger if 5k new events\n\nfailure_handling:\n  retry_attempts: 3\n  fallback_to_previous: true\n  alert_on_failure: true\n</code></pre>"},{"location":"example_workflows/#use-case-3-interactive-research-exploration","title":"Use Case 3: Interactive Research Exploration","text":"<p>!!! warning \"Planned Feature\"     This interactive exploration workflow demonstrates planned features including interactive mode, LLM-assisted analysis, <code>user_interaction</code>, <code>state</code>, and iterative workflow refinement. These are not yet implemented in the current schema.</p>"},{"location":"example_workflows/#scenario_5","title":"Scenario","text":"<p>An interactive session where researchers explore causal relationships with LLM assistance and iterative refinement.</p>"},{"location":"example_workflows/#workflow-configuration_1","title":"Workflow Configuration","text":"<pre><code># interactive_exploration_workflow.yaml\nmetadata:\n  name: \"interactive_causal_exploration\"\n  version: \"1.0\"\n  description: \"LLM-assisted interactive causal discovery\"\n  mode: \"interactive\"\n\nparameters:\n  data_source: \"research/climate_data.csv\"\n  domain: \"climate_science\"\n  interaction_mode: \"jupyter\"\n\nresources:\n  dask_cluster: \"local\"\n  workers: 2\n  memory_per_worker: \"6GB\"\n\ninteraction:\n  llm_model: \"gpt-4\"\n  enable_suggestions: true\n  save_conversation: true\n\nsteps:\n  - name: \"initial_analysis\"\n    package: \"causaliq-data\"\n    method: \"exploratory_analysis\"\n    parameters:\n      generate_summary: true\n      correlation_analysis: true\n    outputs: [\"data_summary\", \"correlations\"]\n\n  - name: \"llm_initial_consultation\"\n    package: \"causaliq-workflow\"\n    method: \"llm_domain_consultation\"\n    depends_on: [\"initial_analysis\"]\n    parameters:\n      domain: ${parameters.domain}\n      task: \"What causal relationships should we investigate in this climate dataset?\"\n      context_data: true\n    inputs: [\"data_summary\", \"correlations\"]\n    outputs: [\"domain_insights\", \"suggested_relationships\"]\n    interactive: true\n\n  - name: \"guided_discovery\"\n    package: \"causaliq-discovery\"\n    method: \"guided_search\"\n    depends_on: [\"llm_initial_consultation\"]\n    parameters:\n      prior_knowledge: \"domain_insights\"\n      search_strategy: \"hypothesis_driven\"\n    inputs: [\"data_summary\", \"suggested_relationships\"]\n    outputs: [\"candidate_graphs\"]\n    interactive: true\n\n  - name: \"iterative_refinement\"\n    type: \"interactive_loop\"\n    depends_on: [\"guided_discovery\"]\n    max_iterations: 10\n    steps:\n      - name: \"graph_evaluation\"\n        package: \"causaliq-validation\"\n        method: \"evaluate_graph\"\n        inputs: [\"candidate_graphs\"]\n        outputs: [\"evaluation_metrics\"]\n\n      - name: \"llm_feedback\"\n        package: \"causaliq-workflow\"\n        method: \"llm_evaluate_graph\"\n        parameters:\n          domain: ${parameters.domain}\n          include_suggestions: true\n        inputs: [\"candidate_graphs\", \"evaluation_metrics\"]\n        outputs: [\"llm_feedback\", \"improvement_suggestions\"]\n        interactive: true\n\n      - name: \"user_decision\"\n        type: \"user_input\"\n        prompt: \"Based on the LLM feedback, would you like to: (1) Accept graph (2) Refine further (3) Try different approach?\"\n        outputs: [\"user_choice\"]\n\n      - name: \"conditional_refinement\"\n        type: \"conditional\"\n        condition: \"user_choice == 'refine'\"\n        package: \"causaliq-discovery\"\n        method: \"refine_graph\"\n        inputs: [\"candidate_graphs\", \"improvement_suggestions\"]\n        outputs: [\"refined_graphs\"]\n\n  - name: \"final_interpretation\"\n    package: \"causaliq-workflow\"\n    method: \"llm_comprehensive_interpretation\"\n    depends_on: [\"iterative_refinement\"]\n    parameters:\n      domain: ${parameters.domain}\n      include_limitations: true\n      suggest_experiments: true\n    inputs: [\"candidate_graphs\", \"evaluation_metrics\"]\n    outputs: [\"final_interpretation\", \"experiment_suggestions\"]\n\nnotebook_integration:\n  auto_generate_cells: true\n  include_visualizations: true\n  save_checkpoints: true\n</code></pre>"},{"location":"example_workflows/#example-cli-usage","title":"Example CLI Usage","text":"<pre><code># Execute a workflow\ncausaliq-workflow run workflow.yaml --config production.yaml\n\n# Validate workflow before execution\ncausaliq-workflow validate workflow.yaml\n\n# Interactive mode\ncausaliq-workflow interactive --domain healthcare --data patient_data.csv\n\n# Monitor running workflow\ncausaliq-workflow status workflow-123\n\n# Generate workflow template\ncausaliq-workflow template --type discovery --domain finance\n\n# List available packages and methods\ncausaliq-workflow list-methods --package causaliq-discovery\n</code></pre>"},{"location":"example_workflows/#integration-examples","title":"Integration Examples","text":""},{"location":"example_workflows/#python-api-usage","title":"Python API Usage","text":"<pre><code>from causaliq_workflow import WorkflowEngine, DaskClusterManager\n\n# Set up DASK cluster\ncluster_manager = DaskClusterManager()\nclient = cluster_manager.create_local_cluster(workers=4)\n\n# Initialize workflow engine\nengine = WorkflowEngine(client=client)\n\n# Load and execute workflow\nresult = engine.execute_workflow(\"workflow.yaml\")\n\n# Access results\ncausal_graph = result.get_output(\"ensemble_graph\")\ninterpretation = result.get_output(\"interpretation\")\n\n# Interactive LLM consultation\nllm_analyzer = engine.get_llm_analyzer()\ninsights = llm_analyzer.interpret_results(result.all_outputs, domain=\"healthcare\")\n</code></pre>"},{"location":"example_workflows/#jupyter-notebook-integration","title":"Jupyter Notebook Integration","text":"<pre><code>%%causaliq_workflow\nmetadata:\n  name: \"notebook_workflow\"\n\nsteps:\n  - name: \"discovery\"\n    package: \"causaliq-discovery\"\n    method: \"pc\"\n    parameters:\n      alpha: 0.05\n\n# Results automatically displayed in notebook\n</code></pre>"},{"location":"roadmap/","title":"CausalIQ Workflow - Development Roadmap","text":"<p>Last updated: February 18, 2026</p> <p>This project roadmap fits into the overall ecosystem roadmap</p>"},{"location":"roadmap/#under-development","title":"\ud83d\udea7  Under development","text":"<p>No release currently under development.</p>"},{"location":"roadmap/#previous-releases","title":"\u2705 Previous Releases","text":"<ul> <li> <p>v0.1.0 Workflow Foundations [February 2026]: Framework for plug-in actions, basic workflow and CLI support</p> </li> <li> <p>v0.2.0 Knowledge Workflows [February 2026]: Include LLM graph generation in workflows and store results in Workflow caches.</p> </li> </ul>"},{"location":"roadmap/#upcoming-implementation","title":"\ud83d\udee3\ufe0f Upcoming Implementation","text":""},{"location":"roadmap/#release-030-analysis-workflows","title":"Release 0.3.0 - Analysis Workflows","text":"<p>Graph averaging, structural analysis, and cache query capabilities.</p> <p>Scope:</p> <ul> <li>Cache read/scan functionality:</li> <li><code>cache_input</code> source for workflow steps</li> <li>Entry selection by matrix predicates (indexed lookup)</li> <li>Entry selection by metadata predicates (scan)</li> <li>Metadata update capability for enriching cached entries</li> <li>Graph averaging integrated (from causaliq-analysis)</li> <li>Structural evaluation integrated (from causaliq-analysis)</li> <li>Other analysis integrated</li> </ul> <p>Dependencies: Requires causaliq-analysis initial release</p>"},{"location":"roadmap/#release-040-enhanced-workflow","title":"Release 0.4.0: Enhanced workflow","text":"<p>Dry and comparison runs, runtime estimation and processing summary</p> <p>Scope:</p> <ul> <li>conservative execution skipping if results present</li> <li>dry-run capability</li> <li>standardise message format</li> <li>support skip, would do etc messages</li> <li>support comparison (integration test) functionality</li> <li>processing summary</li> <li>estimate runtime</li> <li>progress indicators</li> </ul>"},{"location":"roadmap/#release-05-discovery-integration","title":"Release 0.5: Discovery Integration","text":"<p>Structure learning algorithms integrated</p> <p>Scope:</p> <ul> <li>causaliq-discovery algorithms integrated</li> <li>timeout supported</li> </ul>"},{"location":"roadmap/#possible-future-features","title":"\ud83d\ude80 Possible Future Features","text":"<p>External Algorithm Integration (After robust test infrastructure):</p> <ul> <li>Multi-language workflows (R bnlearn, Java Tetrad, Python causal-learn)</li> <li>External CausalIQ package integration (discovery, analysis)</li> <li>Matrix-driven algorithm comparisons across datasets</li> <li>Automatic dataset download and preprocessing</li> </ul> <p>Production Features:**</p> <ul> <li>\ud83d\udccb Workflow queuing - CI-style runner management</li> <li>\ud83d\udcca Monitoring dashboard - Real-time execution tracking  </li> <li>\ud83d\uddfa Artifacts &amp; caching - Persistent storage, result reuse</li> <li>\ud83d\udd12 Security &amp; isolation - Secrets management, containers</li> <li>\ud83d\udcc8 Performance optimization - Resource limits, scheduling</li> </ul> <p>Research Platform:</p> <ul> <li>\ud83e\udd16 LLM integration - Model averaging, hypothesis generation</li> <li>\ud83c\udf10 Web interface - Browser-based workflow designer</li> <li>\ud83d\ude80 Cloud deployment - AWS/GCP/Azure runners</li> <li>\ud83d\udc65 Collaboration - Multi-researcher workflows</li> <li>\ud83d\udcda Publication workflows - Reproducible research outputs</li> </ul> <p>Advanced Capabilities:</p> <ul> <li>Workflow marketplace - Sharing and discovering research workflow templates</li> <li>Interactive notebooks - Jupyter integration with workflow execution</li> <li>Multi-machine execution - Distributed workflows across compute clusters</li> <li>AI-assisted optimization - Automated hyperparameter and workflow tuning</li> <li>Integration ecosystem - Plugins for major research tools and platforms</li> </ul> <p>This roadmap leverages Git commit history for completed work, provides detailed release-based planning for upcoming functionality, and outlines future possibilities.</p>"},{"location":"api/actions/","title":"Action Framework","text":"<p>The action framework provides the foundational classes for building reusable workflow components that follow GitHub Actions patterns.</p> <p>causaliq-workflow uses the <code>CausalIQActionProvider</code> base class from causaliq-core for all action implementations. This ensures consistency across the CausalIQ ecosystem and provides standardised interfaces for action validation, execution, and result handling.</p>"},{"location":"api/actions/#causaliq-core-integration","title":"causaliq-core Integration","text":"<p>causaliq-workflow imports the following from causaliq-core:</p> <ul> <li>CausalIQActionProvider - Abstract base class for all workflow actions</li> <li>ActionInput - Type-safe input specification for action parameters</li> <li>ActionResult - Tuple type for action return values (status, metadata,   objects)</li> <li>ActionValidationError - Exception for parameter validation failures</li> <li>ActionExecutionError - Exception for runtime execution failures</li> </ul>"},{"location":"api/actions/#core-classes","title":"Core Classes","text":""},{"location":"api/actions/#causaliq_corecausaliqactionprovider","title":"causaliq_core.CausalIQActionProvider","text":"<p>The base class for all action providers in the CausalIQ ecosystem.</p>"},{"location":"api/actions/#causaliq_core.CausalIQActionProvider","title":"CausalIQActionProvider","text":"<p>Base class for action providers that expose multiple workflow actions.</p> <p>Action providers group related actions together. Each provider must declare which actions it supports via the supported_actions attribute. The 'action' input parameter selects which action to execute.</p> <p>Providers can also declare supported_types for compress/decompress operations. Workflow uses these to build a registry of which provider handles compression for each data type.</p> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Provider identifier for workflow 'uses' field.</p> </li> <li> <code>version</code>               (<code>str</code>)           \u2013            <p>Provider version string.</p> </li> <li> <code>description</code>               (<code>str</code>)           \u2013            <p>Human-readable description.</p> </li> <li> <code>author</code>               (<code>str</code>)           \u2013            <p>Provider author/maintainer.</p> </li> <li> <code>supported_actions</code>               (<code>Set[str]</code>)           \u2013            <p>Set of action names this provider supports.</p> </li> <li> <code>supported_types</code>               (<code>Set[str]</code>)           \u2013            <p>Set of data types this provider can compress.</p> </li> <li> <code>inputs</code>               (<code>Dict[str, ActionInput]</code>)           \u2013            <p>Input parameter specifications.</p> </li> <li> <code>outputs</code>               (<code>Dict[str, str]</code>)           \u2013            <p>Output name to description mapping.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>compress</code>             \u2013              <p>Compress textual content to compact binary blob for cache storage.</p> </li> <li> <code>decompress</code>             \u2013              <p>Decompress compact binary blob back to textual content.</p> </li> <li> <code>run</code>             \u2013              <p>Execute the specified action with validated parameters.</p> </li> </ul>"},{"location":"api/actions/#causaliq_core.CausalIQActionProvider.compress","title":"compress","text":"<pre><code>compress(data_type: str, content: str, cache: TokenCache) -&gt; bytes\n</code></pre> <p>Compress textual content to compact binary blob for cache storage.</p> <p>Converts interchange format strings (e.g., GraphML, JSON) to compact binary blobs suitable for SQLite storage. Subclasses should override this to support their data types. The data_type must be declared in supported_types.</p> <p>Compression may use type-specific algorithms. For example, graphml compression parses the XML to extract edges as binary, and uses TokenCache to tokenise node names.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>bytes</code>           \u2013            <p>Compact binary blob representation.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>If the data type is not supported.</p> </li> </ul>"},{"location":"api/actions/#causaliq_core.CausalIQActionProvider.compress(data_type)","title":"<code>data_type</code>","text":"(<code>str</code>)           \u2013            <p>Type of data to compress (must be in supported_types).</p>"},{"location":"api/actions/#causaliq_core.CausalIQActionProvider.compress(content)","title":"<code>content</code>","text":"(<code>str</code>)           \u2013            <p>Textual interchange format string.</p>"},{"location":"api/actions/#causaliq_core.CausalIQActionProvider.compress(cache)","title":"<code>cache</code>","text":"(<code>TokenCache</code>)           \u2013            <p>TokenCache instance for tokenisation.</p>"},{"location":"api/actions/#causaliq_core.CausalIQActionProvider.decompress","title":"decompress","text":"<pre><code>decompress(data_type: str, blob: bytes, cache: TokenCache) -&gt; str\n</code></pre> <p>Decompress compact binary blob back to textual content.</p> <p>Converts compact binary blobs from cache storage back to interchange format strings. Subclasses should override this to support their data types. The data_type must be declared in supported_types.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Textual interchange format string.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>If the data type is not supported.</p> </li> </ul>"},{"location":"api/actions/#causaliq_core.CausalIQActionProvider.decompress(data_type)","title":"<code>data_type</code>","text":"(<code>str</code>)           \u2013            <p>Type of data to decompress (must be in supported_types).</p>"},{"location":"api/actions/#causaliq_core.CausalIQActionProvider.decompress(blob)","title":"<code>blob</code>","text":"(<code>bytes</code>)           \u2013            <p>Compact binary blob from cache.</p>"},{"location":"api/actions/#causaliq_core.CausalIQActionProvider.decompress(cache)","title":"<code>cache</code>","text":"(<code>TokenCache</code>)           \u2013            <p>TokenCache instance for detokenisation.</p>"},{"location":"api/actions/#causaliq_core.CausalIQActionProvider.run","title":"run  <code>abstractmethod</code>","text":"<pre><code>run(\n    action: str,\n    parameters: Dict[str, Any],\n    mode: str = \"dry-run\",\n    context: Optional[Any] = None,\n    logger: Optional[Any] = None,\n) -&gt; ActionResult\n</code></pre> <p>Execute the specified action with validated parameters.</p> <p>The action parameter specifies which action to execute. Implementations should validate that action is in supported_actions.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>ActionResult</code>           \u2013            <p>Tuple of (status, metadata, objects) where:</p> </li> <li> <code>ActionResult</code>           \u2013            <ul> <li>status: \"success\", \"skipped\", or \"error\"</li> </ul> </li> <li> <code>ActionResult</code>           \u2013            <ul> <li>metadata: Dictionary of execution metadata</li> </ul> </li> <li> <code>ActionResult</code>           \u2013            <ul> <li>objects: List of object dicts with keys:</li> <li>type: Data type (e.g., \"graphml\", \"json\")</li> <li>name: Object name identifier</li> <li>content: String content</li> </ul> </li> </ul> <p>Raises:</p> <ul> <li> <code>ActionExecutionError</code>             \u2013            <p>If action execution fails</p> </li> <li> <code>ActionValidationError</code>             \u2013            <p>If action is not supported</p> </li> </ul>"},{"location":"api/actions/#causaliq_core.CausalIQActionProvider.run(action)","title":"<code>action</code>","text":"(<code>str</code>)           \u2013            <p>Name of the action to execute (must be in supported_actions)</p>"},{"location":"api/actions/#causaliq_core.CausalIQActionProvider.run(parameters)","title":"<code>parameters</code>","text":"(<code>Dict[str, Any]</code>)           \u2013            <p>Dictionary of parameter values for the action</p>"},{"location":"api/actions/#causaliq_core.CausalIQActionProvider.run(mode)","title":"<code>mode</code>","text":"(<code>str</code>, default:                   <code>'dry-run'</code> )           \u2013            <p>Execution mode ('dry-run', 'run', 'compare')</p>"},{"location":"api/actions/#causaliq_core.CausalIQActionProvider.run(context)","title":"<code>context</code>","text":"(<code>Optional[Any]</code>, default:                   <code>None</code> )           \u2013            <p>Workflow context for optimisation and intelligence</p>"},{"location":"api/actions/#causaliq_core.CausalIQActionProvider.run(logger)","title":"<code>logger</code>","text":"(<code>Optional[Any]</code>, default:                   <code>None</code> )           \u2013            <p>Optional logger for task execution reporting</p>"},{"location":"api/actions/#causaliq_coreactioninput","title":"causaliq_core.ActionInput","text":""},{"location":"api/actions/#causaliq_core.ActionInput","title":"ActionInput  <code>dataclass</code>","text":"<pre><code>ActionInput(\n    name: str,\n    description: str,\n    required: bool = False,\n    default: Any = None,\n    type_hint: str = \"Any\",\n)\n</code></pre> <p>Define action input specification.</p> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Parameter name.</p> </li> <li> <code>description</code>               (<code>str</code>)           \u2013            <p>Human-readable description.</p> </li> <li> <code>required</code>               (<code>bool</code>)           \u2013            <p>Whether the parameter is required.</p> </li> <li> <code>default</code>               (<code>Any</code>)           \u2013            <p>Default value if not provided.</p> </li> <li> <code>type_hint</code>               (<code>str</code>)           \u2013            <p>Type hint string for documentation.</p> </li> </ul>"},{"location":"api/actions/#exception-handling","title":"Exception Handling","text":""},{"location":"api/actions/#causaliq_coreactionexecutionerror","title":"causaliq_core.ActionExecutionError","text":""},{"location":"api/actions/#causaliq_core.ActionExecutionError","title":"ActionExecutionError","text":"<p>Raised when action execution fails.</p>"},{"location":"api/actions/#causaliq_coreactionvalidationerror","title":"causaliq_core.ActionValidationError","text":""},{"location":"api/actions/#causaliq_core.ActionValidationError","title":"ActionValidationError","text":"<p>Raised when action input validation fails.</p>"},{"location":"api/actions/#quick-example","title":"Quick Example","text":"<pre><code>from typing import Any, Dict, Optional, Set, TYPE_CHECKING\n\nfrom causaliq_core import (\n    ActionInput,\n    ActionResult,\n    ActionValidationError,\n    CausalIQActionProvider,\n)\n\nif TYPE_CHECKING:\n    from causaliq_workflow.registry import WorkflowContext\n    from causaliq_workflow.logger import WorkflowLogger\n\n\nclass MyStructureLearnerAction(CausalIQActionProvider):\n    \"\"\"Custom structure learning action.\"\"\"\n\n    name = \"my-structure-learner\"\n    version = \"1.0.0\"\n    description = \"Custom causal structure learning implementation\"\n    author = \"CausalIQ\"\n\n    supported_actions: Set[str] = {\"learn_structure\", \"evaluate_graph\"}\n    supported_types: Set[str] = set()\n\n    inputs = {\n        \"action\": ActionInput(\n            name=\"action\",\n            description=\"Action to perform\",\n            required=True,\n            type_hint=\"str\",\n        ),\n        \"data_path\": ActionInput(\n            name=\"data_path\",\n            description=\"Path to input data file\",\n            required=True,\n            type_hint=\"str\",\n        ),\n        \"alpha\": ActionInput(\n            name=\"alpha\",\n            description=\"Significance level\",\n            required=False,\n            default=0.05,\n            type_hint=\"float\",\n        ),\n    }\n\n    outputs = {\n        \"graph_path\": \"Path to output GraphML file\",\n        \"node_count\": \"Number of nodes in learned graph\",\n        \"edge_count\": \"Number of edges in learned graph\",\n    }\n\n    def run(\n        self,\n        action: str,\n        parameters: Dict[str, Any],\n        mode: str = \"dry-run\",\n        context: Optional[Any] = None,\n        logger: Optional[Any] = None,\n    ) -&gt; ActionResult:\n        \"\"\"Execute the structure learning algorithm.\n\n        Args:\n            action: Name of action to execute (e.g., \"learn_structure\")\n            parameters: Action parameters (data_path, output_dir, etc.)\n            mode: Execution mode (\"dry-run\", \"run\", \"compare\")\n            context: Workflow context with matrix values and cache\n            logger: Optional logger for progress reporting\n\n        Returns:\n            ActionResult tuple (status, metadata, objects)\n        \"\"\"\n        if action not in self.supported_actions:\n            raise ActionValidationError(f\"Unknown action: {action}\")\n\n        # Dry-run mode: return metadata only\n        if mode == \"dry-run\":\n            return (\"skipped\", {\"dry_run\": True}, [])\n\n        # Your implementation here\n        metadata = {\n            \"graph_path\": \"/path/to/output.graphml\",\n            \"node_count\": 5,\n            \"edge_count\": 8,\n        }\n\n        # Objects list contains graph data\n        objects = [\n            {\n                \"type\": \"graphml\",\n                \"name\": \"graph\",\n                \"content\": \"&lt;graphml&gt;...&lt;/graphml&gt;\",\n            }\n        ]\n\n        return (\"success\", metadata, objects)\n</code></pre>"},{"location":"api/actions/#design-patterns","title":"Design Patterns","text":""},{"location":"api/actions/#action-implementation-guidelines","title":"Action Implementation Guidelines","text":"<ol> <li>Inherit from CausalIQActionProvider - The base class from causaliq-core    provides the standardised interface</li> <li>Define comprehensive inputs - Use ActionInput for type safety and    documentation</li> <li>Document outputs clearly - Help users understand action results</li> <li>Handle errors gracefully - Use ActionValidationError for parameter issues    and ActionExecutionError for runtime failures</li> <li>Follow semantic versioning - Enable workflow compatibility tracking</li> <li>Return ActionResult tuples - Status string, metadata dict, and objects    list</li> <li>Support dry-run mode - Return early with skipped status for validation</li> </ol>"},{"location":"api/actions/#testing-your-actions","title":"Testing Your Actions","text":"<pre><code>import pytest\n\nfrom causaliq_core import ActionValidationError\n\n\n# Test successful action execution.\ndef test_my_action_success() -&gt; None:\n    action = MyStructureLearnerAction()\n    parameters = {\n        \"data_path\": \"/path/to/test_data.csv\",\n        \"alpha\": 0.05,\n    }\n\n    status, metadata, objects = action.run(\n        \"learn_structure\", parameters, mode=\"run\"\n    )\n\n    assert status == \"success\"\n    assert \"node_count\" in metadata\n    assert \"edge_count\" in metadata\n\n\n# Test action validation with invalid action name.\ndef test_my_action_invalid_action() -&gt; None:\n    action = MyStructureLearnerAction()\n    parameters = {\"data_path\": \"/path/to/data.csv\"}\n\n    with pytest.raises(ActionValidationError):\n        action.run(\"invalid_action\", parameters, mode=\"run\")\n\n\n# Test dry-run mode returns skipped status.\ndef test_my_action_dry_run() -&gt; None:\n    action = MyStructureLearnerAction()\n    parameters = {\"data_path\": \"/path/to/data.csv\"}\n\n    status, metadata, objects = action.run(\n        \"learn_structure\", parameters, mode=\"dry-run\"\n    )\n\n    assert status == \"skipped\"\n    assert metadata.get(\"dry_run\") is True\n    assert objects == []\n</code></pre> <p>\u2190 Back to API Overview | Next: Action Registry \u2192</p>"},{"location":"api/cache/","title":"Workflow Cache API","text":"<p>The workflow cache provides SQLite-based storage for workflow step results, enabling conservative execution and reproducibility. It is built on causaliq-core's <code>TokenCache</code> infrastructure.</p>"},{"location":"api/cache/#causaliq-core-foundation","title":"causaliq-core Foundation","text":"<p>The workflow cache imports the following from causaliq-core:</p> <ul> <li>TokenCache - SQLite-based caching with tokenised JSON storage</li> <li>JsonCompressor - Compressor for JSON tokenisation</li> <li>Compressor - Abstract compressor interface</li> </ul>"},{"location":"api/cache/#core-classes","title":"Core Classes","text":""},{"location":"api/cache/#causaliq_workflowcacheworkflowcache","title":"causaliq_workflow.cache.WorkflowCache","text":""},{"location":"api/cache/#causaliq_workflow.cache.WorkflowCache","title":"WorkflowCache","text":"<pre><code>WorkflowCache(db_path: str | Path)\n</code></pre> <p>High-level cache for workflow step results.</p> <p>Provides a simplified interface for storing and retrieving workflow results as CacheEntry objects. Uses matrix variable values as cache keys, with SHA-256 hashing for compact storage.</p> <p>Each entry contains metadata and named objects (e.g., 'graph', 'confidences'), allowing a single workflow step to produce multiple outputs that are stored together.</p> <p>Attributes:</p> <ul> <li> <code>db_path</code>           \u2013            <p>Path to SQLite database file, or \":memory:\" for in-memory.</p> </li> </ul> Example <p>from causaliq_workflow.cache import WorkflowCache, CacheEntry with WorkflowCache(\":memory:\") as cache: ...     entry = CacheEntry() ...     entry.metadata[\"node_count\"] = 5 ...     entry.add_object(\"graph\", \"graphml\", \"...\") ...     key = {\"algorithm\": \"pc\", \"network\": \"asia\"} ...     cache.put(key, entry) ...     result = cache.get(key) ...     print(result.metadata) {'node_count': 5}</p> <p>Parameters:</p> <ul> <li> </li> </ul> <p>Methods:</p> <ul> <li> <code>open</code>             \u2013              <p>Open the database connection and initialise schema.</p> </li> <li> <code>close</code>             \u2013              <p>Close the database connection.</p> </li> <li> <code>put</code>             \u2013              <p>Store a workflow entry in the cache.</p> </li> <li> <code>get</code>             \u2013              <p>Retrieve a workflow entry from the cache.</p> </li> <li> <code>exists</code>             \u2013              <p>Check if a cache entry exists.</p> </li> <li> <code>entry_count</code>             \u2013              <p>Count cache entries.</p> </li> <li> <code>export</code>             \u2013              <p>Export cache entries to directory or zip file.</p> </li> <li> <code>import_entries</code>             \u2013              <p>Import cache entries from directory or zip file.</p> </li> </ul>"},{"location":"api/cache/#causaliq_workflow.cache.WorkflowCache(db_path)","title":"<code>db_path</code>","text":"(<code>str | Path</code>)           \u2013            <p>Path to SQLite database file. Use \":memory:\" for in-memory database (fast, non-persistent).</p>"},{"location":"api/cache/#causaliq_workflow.cache.WorkflowCache.open","title":"open","text":"<pre><code>open() -&gt; WorkflowCache\n</code></pre> <p>Open the database connection and initialise schema.</p> <p>Returns:</p> <ul> <li> <code>WorkflowCache</code>           \u2013            <p>self for method chaining.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>RuntimeError</code>             \u2013            <p>If already connected.</p> </li> </ul>"},{"location":"api/cache/#causaliq_workflow.cache.WorkflowCache.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the database connection.</p>"},{"location":"api/cache/#causaliq_workflow.cache.WorkflowCache.put","title":"put","text":"<pre><code>put(key_data: dict[str, Any], entry: CacheEntry) -&gt; str\n</code></pre> <p>Store a workflow entry in the cache.</p> <p>If an entry with the same key already exists, it is replaced.</p> <p>Validates that key_data uses the same matrix variable names as existing entries to ensure cache consistency.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>The hash key used for storage.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>MatrixSchemaError</code>             \u2013            <p>If key_data uses different variable names than existing entries.</p> </li> </ul> Example <p>with WorkflowCache(\":memory:\") as cache: ...     entry = CacheEntry() ...     entry.metadata[\"result\"] = \"ok\" ...     key = {\"algorithm\": \"pc\"} ...     hash_key = cache.put(key, entry)</p>"},{"location":"api/cache/#causaliq_workflow.cache.WorkflowCache.put(key_data)","title":"<code>key_data</code>","text":"(<code>dict[str, Any]</code>)           \u2013            <p>Dictionary of matrix variable values (cache key).</p>"},{"location":"api/cache/#causaliq_workflow.cache.WorkflowCache.put(entry)","title":"<code>entry</code>","text":"(<code>CacheEntry</code>)           \u2013            <p>CacheEntry containing metadata and objects.</p>"},{"location":"api/cache/#causaliq_workflow.cache.WorkflowCache.get","title":"get","text":"<pre><code>get(key_data: dict[str, Any]) -&gt; CacheEntry | None\n</code></pre> <p>Retrieve a workflow entry from the cache.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>CacheEntry | None</code>           \u2013            <p>CacheEntry if found, None otherwise.</p> </li> </ul> Example <p>with WorkflowCache(\":memory:\") as cache: ...     entry = CacheEntry() ...     entry.metadata[\"result\"] = \"ok\" ...     cache.put({\"algo\": \"pc\"}, entry) ...     result = cache.get({\"algo\": \"pc\"}) ...     print(result.metadata) {'result': 'ok'}</p>"},{"location":"api/cache/#causaliq_workflow.cache.WorkflowCache.get(key_data)","title":"<code>key_data</code>","text":"(<code>dict[str, Any]</code>)           \u2013            <p>Dictionary of matrix variable values (cache key).</p>"},{"location":"api/cache/#causaliq_workflow.cache.WorkflowCache.exists","title":"exists","text":"<pre><code>exists(key_data: dict[str, Any]) -&gt; bool\n</code></pre> <p>Check if a cache entry exists.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if entry exists, False otherwise.</p> </li> </ul> Example <p>with WorkflowCache(\":memory:\") as cache: ...     cache.exists({\"algo\": \"pc\"})  # False ...     cache.put({\"algo\": \"pc\"}, CacheEntry()) ...     cache.exists({\"algo\": \"pc\"})  # True</p>"},{"location":"api/cache/#causaliq_workflow.cache.WorkflowCache.exists(key_data)","title":"<code>key_data</code>","text":"(<code>dict[str, Any]</code>)           \u2013            <p>Dictionary of matrix variable values (cache key).</p>"},{"location":"api/cache/#causaliq_workflow.cache.WorkflowCache.entry_count","title":"entry_count","text":"<pre><code>entry_count() -&gt; int\n</code></pre> <p>Count cache entries.</p> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>Number of entries in the cache.</p> </li> </ul>"},{"location":"api/cache/#causaliq_workflow.cache.WorkflowCache.export","title":"export","text":"<pre><code>export(output_path: str | Path, matrix_keys: list[str] | None = None) -&gt; int\n</code></pre> <p>Export cache entries to directory or zip file.</p> <p>Creates a hierarchical directory structure based on matrix variable values. Each entry's objects are written as individual files.</p> <p>The output format is determined by the path extension: - Path ending in .zip: creates a zip archive - Otherwise: creates a directory structure</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>Number of entries exported.</p> </li> </ul> Example <p>with WorkflowCache(\"cache.db\") as cache: ...     # Export to dir: asia/pc/graph.graphml ...     cache.export(\"./out\", [\"dataset\", \"algorithm\"]) ...     # Export to zip file ...     cache.export(\"./out.zip\", [\"dataset\"])</p>"},{"location":"api/cache/#causaliq_workflow.cache.WorkflowCache.export(output_path)","title":"<code>output_path</code>","text":"(<code>str | Path</code>)           \u2013            <p>Path to output directory or .zip file.</p>"},{"location":"api/cache/#causaliq_workflow.cache.WorkflowCache.export(matrix_keys)","title":"<code>matrix_keys</code>","text":"(<code>list[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Ordered list of matrix variable names for directory hierarchy. If None, uses alphabetical order.</p>"},{"location":"api/cache/#causaliq_workflow.cache.WorkflowCache.import_entries","title":"import_entries","text":"<pre><code>import_entries(input_path: str | Path) -&gt; int\n</code></pre> <p>Import cache entries from directory or zip file.</p> <p>Reads entries exported by <code>export()</code> and stores them back into the cache. The input format is determined by the path: - Path ending in .zip: reads from a zip archive - Otherwise: reads from a directory structure</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>Number of entries imported.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FileNotFoundError</code>             \u2013            <p>If input_path does not exist.</p> </li> </ul> Example <p>with WorkflowCache(\"cache.db\") as cache: ...     # Import from directory ...     cache.import_entries(\"./exported\") ...     # Import from zip file ...     cache.import_entries(\"./exported.zip\")</p>"},{"location":"api/cache/#causaliq_workflow.cache.WorkflowCache.import_entries(input_path)","title":"<code>input_path</code>","text":"(<code>str | Path</code>)           \u2013            <p>Path to input directory or .zip file.</p>"},{"location":"api/cache/#causaliq_workflowcachecacheentry","title":"causaliq_workflow.cache.CacheEntry","text":""},{"location":"api/cache/#causaliq_workflow.cache.CacheEntry","title":"CacheEntry  <code>dataclass</code>","text":"<pre><code>CacheEntry(metadata: Dict[str, Any] = dict(), objects: Dict[str, CacheObject] = dict())\n</code></pre> <p>A cached workflow result containing metadata and named objects.</p> <p>Represents a single cache entry identified by matrix variable values. Contains workflow metadata and zero or more named objects, each with a type and content.</p> <p>The entry structure maps directly to TokenCache storage: - metadata \u2192 TokenCache metadata field - objects \u2192 TokenCache data field</p> <p>Attributes:</p> <ul> <li> <code>metadata</code>               (<code>Dict[str, Any]</code>)           \u2013            <p>Workflow metadata dictionary.</p> </li> <li> <code>objects</code>               (<code>Dict[str, CacheObject]</code>)           \u2013            <p>Named objects dictionary (name \u2192 CacheObject).</p> </li> </ul> Example <p>entry = CacheEntry() entry.metadata[\"node_count\"] = 5 entry.objects[\"graph\"] = CacheObject( ...     type=\"graphml\", ...     content=\"...\" ... ) entry.objects[\"confidences\"] = CacheObject( ...     type=\"json\", ...     content='{\"A-&gt;B\": 0.95}' ... )</p> <p>Methods:</p> <ul> <li> <code>add_object</code>             \u2013              <p>Add or replace a named object.</p> </li> <li> <code>from_action_result</code>             \u2013              <p>Create from action result format.</p> </li> <li> <code>from_storage</code>             \u2013              <p>Create from TokenCache storage format.</p> </li> <li> <code>get_object</code>             \u2013              <p>Get a named object.</p> </li> <li> <code>has_object</code>             \u2013              <p>Check if a named object exists.</p> </li> <li> <code>object_names</code>             \u2013              <p>Get list of object names.</p> </li> <li> <code>remove_object</code>             \u2013              <p>Remove a named object.</p> </li> <li> <code>to_action_result</code>             \u2013              <p>Convert to action result format.</p> </li> <li> <code>to_storage</code>             \u2013              <p>Convert to storage format for TokenCache.</p> </li> </ul>"},{"location":"api/cache/#causaliq_workflow.cache.CacheEntry.add_object","title":"add_object","text":"<pre><code>add_object(name: str, obj_type: str, content: Any) -&gt; None\n</code></pre> <p>Add or replace a named object.</p> <p>Parameters:</p> Example <p>entry = CacheEntry() entry.add_object(\"graph\", \"graphml\", \"...\")"},{"location":"api/cache/#causaliq_workflow.cache.CacheEntry.add_object(name)","title":"<code>name</code>","text":"(<code>str</code>)           \u2013            <p>Object name (e.g., 'graph', 'confidences').</p>"},{"location":"api/cache/#causaliq_workflow.cache.CacheEntry.add_object(obj_type)","title":"<code>obj_type</code>","text":"(<code>str</code>)           \u2013            <p>Object type (e.g., 'graphml', 'json').</p>"},{"location":"api/cache/#causaliq_workflow.cache.CacheEntry.add_object(content)","title":"<code>content</code>","text":"(<code>Any</code>)           \u2013            <p>Object content.</p>"},{"location":"api/cache/#causaliq_workflow.cache.CacheEntry.from_action_result","title":"from_action_result  <code>classmethod</code>","text":"<pre><code>from_action_result(\n    metadata: Dict[str, Any], objects: list[Dict[str, Any]]\n) -&gt; CacheEntry\n</code></pre> <p>Create from action result format.</p> <p>Converts the current action return format (metadata dict and objects list) to a CacheEntry.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>CacheEntry</code>           \u2013            <p>CacheEntry instance.</p> </li> </ul> Example <p>entry = CacheEntry.from_action_result( ...     {\"node_count\": 5}, ...     [{\"type\": \"graphml\", \"name\": \"graph\", \"content\": \"...\"}] ... )</p>"},{"location":"api/cache/#causaliq_workflow.cache.CacheEntry.from_action_result(metadata)","title":"<code>metadata</code>","text":"(<code>Dict[str, Any]</code>)           \u2013            <p>Action metadata dictionary.</p>"},{"location":"api/cache/#causaliq_workflow.cache.CacheEntry.from_action_result(objects)","title":"<code>objects</code>","text":"(<code>list[Dict[str, Any]]</code>)           \u2013            <p>List of object dicts with 'type', 'name', 'content'.</p>"},{"location":"api/cache/#causaliq_workflow.cache.CacheEntry.from_storage","title":"from_storage  <code>classmethod</code>","text":"<pre><code>from_storage(\n    data: Dict[str, Any] | None, metadata: Dict[str, Any] | None\n) -&gt; CacheEntry\n</code></pre> <p>Create from TokenCache storage format.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>CacheEntry</code>           \u2013            <p>CacheEntry instance.</p> </li> </ul>"},{"location":"api/cache/#causaliq_workflow.cache.CacheEntry.from_storage(data)","title":"<code>data</code>","text":"(<code>Dict[str, Any] | None</code>)           \u2013            <p>Objects dict from TokenCache.get_data().</p>"},{"location":"api/cache/#causaliq_workflow.cache.CacheEntry.from_storage(metadata)","title":"<code>metadata</code>","text":"(<code>Dict[str, Any] | None</code>)           \u2013            <p>Metadata dict from TokenCache.</p>"},{"location":"api/cache/#causaliq_workflow.cache.CacheEntry.get_object","title":"get_object","text":"<pre><code>get_object(name: str) -&gt; CacheObject | None\n</code></pre> <p>Get a named object.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>CacheObject | None</code>           \u2013            <p>CacheObject if found, None otherwise.</p> </li> </ul>"},{"location":"api/cache/#causaliq_workflow.cache.CacheEntry.get_object(name)","title":"<code>name</code>","text":"(<code>str</code>)           \u2013            <p>Object name to retrieve.</p>"},{"location":"api/cache/#causaliq_workflow.cache.CacheEntry.has_object","title":"has_object","text":"<pre><code>has_object(name: str) -&gt; bool\n</code></pre> <p>Check if a named object exists.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if object exists.</p> </li> </ul>"},{"location":"api/cache/#causaliq_workflow.cache.CacheEntry.has_object(name)","title":"<code>name</code>","text":"(<code>str</code>)           \u2013            <p>Object name to check.</p>"},{"location":"api/cache/#causaliq_workflow.cache.CacheEntry.object_names","title":"object_names","text":"<pre><code>object_names() -&gt; list[str]\n</code></pre> <p>Get list of object names.</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>List of object names in the entry.</p> </li> </ul>"},{"location":"api/cache/#causaliq_workflow.cache.CacheEntry.remove_object","title":"remove_object","text":"<pre><code>remove_object(name: str) -&gt; bool\n</code></pre> <p>Remove a named object.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if object was removed, False if not found.</p> </li> </ul>"},{"location":"api/cache/#causaliq_workflow.cache.CacheEntry.remove_object(name)","title":"<code>name</code>","text":"(<code>str</code>)           \u2013            <p>Object name to remove.</p>"},{"location":"api/cache/#causaliq_workflow.cache.CacheEntry.to_action_result","title":"to_action_result","text":"<pre><code>to_action_result() -&gt; tuple[Dict[str, Any], list[Dict[str, Any]]]\n</code></pre> <p>Convert to action result format.</p> <p>Returns:</p> <ul> <li> <code>tuple[Dict[str, Any], list[Dict[str, Any]]]</code>           \u2013            <p>Tuple of (metadata, objects_list) matching action return format.</p> </li> </ul>"},{"location":"api/cache/#causaliq_workflow.cache.CacheEntry.to_storage","title":"to_storage","text":"<pre><code>to_storage() -&gt; tuple[Dict[str, Any], Dict[str, Any]]\n</code></pre> <p>Convert to storage format for TokenCache.</p> <p>Returns:</p> <ul> <li> <code>Dict[str, Any]</code>           \u2013            <p>Tuple of (data, metadata) for TokenCache.put_data().</p> </li> <li> <code>Dict[str, Any]</code>           \u2013            <ul> <li>data: Objects dict serialised to dicts</li> </ul> </li> <li> <code>tuple[Dict[str, Any], Dict[str, Any]]</code>           \u2013            <ul> <li>metadata: Entry metadata dict</li> </ul> </li> </ul>"},{"location":"api/cache/#causaliq_workflowcachecacheobject","title":"causaliq_workflow.cache.CacheObject","text":""},{"location":"api/cache/#causaliq_workflow.cache.CacheObject","title":"CacheObject  <code>dataclass</code>","text":"<pre><code>CacheObject(type: str, content: Any)\n</code></pre> <p>A named object within a cache entry.</p> <p>Represents a single piece of data with a type identifier used for serialisation and export (e.g., graphml, json).</p> <p>Attributes:</p> <ul> <li> <code>type</code>               (<code>str</code>)           \u2013            <p>Object type identifier (e.g., 'graphml', 'json').</p> </li> <li> <code>content</code>               (<code>Any</code>)           \u2013            <p>The object content (string for serialised formats).</p> </li> </ul> Example <p>obj = CacheObject(type=\"graphml\", content=\"...\") obj.type 'graphml'</p> <p>Methods:</p> <ul> <li> <code>from_dict</code>             \u2013              <p>Create from dictionary.</p> </li> <li> <code>to_dict</code>             \u2013              <p>Convert to dictionary for serialisation.</p> </li> </ul>"},{"location":"api/cache/#causaliq_workflow.cache.CacheObject.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: Dict[str, Any]) -&gt; CacheObject\n</code></pre> <p>Create from dictionary.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>CacheObject</code>           \u2013            <p>CacheObject instance.</p> </li> </ul>"},{"location":"api/cache/#causaliq_workflow.cache.CacheObject.from_dict(data)","title":"<code>data</code>","text":"(<code>Dict[str, Any]</code>)           \u2013            <p>Dictionary with 'type' and 'content' keys.</p>"},{"location":"api/cache/#causaliq_workflow.cache.CacheObject.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert to dictionary for serialisation.</p> <p>Returns:</p> <ul> <li> <code>Dict[str, Any]</code>           \u2013            <p>Dictionary with 'type' and 'content' keys.</p> </li> </ul>"},{"location":"api/cache/#exception-handling","title":"Exception Handling","text":""},{"location":"api/cache/#causaliq_workflowcachematrixschemaerror","title":"causaliq_workflow.cache.MatrixSchemaError","text":""},{"location":"api/cache/#causaliq_workflow.cache.MatrixSchemaError","title":"MatrixSchemaError","text":"<p>Raised when matrix variable keys don't match existing cache entries.</p> <p>Once a cache contains entries, all subsequent entries must use the same matrix variable names. This ensures cache consistency and prevents accidental data corruption from mismatched workflows.</p> Example"},{"location":"api/cache/#causaliq_workflow.cache.MatrixSchemaError--cache-has-entries-with-keys-algorithm-dataset","title":"Cache has entries with keys {\"algorithm\", \"dataset\"}","text":""},{"location":"api/cache/#causaliq_workflow.cache.MatrixSchemaError--trying-to-add-entry-with-algorithm-network-raises","title":"Trying to add entry with {\"algorithm\", \"network\"} raises","text":"<p>raise MatrixSchemaError( ...     \"Matrix keys mismatch: got {'algorithm', 'network'}, \" ...     \"expected {'algorithm', 'dataset'}\" ... )</p>"},{"location":"api/cache/#usage-examples","title":"Usage Examples","text":""},{"location":"api/cache/#basic-cache-operations","title":"Basic Cache Operations","text":"<pre><code>from causaliq_workflow.cache import WorkflowCache, CacheEntry\n\n# Create and use cache with context manager\nwith WorkflowCache(\"results/experiment.db\") as cache:\n    # Create an entry with metadata\n    entry = CacheEntry()\n    entry.metadata[\"algorithm\"] = \"pc\"\n    entry.metadata[\"alpha\"] = 0.05\n    entry.metadata[\"node_count\"] = 8\n\n    # Add a graph object\n    graphml_content = '''&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n    &lt;graphml xmlns=\"http://graphml.graphdrawing.org/xmlns\"&gt;\n      &lt;graph id=\"G\" edgedefault=\"directed\"&gt;\n        &lt;node id=\"A\"/&gt;&lt;node id=\"B\"/&gt;\n        &lt;edge source=\"A\" target=\"B\"/&gt;\n      &lt;/graph&gt;\n    &lt;/graphml&gt;'''\n    entry.add_object(\"graph\", \"graphml\", graphml_content)\n\n    # Store with matrix key\n    key = {\"network\": \"asia\", \"algorithm\": \"pc\", \"sample_size\": \"1000\"}\n    cache.put(key, entry)\n\n    # Check existence\n    if cache.exists(key):\n        print(\"Entry found in cache\")\n\n    # Retrieve entry\n    result = cache.get(key)\n    print(f\"Algorithm: {result.metadata['algorithm']}\")\n</code></pre>"},{"location":"api/cache/#matrix-key-hashing","title":"Matrix Key Hashing","text":"<p>The cache uses SHA-256 hashing of matrix variable values as keys:</p> <pre><code>from causaliq_workflow.cache import WorkflowCache\n\nwith WorkflowCache(\":memory:\") as cache:\n    # These keys produce different hashes\n    key1 = {\"algorithm\": \"pc\", \"network\": \"asia\"}\n    key2 = {\"algorithm\": \"ges\", \"network\": \"asia\"}\n\n    # Key order doesn't matter - sorted before hashing\n    key3 = {\"network\": \"asia\", \"algorithm\": \"pc\"}  # Same hash as key1\n</code></pre>"},{"location":"api/cache/#matrix-schema-validation","title":"Matrix Schema Validation","text":"<p>Once a cache contains entries, all subsequent entries must use the same matrix variable names:</p> <pre><code>from causaliq_workflow.cache import WorkflowCache, CacheEntry, MatrixSchemaError\n\nwith WorkflowCache(\":memory:\") as cache:\n    entry = CacheEntry()\n\n    # First entry establishes schema\n    cache.put({\"algorithm\": \"pc\", \"network\": \"asia\"}, entry)\n\n    # This raises MatrixSchemaError - wrong keys\n    try:\n        cache.put({\"method\": \"pc\", \"dataset\": \"asia\"}, entry)\n    except MatrixSchemaError as e:\n        print(f\"Schema error: {e}\")\n</code></pre>"},{"location":"api/cache/#export-and-import","title":"Export and Import","text":"<pre><code>from causaliq_workflow.cache import WorkflowCache\nfrom pathlib import Path\n\n# Export cache to directory\nwith WorkflowCache(\"experiment.db\") as cache:\n    exported = cache.export(Path(\"./exported\"))\n    print(f\"Exported {exported} entries\")\n\n# Export to zip file\nwith WorkflowCache(\"experiment.db\") as cache:\n    exported = cache.export(Path(\"./results.zip\"))\n\n# Import from directory\nwith WorkflowCache(\"new_cache.db\") as cache:\n    imported = cache.import_entries(Path(\"./exported\"))\n    print(f\"Imported {imported} entries\")\n</code></pre>"},{"location":"api/cache/#in-memory-cache","title":"In-Memory Cache","text":"<p>Use <code>:memory:</code> for fast, non-persistent caching:</p> <pre><code>from causaliq_workflow.cache import WorkflowCache, CacheEntry\n\n# In-memory cache for testing\nwith WorkflowCache(\":memory:\") as cache:\n    entry = CacheEntry()\n    entry.metadata[\"test\"] = True\n\n    cache.put({\"key\": \"value\"}, entry)\n    assert cache.entry_count() == 1\n# Cache automatically closed and discarded\n</code></pre>"},{"location":"api/cache/#architecture-notes","title":"Architecture Notes","text":"<p>The WorkflowCache wraps causaliq-core's TokenCache with workflow-specific functionality:</p> <ul> <li>Matrix key hashing - SHA-256 hash of sorted matrix values (16 hex chars)</li> <li>Schema validation - Ensures consistent matrix variable names across   entries</li> <li>Entry model - CacheEntry with metadata dict and named objects list</li> <li>Export/Import - Convert entries to/from open standard formats (GraphML,   JSON)</li> </ul> <p>The cache design focuses on:</p> <ul> <li>Speed - Fast existence checks and lookups via hash keys</li> <li>Compactness - JSON tokenisation reduces storage size</li> <li>Reproducibility - Entries can be exported to human-readable formats</li> </ul> <p>\u2190 Previous: Workflow Engine | Back to API Overview | Next: Schema Validation \u2192</p>"},{"location":"api/cli/","title":"CLI Interface","text":"<p>The command-line interface provides direct workflow execution and cache management capabilities.</p>"},{"location":"api/cli/#command-overview","title":"Command Overview","text":"<p>causaliq-workflow provides three main commands:</p> <ul> <li>cqflow run - Execute workflow files</li> <li>cqflow export_cache - Export cache entries to directory or zip file</li> <li>cqflow import_cache - Import cache entries from directory or zip file</li> </ul> <p><code>cqflow</code> is an alias for <code>causaliq-workflow</code>.</p>"},{"location":"api/cli/#core-interface","title":"Core Interface","text":""},{"location":"api/cli/#causaliq_workflowcli","title":"causaliq_workflow.cli","text":""},{"location":"api/cli/#causaliq_workflow.cli","title":"cli","text":"<p>Command-line interface for causaliq-workflow.</p> <p>Functions:</p> <ul> <li> <code>cli</code>             \u2013              <p>CausalIQ Workflow - Execute and manage causal discovery workflows.</p> </li> <li> <code>export_cache</code>             \u2013              <p>Export cache entries to directory or zip file.</p> </li> <li> <code>import_cache</code>             \u2013              <p>Import cache entries from directory or zip file.</p> </li> <li> <code>main</code>             \u2013              <p>Entry point for the CLI.</p> </li> <li> <code>run_workflow</code>             \u2013              <p>Execute a CausalIQ workflow file.</p> </li> </ul>"},{"location":"api/cli/#causaliq_workflow.cli.cli","title":"cli","text":"<pre><code>cli(ctx: Context) -&gt; None\n</code></pre> <p>CausalIQ Workflow - Execute and manage causal discovery workflows.</p> <p>Use 'cqwork run' to execute workflows. Use 'cqwork export_cache' to export cache entries. Use 'cqwork import_cache' to import cache entries.</p>"},{"location":"api/cli/#causaliq_workflow.cli.export_cache","title":"export_cache","text":"<pre><code>export_cache(cache_file: Path, output: Path) -&gt; None\n</code></pre> <p>Export cache entries to directory or zip file.</p> <p>Reads all entries from a WorkflowCache database and exports them to a hierarchical directory structure based on matrix variable values. Each entry's objects are exported to separate files.</p> <p>Examples:</p> <pre><code>cqflow export_cache -c cache.db -o ./results\n\ncqflow export_cache -c cache.db -o results.zip\n</code></pre>"},{"location":"api/cli/#causaliq_workflow.cli.import_cache","title":"import_cache","text":"<pre><code>import_cache(input_path: Path, cache_file: Path) -&gt; None\n</code></pre> <p>Import cache entries from directory or zip file.</p> <p>Reads entries previously exported by 'export_cache' and stores them into the specified cache database. Creates the cache file if it does not exist.</p> <p>Examples:</p> <pre><code>cqwork import_cache -i ./results -c cache.db\n\ncqwork import_cache -i results.zip -c cache.db\n</code></pre>"},{"location":"api/cli/#causaliq_workflow.cli.main","title":"main","text":"<pre><code>main() -&gt; None\n</code></pre> <p>Entry point for the CLI.</p>"},{"location":"api/cli/#causaliq_workflow.cli.run_workflow","title":"run_workflow","text":"<pre><code>run_workflow(workflow_file: Path, mode: str, log_level: str) -&gt; None\n</code></pre> <p>Execute a CausalIQ workflow file.</p> <p>WORKFLOW_FILE is the path to a YAML workflow file to execute.</p> <p>Examples:</p> <pre><code>causaliq-workflow run experiment.yml\n\ncausaliq-workflow run experiment.yml --mode=run\n\ncausaliq-workflow run experiment.yml --mode=dry-run --log-level=all\n</code></pre>"},{"location":"api/cli/#command-line-usage","title":"Command Line Usage","text":""},{"location":"api/cli/#run-command","title":"Run Command","text":"<p>Execute a CausalIQ workflow file:</p> <pre><code># Execute a workflow in dry-run mode (validate and preview - default)\ncqflow run experiment.yml\n\n# Execute a workflow in run mode (actually execute actions)\ncqflow run experiment.yml --mode=run\n\n# Set logging level\ncqflow run experiment.yml --mode=run --log-level=all\n</code></pre> <p>Options:</p> <ul> <li><code>--mode</code> - Execution mode: <code>dry-run</code> (default) or <code>run</code></li> <li><code>--log-level</code> - Logging level: <code>none</code>, <code>summary</code> (default), or <code>all</code></li> </ul>"},{"location":"api/cli/#export-cache-command","title":"Export Cache Command","text":"<p>Export cache entries to a directory or zip file:</p> <pre><code># Export to directory\ncqflow export_cache -c results.db -o ./exported\n\n# Export to zip file\ncqflow export_cache -c results.db -o results.zip\n</code></pre> <p>Options:</p> <ul> <li><code>-c, --cache</code> - Path to WorkflowCache database file (.db)</li> <li><code>-o, --output</code> - Output directory or .zip file path</li> </ul>"},{"location":"api/cli/#import-cache-command","title":"Import Cache Command","text":"<p>Import cache entries from a previously exported directory or zip:</p> <pre><code># Import from directory\ncqflow import_cache -i ./exported -c new_cache.db\n\n# Import from zip file\ncqflow import_cache -i results.zip -c new_cache.db\n</code></pre> <p>Options:</p> <ul> <li><code>-i, --input</code> - Path to exported directory or .zip file</li> <li><code>-c, --cache</code> - Destination WorkflowCache database file (.db)</li> </ul>"},{"location":"api/cli/#cicd-integration","title":"CI/CD Integration","text":""},{"location":"api/cli/#github-actions-integration","title":"GitHub Actions Integration","text":"<pre><code>name: Execute Causal Discovery Workflow\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  validate:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v3\n    - uses: actions/setup-python@v4\n      with:\n        python-version: '3.11'\n\n    - name: Install dependencies\n      run: |\n        pip install causaliq-workflow\n\n    - name: Validate workflows (dry-run)\n      run: |\n        cqflow run workflows/experiment.yml --mode=dry-run\n\n  execute:\n    needs: validate\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v3\n    - uses: actions/setup-python@v4\n      with:\n        python-version: '3.11'\n\n    - name: Install dependencies\n      run: |\n        pip install causaliq-workflow\n\n    - name: Execute workflow\n      run: |\n        cqflow run workflows/experiment.yml --mode=run --log-level=all\n\n    - name: Export results\n      run: |\n        cqflow export_cache -c results.db -o results.zip\n\n    - name: Upload results\n      uses: actions/upload-artifact@v3\n      with:\n        name: workflow-results\n        path: results.zip\n</code></pre>"},{"location":"api/cli/#usage-patterns","title":"Usage Patterns","text":""},{"location":"api/cli/#batch-processing","title":"Batch Processing","text":"<pre><code># Execute multiple workflows sequentially\nfor workflow in workflows/*.yml; do\n    echo \"Executing $workflow...\"\n    cqflow run \"$workflow\" --mode=run\ndone\n</code></pre>"},{"location":"api/cli/#export-and-share-results","title":"Export and Share Results","text":"<pre><code># Export cache to share with collaborators\ncqflow export_cache -c experiment.db -o results.zip\n\n# Import shared results into local cache\ncqflow import_cache -i results.zip -c local_cache.db\n</code></pre>"},{"location":"api/cli/#exit-codes","title":"Exit Codes","text":"<p>The CLI uses standard exit codes:</p> <ul> <li>0 - Success</li> <li>1 - General error (validation, execution failure)</li> <li>130 - Interrupted by user (Ctrl+C)</li> </ul> <p>\u2190 Previous: Logging System | Back to API Overview | Next: Examples \u2192</p>"},{"location":"api/examples/","title":"Usage Examples","text":"<p>Comprehensive examples demonstrating common patterns and advanced usage of the CausalIQ Workflow framework.</p>"},{"location":"api/examples/#basic-workflows","title":"Basic Workflows","text":""},{"location":"api/examples/#simple-action-execution","title":"Simple Action Execution","text":"<pre><code>from causaliq_workflow import WorkflowExecutor\n\n# Create and execute a basic workflow\nexecutor = WorkflowExecutor()\n\n# Define workflow programmatically\nsimple_workflow = {\n    \"id\": \"simple-experiment\",\n    \"description\": \"Basic structure learning experiment\",\n    \"steps\": [\n        {\n            \"name\": \"Learn Structure\", \n            \"uses\": \"structure-learner\",\n            \"with\": {\n                \"data_path\": \"/data/asia.csv\",\n                \"output_path\": \"/results/asia_structure.graphml\",\n                \"algorithm\": \"pc\",\n                \"alpha\": 0.05\n            }\n        }\n    ]\n}\n\n# Execute workflow\nresults = executor.execute_workflow(simple_workflow, mode=\"run\")\nprint(f\"Workflow completed: {results}\")\n</code></pre>"},{"location":"api/examples/#loading-from-yaml","title":"Loading from YAML","text":"<pre><code># experiments/basic-experiment.yml\nid: \"basic-experiment\"\ndescription: \"Structure learning with PC algorithm\"\ndata_root: \"/experiments/data\"\noutput_root: \"/experiments/results\"\n\nsteps:\n  - name: \"PC Structure Learning\"\n    uses: \"pc-learner\"\n    with:\n      data_path: \"{{data_root}}/asia.csv\"\n      output_path: \"{{output_root}}/{{id}}/structure.graphml\"\n      alpha: 0.05\n      max_depth: 3\n</code></pre> <pre><code># Execute YAML workflow\nworkflow = executor.parse_workflow(\"experiments/basic-experiment.yml\")\nresults = executor.execute_workflow(workflow, mode=\"run\")\n</code></pre>"},{"location":"api/examples/#matrix-workflows","title":"Matrix Workflows","text":""},{"location":"api/examples/#parameter-sweeps","title":"Parameter Sweeps","text":"<pre><code># experiments/parameter-sweep.yml\nid: \"parameter-sweep\"\ndescription: \"Multi-algorithm parameter sweep\"\ndata_root: \"/experiments/data\"\noutput_root: \"/experiments/results\"\n\nmatrix:\n  dataset: [\"asia\", \"cancer\", \"earthquake\"]\n  algorithm: [\"pc\", \"ges\", \"lingam\"]\n  alpha: [0.01, 0.05, 0.1]\n\nsteps:\n  - name: \"Structure Learning\"\n    uses: \"structure-learner\"\n    with:\n      data_path: \"{{data_root}}/{{dataset}}.csv\"\n      output_path: \"{{output_root}}/{{id}}/{{dataset}}_{{algorithm}}_{{alpha}}.graphml\"\n      algorithm: \"{{algorithm}}\"\n      alpha: \"{{alpha}}\"\n\n  - name: \"Validate Structure\"\n    uses: \"structure-validator\"\n    with:\n      structure_path: \"{{output_root}}/{{id}}/{{dataset}}_{{algorithm}}_{{alpha}}.graphml\"\n      metrics_path: \"{{output_root}}/{{id}}/{{dataset}}_{{algorithm}}_{{alpha}}_metrics.json\"\n</code></pre> <pre><code># Execute matrix workflow\nworkflow = executor.parse_workflow(\"experiments/parameter-sweep.yml\")\n\n# Show matrix expansion\nif \"matrix\" in workflow:\n    jobs = executor.expand_matrix(workflow[\"matrix\"])\n    print(f\"Generated {len(jobs)} parameter combinations\")\n\n    # Preview first few jobs\n    for i, job in enumerate(jobs[:3]):\n        print(f\"Job {i}: {job}\")\n\n# Execute all combinations\nresults = executor.execute_workflow(workflow, mode=\"run\")\n</code></pre>"},{"location":"api/examples/#conditional-matrix","title":"Conditional Matrix","text":"<pre><code># experiments/conditional-matrix.yml\nid: \"conditional-matrix\"\ndescription: \"Matrix with conditional parameters\"\n\nmatrix:\n  dataset: [\"small_data\", \"medium_data\", \"large_data\"]\n  algorithm: [\"pc\", \"ges\"]\n  include:\n    - dataset: \"small_data\"\n      algorithm: \"pc\"\n      alpha: 0.05\n      max_iter: 1000\n    - dataset: \"medium_data\" \n      algorithm: \"pc\"\n      alpha: 0.01\n      max_iter: 5000\n    - dataset: \"large_data\"\n      algorithm: \"ges\" \n      regularization: 0.1\n      max_iter: 10000\n\nsteps:\n  - name: \"Adaptive Structure Learning\"\n    uses: \"adaptive-learner\"\n    with:\n      data_path: \"/data/{{dataset}}.csv\"\n      algorithm: \"{{algorithm}}\"\n      alpha: \"{{alpha|default(0.05)}}\"\n      max_iter: \"{{max_iter|default(1000)}}\"\n      regularization: \"{{regularization|default(0.0)}}\"\n</code></pre>"},{"location":"api/examples/#multi-step-workflows","title":"Multi-Step Workflows","text":""},{"location":"api/examples/#data-processing-pipeline","title":"Data Processing Pipeline","text":"<pre><code># experiments/data-pipeline.yml\nid: \"data-processing-pipeline\"\ndescription: \"Complete data processing and analysis pipeline\"\n\nmatrix:\n  dataset: [\"healthcare\", \"finance\", \"biology\"]\n  preprocessing: [\"standard\", \"robust\"]\n\nsteps:\n  - name: \"Data Preprocessing\"\n    uses: \"data-preprocessor\"\n    with:\n      input_path: \"/raw_data/{{dataset}}.csv\"\n      output_path: \"/processed_data/{{dataset}}_{{preprocessing}}.csv\"\n      method: \"{{preprocessing}}\"\n      remove_outliers: true\n      normalize: true\n\n  - name: \"Feature Selection\"\n    uses: \"feature-selector\"\n    with:\n      input_path: \"/processed_data/{{dataset}}_{{preprocessing}}.csv\"\n      output_path: \"/features/{{dataset}}_{{preprocessing}}_selected.csv\"\n      method: \"mutual_info\"\n      max_features: 50\n\n  - name: \"Structure Learning\"\n    uses: \"structure-learner\"\n    with:\n      data_path: \"/features/{{dataset}}_{{preprocessing}}_selected.csv\"\n      output_path: \"/structures/{{dataset}}_{{preprocessing}}_structure.graphml\"\n      algorithm: \"pc\"\n\n  - name: \"Model Validation\"\n    uses: \"model-validator\"\n    with:\n      structure_path: \"/structures/{{dataset}}_{{preprocessing}}_structure.graphml\"\n      data_path: \"/features/{{dataset}}_{{preprocessing}}_selected.csv\"\n      output_path: \"/validation/{{dataset}}_{{preprocessing}}_results.json\"\n      k_fold: 5\n\n  - name: \"Generate Report\"\n    uses: \"report-generator\"\n    with:\n      structure_path: \"/structures/{{dataset}}_{{preprocessing}}_structure.graphml\"\n      validation_path: \"/validation/{{dataset}}_{{preprocessing}}_results.json\"\n      report_path: \"/reports/{{dataset}}_{{preprocessing}}_report.html\"\n</code></pre>"},{"location":"api/examples/#custom-actions","title":"Custom Actions","text":""},{"location":"api/examples/#creating-domain-specific-actions","title":"Creating Domain-Specific Actions","text":"<pre><code># custom_actions/causal_discovery.py\nfrom causaliq_workflow.action import BaseActionProvider, ActionExecutionError\nfrom typing import Any, Dict\nimport pandas as pd\nimport networkx as nx\n\nclass PCAlgorithmAction(BaseActionProvider):\n    \"\"\"PC algorithm for causal structure learning.\"\"\"\n\n    name = \"pc-algorithm\"\n    version = \"2.1.0\"\n    description = \"Peter-Clark algorithm for causal discovery\"\n    supported_actions = {\"learn_structure\"}\n\n    def run(\n        self,\n        action: str,\n        parameters: Dict[str, Any],\n        mode: str = \"dry-run\",\n        context=None,\n        logger=None,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Execute PC algorithm.\"\"\"\n        try:\n            # Load data\n            data_path = parameters[\"data_path\"]\n            data = pd.read_csv(data_path)\n\n            # PC algorithm parameters\n            alpha = parameters.get(\"alpha\", 0.05)\n            max_depth = parameters.get(\"max_depth\", 3)\n\n            # Run PC algorithm (simplified example)\n            graph = self._run_pc_algorithm(data, alpha, max_depth)\n\n            # Save results\n            output_path = parameters[\"output_path\"]\n            nx.write_graphml(graph, output_path)\n\n            return {\n                \"structure_path\": output_path,\n                \"node_count\": graph.number_of_nodes(),\n                \"edge_count\": graph.number_of_edges(),\n                \"alpha_used\": alpha,\n                \"max_depth_used\": max_depth\n            }\n\n        except Exception as e:\n            raise ActionExecutionError(f\"PC algorithm failed: {e}\") from e\n\n    def _run_pc_algorithm(self, data, alpha, max_depth):\n        \"\"\"Simplified PC algorithm implementation.\"\"\"\n        # This would contain the actual PC algorithm logic\n        # For demonstration, create a simple graph\n        G = nx.DiGraph()\n        G.add_nodes_from(data.columns)\n        # Add some edges based on correlations (simplified)\n        return G\n\nclass NetworkAnalysisAction(BaseActionProvider):\n    \"\"\"Network analysis and metrics calculation.\"\"\"\n\n    name = \"network-analysis\" \n    version = \"1.3.0\"\n    description = \"Compute network topology metrics\"\n    supported_actions = {\"analyse_network\"}\n\n    def run(\n        self,\n        action: str,\n        parameters: Dict[str, Any],\n        mode: str = \"dry-run\",\n        context=None,\n        logger=None,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Analyse network structure.\"\"\"\n        try:\n            # Load structure\n            structure_path = parameters[\"structure_path\"]\n            graph = nx.read_graphml(structure_path)\n\n            # Compute metrics\n            metrics = {\n                \"node_count\": graph.number_of_nodes(),\n                \"edge_count\": graph.number_of_edges(),\n                \"density\": nx.density(graph),\n                \"transitivity\": nx.transitivity(graph),\n                \"average_clustering\": nx.average_clustering(graph),\n            }\n\n            # Add centrality measures\n            centrality = nx.degree_centrality(graph)\n            metrics[\"max_centrality\"] = max(centrality.values())\n            metrics[\"avg_centrality\"] = sum(centrality.values()) / len(centrality)\n\n            # Save metrics\n            output_path = parameters[\"output_path\"]\n            import json\n            with open(output_path, \"w\") as f:\n                json.dump(metrics, f, indent=2)\n\n            return metrics\n\n        except Exception as e:\n            raise ActionExecutionError(f\"Network analysis failed: {e}\") from e\n</code></pre>"},{"location":"api/examples/#using-custom-actions","title":"Using Custom Actions","text":"<pre><code># experiments/custom-workflow.yml\nid: \"custom-causal-discovery\"\ndescription: \"Workflow using custom actions\"\n\nmatrix:\n  dataset: [\"asia\", \"cancer\"]\n  alpha: [0.01, 0.05]\n\nsteps:\n  - name: \"PC Structure Learning\"\n    uses: \"pc-algorithm\"\n    with:\n      data_path: \"/data/{{dataset}}.csv\"\n      output_path: \"/results/{{dataset}}_{{alpha}}_structure.graphml\"\n      alpha: \"{{alpha}}\"\n      max_depth: 3\n\n  - name: \"Network Analysis\"\n    uses: \"network-analysis\"\n    with:\n      structure_path: \"/results/{{dataset}}_{{alpha}}_structure.graphml\"\n      output_path: \"/results/{{dataset}}_{{alpha}}_metrics.json\"\n</code></pre>"},{"location":"api/examples/#error-handling-and-validation","title":"Error Handling and Validation","text":""},{"location":"api/examples/#robust-workflow-execution","title":"Robust Workflow Execution","text":"<pre><code>from causaliq_workflow import WorkflowExecutor, WorkflowExecutionError\nfrom causaliq_workflow.schema import WorkflowValidationError\nimport logging\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef execute_workflow_safely(workflow_path: str) -&gt; bool:\n    \"\"\"Execute workflow with comprehensive error handling.\"\"\"\n    executor = WorkflowExecutor()\n\n    try:\n        # Parse and validate\n        logger.info(f\"Loading workflow: {workflow_path}\")\n        workflow = executor.parse_workflow(workflow_path)\n        logger.info(f\"Workflow loaded: {workflow['id']}\")\n\n        # Execute with error handling\n        results = executor.execute_workflow(workflow, mode=\"run\")\n        logger.info(\"Workflow completed successfully\")\n\n        return True\n\n    except FileNotFoundError:\n        logger.error(f\"Workflow file not found: {workflow_path}\")\n        return False\n\n    except WorkflowValidationError as e:\n        logger.error(f\"Workflow validation failed: {e}\")\n        logger.error(f\"Schema path: {e.schema_path}\")\n        return False\n\n    except WorkflowExecutionError as e:\n        logger.error(f\"Workflow execution failed: {e}\")\n        # Check for specific error types\n        if \"Template\" in str(e):\n            logger.error(\"Template variable issue - check matrix and step parameters\")\n        elif \"Action\" in str(e):\n            logger.error(\"Action execution issue - check action inputs and availability\")\n        return False\n\n    except Exception as e:\n        logger.error(f\"Unexpected error: {e}\")\n        return False\n\n# Execute multiple workflows with error recovery\nworkflow_files = [\n    \"experiments/experiment-1.yml\",\n    \"experiments/experiment-2.yml\", \n    \"experiments/experiment-3.yml\"\n]\n\nsuccessful_workflows = []\nfailed_workflows = []\n\nfor workflow_file in workflow_files:\n    if execute_workflow_safely(workflow_file):\n        successful_workflows.append(workflow_file)\n    else:\n        failed_workflows.append(workflow_file)\n\nprint(f\"Successful: {len(successful_workflows)}\")\nprint(f\"Failed: {len(failed_workflows)}\")\n</code></pre>"},{"location":"api/examples/#integration-examples","title":"Integration Examples","text":""},{"location":"api/examples/#jupyter-notebook-integration","title":"Jupyter Notebook Integration","text":"<pre><code># notebook_integration.py\nfrom causaliq_workflow import WorkflowExecutor\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport json\n\nclass NotebookWorkflowRunner:\n    \"\"\"Workflow execution optimized for Jupyter notebooks.\"\"\"\n\n    def __init__(self):\n        self.executor = WorkflowExecutor()\n        self.results = {}\n\n    def run_experiment(self, workflow_path, display_results=True):\n        \"\"\"Run workflow and display results inline.\"\"\"\n        workflow = self.executor.parse_workflow(workflow_path)\n\n        print(f\"\ud83d\udd04 Executing: {workflow['id']}\")\n        print(f\"\ud83d\udccb Description: {workflow.get('description', 'No description')}\")\n\n        # Show matrix expansion if present\n        if \"matrix\" in workflow:\n            jobs = self.executor.expand_matrix(workflow[\"matrix\"])\n            print(f\"\ud83d\udd22 Matrix jobs: {len(jobs)}\")\n\n        # Execute\n        results = self.executor.execute_workflow(workflow, mode=\"run\")\n        self.results[workflow['id']] = results\n\n        if display_results:\n            self.display_results(workflow['id'])\n\n        return results\n\n    def display_results(self, workflow_id):\n        \"\"\"Display workflow results with visualizations.\"\"\"\n        results = self.results.get(workflow_id)\n        if not results:\n            print(\"No results available\")\n            return\n\n        # Display summary\n        print(\"\\\\n\ud83d\udcca Results Summary:\")\n        for step_result in results:\n            if isinstance(step_result, dict) and \"structure_path\" in step_result:\n                self.visualize_structure(step_result[\"structure_path\"])\n\n    def visualize_structure(self, structure_path):\n        \"\"\"Visualize learned causal structure.\"\"\"\n        try:\n            graph = nx.read_graphml(structure_path)\n\n            plt.figure(figsize=(10, 8))\n            pos = nx.spring_layout(graph)\n            nx.draw(graph, pos, with_labels=True, node_color='lightblue', \n                   node_size=1000, font_size=10, arrows=True)\n            plt.title(f\"Causal Structure ({graph.number_of_nodes()} nodes, {graph.number_of_edges()} edges)\")\n            plt.show()\n\n        except Exception as e:\n            print(f\"Visualization failed: {e}\")\n\n# Usage in Jupyter notebook\nrunner = NotebookWorkflowRunner()\n\n# Run experiments\nrunner.run_experiment(\"experiments/pc-analysis.yml\")\nrunner.run_experiment(\"experiments/ges-analysis.yml\")\n\n# Compare results\nprint(\"\\\\n\ud83d\udcc8 Experiment Comparison:\")\nfor workflow_id, results in runner.results.items():\n    print(f\"{workflow_id}: {len(results)} steps completed\")\n</code></pre>"},{"location":"api/examples/#docker-integration","title":"Docker Integration","text":"<pre><code># Dockerfile\nFROM python:3.9-slim\n\nWORKDIR /app\n\n# Install dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy workflow framework\nCOPY causaliq_workflow/ ./causaliq_workflow/\nCOPY experiments/ ./experiments/\nCOPY data/ ./data/\n\n# Create output directory\nRUN mkdir -p /app/results\n\n# Default command\nCMD [\"python\", \"-m\", \"causaliq_workflow\", \"experiments/default-experiment.yml\", \"--output-dir\", \"/app/results\"]\n</code></pre> <pre><code># Build and run\ndocker build -t causal-workflow .\n\n# Run specific experiment\ndocker run -v $(pwd)/results:/app/results causal-workflow \\\\\n    python -m causaliq_workflow experiments/large-scale-analysis.yml\n\n# Run with custom data\ndocker run -v $(pwd)/data:/app/custom_data -v $(pwd)/results:/app/results causal-workflow \\\\\n    python -m causaliq_workflow experiments/custom-data-experiment.yml\n</code></pre>"},{"location":"api/examples/#performance-optimization","title":"Performance Optimization","text":""},{"location":"api/examples/#parallel-matrix-execution","title":"Parallel Matrix Execution","text":"<pre><code>from causaliq_workflow import WorkflowExecutor\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\nimport multiprocessing as mp\n\ndef execute_matrix_job(workflow_data, job_index, job_params):\n    \"\"\"Execute single matrix job in separate process.\"\"\"\n    try:\n        executor = WorkflowExecutor()\n\n        # Create job-specific workflow\n        job_workflow = workflow_data.copy()\n        # Apply matrix parameters to workflow\n        # (implementation would substitute template variables)\n\n        results = executor.execute_workflow(job_workflow, mode=\"run\")\n        return job_index, results, None\n\n    except Exception as e:\n        return job_index, None, str(e)\n\ndef execute_workflow_parallel(workflow_path, max_workers=None):\n    \"\"\"Execute matrix workflow with parallel job execution.\"\"\"\n    executor = WorkflowExecutor()\n    workflow = executor.parse_workflow(workflow_path)\n\n    if \"matrix\" not in workflow:\n        # No matrix, execute normally\n        return executor.execute_workflow(workflow, mode=\"run\")\n\n    # Expand matrix\n    jobs = executor.expand_matrix(workflow[\"matrix\"])\n\n    if max_workers is None:\n        max_workers = min(len(jobs), mp.cpu_count())\n\n    print(f\"Executing {len(jobs)} matrix jobs with {max_workers} workers\")\n\n    results = {}\n    errors = {}\n\n    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n        # Submit all jobs\n        future_to_job = {\n            executor.submit(execute_matrix_job, workflow, i, job): i \n            for i, job in enumerate(jobs)\n        }\n\n        # Collect results\n        for future in as_completed(future_to_job):\n            job_index = future_to_job[future]\n            job_index_result, result, error = future.result()\n\n            if error:\n                errors[job_index] = error\n                print(f\"\u274c Job {job_index} failed: {error}\")\n            else:\n                results[job_index] = result\n                print(f\"\u2705 Job {job_index} completed\")\n\n    print(f\"\\\\nCompleted: {len(results)}/{len(jobs)} jobs\")\n    if errors:\n        print(f\"Errors: {len(errors)} jobs failed\")\n\n    return results, errors\n\n# Usage\nresults, errors = execute_workflow_parallel(\"experiments/large-matrix.yml\", max_workers=8)\n</code></pre> <p>\u2190 Previous: CLI Interface | Back to API Overview</p>"},{"location":"api/logging/","title":"Logging System API","text":"<p>The Logging System provides centralized logging infrastructure with multiple output destinations for workflow execution monitoring, debugging, and audit trails. It supports configurable verbosity levels and flexible output routing.</p>"},{"location":"api/logging/#workflowlogger-class","title":"WorkflowLogger Class","text":"<p>The <code>WorkflowLogger</code> class provides centralized logging with support for multiple output destinations including terminal output, file logging, and test-capturable output.</p>"},{"location":"api/logging/#causaliq_workflow.logger.WorkflowLogger","title":"WorkflowLogger","text":"<pre><code>WorkflowLogger(\n    terminal: bool = True,\n    log_file: Optional[Path] = None,\n    log_level: LogLevel = SUMMARY,\n)\n</code></pre> <p>Centralized logging with multiple output destinations.</p> <p>Provides standardized logging infrastructure supporting file output, terminal output, and test-capturable output for workflow execution monitoring and debugging.</p> <p>This is the core logging structure without task-specific logic. The log_task method and progress functionality will be added in subsequent commits.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>__enter__</code>             \u2013              <p>Context manager entry.</p> </li> <li> <code>__exit__</code>             \u2013              <p>Context manager exit with cleanup.</p> </li> <li> <code>close</code>             \u2013              <p>Close file streams and cleanup resources.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>has_output_destinations</code>               (<code>bool</code>)           \u2013            <p>Return True if any output destination is configured.</p> </li> <li> <code>is_file_logging</code>               (<code>bool</code>)           \u2013            <p>Return True if file logging is enabled.</p> </li> <li> <code>is_terminal_logging</code>               (<code>bool</code>)           \u2013            <p>Return True if terminal logging is enabled.</p> </li> </ul>"},{"location":"api/logging/#causaliq_workflow.logger.WorkflowLogger(terminal)","title":"<code>terminal</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Enable terminal output (default: True)</p>"},{"location":"api/logging/#causaliq_workflow.logger.WorkflowLogger(log_file)","title":"<code>log_file</code>","text":"(<code>Optional[Path]</code>, default:                   <code>None</code> )           \u2013            <p>Optional file path for log output</p>"},{"location":"api/logging/#causaliq_workflow.logger.WorkflowLogger(log_level)","title":"<code>log_level</code>","text":"(<code>LogLevel</code>, default:                   <code>SUMMARY</code> )           \u2013            <p>Logging verbosity level (default: SUMMARY)</p>"},{"location":"api/logging/#causaliq_workflow.logger.WorkflowLogger.has_output_destinations","title":"has_output_destinations  <code>property</code>","text":"<pre><code>has_output_destinations: bool\n</code></pre> <p>Return True if any output destination is configured.</p>"},{"location":"api/logging/#causaliq_workflow.logger.WorkflowLogger.is_file_logging","title":"is_file_logging  <code>property</code>","text":"<pre><code>is_file_logging: bool\n</code></pre> <p>Return True if file logging is enabled.</p>"},{"location":"api/logging/#causaliq_workflow.logger.WorkflowLogger.is_terminal_logging","title":"is_terminal_logging  <code>property</code>","text":"<pre><code>is_terminal_logging: bool\n</code></pre> <p>Return True if terminal logging is enabled.</p>"},{"location":"api/logging/#causaliq_workflow.logger.WorkflowLogger.__enter__","title":"__enter__","text":"<pre><code>__enter__() -&gt; WorkflowLogger\n</code></pre> <p>Context manager entry.</p>"},{"location":"api/logging/#causaliq_workflow.logger.WorkflowLogger.__exit__","title":"__exit__","text":"<pre><code>__exit__(exc_type: Optional[type], exc_val: Optional[Exception], exc_tb: Any) -&gt; None\n</code></pre> <p>Context manager exit with cleanup.</p>"},{"location":"api/logging/#causaliq_workflow.logger.WorkflowLogger.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close file streams and cleanup resources.</p>"},{"location":"api/logging/#key-features","title":"Key Features","text":""},{"location":"api/logging/#multi-destination-output","title":"Multi-Destination Output","text":"<ul> <li>Terminal logging - Real-time output to stdout for interactive monitoring</li> <li>File logging - Persistent log files with automatic directory creation</li> <li>Test capture - Structured output that can be captured and verified in tests</li> </ul>"},{"location":"api/logging/#resource-management","title":"Resource Management","text":"<ul> <li>Context manager support - Automatic cleanup with <code>with</code> statements</li> <li>File stream handling - Proper opening, writing, and closing of log files</li> <li>Error handling - Graceful handling of file system errors</li> </ul>"},{"location":"api/logging/#flexible-configuration","title":"Flexible Configuration","text":"<ul> <li>Verbosity control - Configure logging level at initialization</li> <li>Output selection - Enable/disable terminal and file output independently</li> <li>Append mode - Log files opened in append mode for multiple workflow runs</li> </ul>"},{"location":"api/logging/#loglevel-enum","title":"LogLevel Enum","text":"<p>The <code>LogLevel</code> enum defines verbosity levels for controlling the amount of logging output during workflow execution.</p>"},{"location":"api/logging/#causaliq_workflow.logger.LogLevel","title":"LogLevel","text":"<p>Logging verbosity levels for workflow execution.</p> <p>Attributes:</p> <ul> <li> <code>ALL</code>           \u2013            <p>Comprehensive logging - all task details and intermediate steps.</p> </li> <li> <code>NONE</code>           \u2013            <p>No logging output - silent execution.</p> </li> <li> <code>SUMMARY</code>           \u2013            <p>Summary-level logging - key status and final results only.</p> </li> </ul>"},{"location":"api/logging/#causaliq_workflow.logger.LogLevel.ALL","title":"ALL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ALL = 'all'\n</code></pre> <p>Comprehensive logging - all task details and intermediate steps.</p>"},{"location":"api/logging/#causaliq_workflow.logger.LogLevel.NONE","title":"NONE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>NONE = 'none'\n</code></pre> <p>No logging output - silent execution.</p>"},{"location":"api/logging/#causaliq_workflow.logger.LogLevel.SUMMARY","title":"SUMMARY  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SUMMARY = 'summary'\n</code></pre> <p>Summary-level logging - key status and final results only.</p>"},{"location":"api/logging/#verbosity-levels","title":"Verbosity Levels","text":""},{"location":"api/logging/#none","title":"NONE","text":"<p>Silent execution with no logging output. Useful for automated scripts where output needs to be minimal.</p>"},{"location":"api/logging/#summary","title":"SUMMARY","text":"<p>Summary-level logging showing key status information and final results only. Default level providing essential information without overwhelming detail.</p>"},{"location":"api/logging/#all","title":"ALL","text":"<p>Comprehensive logging including all task details and intermediate steps. Useful for debugging and detailed workflow analysis.</p>"},{"location":"api/logging/#usage-examples","title":"Usage Examples","text":""},{"location":"api/logging/#basic-logger-setup","title":"Basic Logger Setup","text":"<pre><code>from causaliq_workflow import WorkflowLogger, LogLevel\nfrom pathlib import Path\n\n# Terminal-only logging with default verbosity\nlogger = WorkflowLogger()\n\n# File-only logging with high verbosity\nlog_file = Path(\"workflow_execution.log\")\nlogger = WorkflowLogger(\n    terminal=False, \n    log_file=log_file, \n    log_level=LogLevel.ALL\n)\n\n# Both terminal and file logging\nlogger = WorkflowLogger(\n    terminal=True,\n    log_file=Path(\"logs/workflow.log\"),\n    log_level=LogLevel.SUMMARY\n)\n</code></pre>"},{"location":"api/logging/#context-manager-usage","title":"Context Manager Usage","text":"<pre><code>from pathlib import Path\n\n# Automatic resource cleanup\nwith WorkflowLogger(log_file=Path(\"execution.log\")) as logger:\n    # Logger will automatically close file streams\n    pass  # Log workflow execution here\n</code></pre>"},{"location":"api/logging/#configuration-examples","title":"Configuration Examples","text":"<pre><code># Silent execution for automated environments\nsilent_logger = WorkflowLogger(\n    terminal=False, \n    log_file=None, \n    log_level=LogLevel.NONE\n)\n\n# Development setup with verbose output\ndev_logger = WorkflowLogger(\n    terminal=True,\n    log_file=Path(\"debug.log\"),\n    log_level=LogLevel.ALL\n)\n\n# Production setup with summary logging\nprod_logger = WorkflowLogger(\n    terminal=True,\n    log_file=Path(\"/var/log/causaliq/workflow.log\"),\n    log_level=LogLevel.SUMMARY\n)\n</code></pre>"},{"location":"api/logging/#checking-logger-configuration","title":"Checking Logger Configuration","text":"<pre><code>logger = WorkflowLogger(\n    terminal=True, \n    log_file=Path(\"workflow.log\")\n)\n\n# Check configuration\nif logger.is_terminal_logging:\n    print(\"Terminal output enabled\")\n\nif logger.is_file_logging:\n    print(f\"File logging to: {logger.log_file}\")\n\nif logger.has_output_destinations:\n    print(\"Logger has at least one output destination\")\nelse:\n    print(\"Warning: No output destinations configured\")\n</code></pre>"},{"location":"api/logging/#integration-with-workflow-execution","title":"Integration with Workflow Execution","text":"<pre><code>from causaliq_workflow import WorkflowExecutor, WorkflowLogger, LogLevel\nfrom pathlib import Path\n\n# Configure logging for workflow execution\nlog_file = Path(\"experiments/workflow_run.log\")\n\nwith WorkflowLogger(\n    terminal=True, \n    log_file=log_file, \n    log_level=LogLevel.ALL\n) as logger:\n\n    # Future integration - logger will be passed to workflow executor\n    executor = WorkflowExecutor()\n    # workflow_results = executor.execute_with_logging(workflow, logger)\n</code></pre>"},{"location":"api/logging/#design-principles","title":"Design Principles","text":""},{"location":"api/logging/#separation-of-concerns","title":"Separation of Concerns","text":"<p>The logging system is designed as a separate module that can be integrated with various components without tight coupling.</p>"},{"location":"api/logging/#resource-safety","title":"Resource Safety","text":"<p>Proper resource management ensures file streams are always closed, even in error conditions, preventing resource leaks.</p>"},{"location":"api/logging/#test-friendly-design","title":"Test-Friendly Design","text":"<p>The logging infrastructure is designed to be easily mocked and tested, with clear interfaces for capturing output in automated tests.</p>"},{"location":"api/logging/#future-extensibility","title":"Future Extensibility","text":"<p>The core structure provides a foundation for adding task-specific logging functionality in subsequent development phases.</p>"},{"location":"api/logging/#integration-points","title":"Integration Points","text":""},{"location":"api/logging/#workflow-executor-integration","title":"Workflow Executor Integration","text":"<p>The WorkflowLogger is designed to integrate with the WorkflowExecutor for comprehensive workflow execution logging:</p> <pre><code># Future integration pattern\nexecutor = WorkflowExecutor(logger=logger)\nresults = executor.execute_workflow(workflow, mode=\"run\")\n</code></pre>"},{"location":"api/logging/#action-integration","title":"Action Integration","text":"<p>Actions will receive the logger as an optional parameter for task-level logging:</p> <pre><code># Action integration pattern\ndef run(\n    self,\n    action: str,\n    parameters: Dict[str, Any],\n    mode: str = \"dry-run\",\n    context=None,\n    logger=None,\n) -&gt; Dict[str, Any]:\n    if logger:\n        # Log task execution details\n        logger.info(f\"Executing action: {action}\")\n    return results\n</code></pre>"},{"location":"api/logging/#cli-integration","title":"CLI Integration","text":"<p>The CLI will provide logging configuration options:</p> <pre><code># Future CLI integration\ncausaliq-workflow run workflow.yml --log-file=execution.log --log-level=all\n</code></pre>"},{"location":"api/logging/#error-handling","title":"Error Handling","text":""},{"location":"api/logging/#file-system-errors","title":"File System Errors","text":"<p>The logger handles common file system errors gracefully:</p> <ul> <li>Permission errors - Clear error messages for access denied scenarios</li> <li>Missing directories - Automatic creation of parent directories</li> <li>Disk space issues - Proper error reporting for write failures</li> </ul>"},{"location":"api/logging/#resource-cleanup","title":"Resource Cleanup","text":"<p>Context manager support ensures proper cleanup even when exceptions occur:</p> <pre><code>try:\n    with WorkflowLogger(log_file=Path(\"workflow.log\")) as logger:\n        # Workflow execution that might raise exceptions\n        pass\nexcept Exception as e:\n    # Logger file streams are automatically closed\n    print(f\"Workflow failed: {e}\")\n</code></pre> <p>\u2190 Previous: Status System | Back to API Overview | Next: CLI Interface \u2192</p>"},{"location":"api/overview/","title":"API Reference","text":"<p>The CausalIQ Workflow framework provides a comprehensive API for building, validating, and executing data processing workflows. It is part of the CausalIQ ecosystem for intelligent causal discovery.</p>"},{"location":"api/overview/#causaliq-core-foundation","title":"causaliq-core Foundation","text":"<p>causaliq-workflow builds on causaliq-core for its action framework. The following components are imported from causaliq-core:</p> <ul> <li>CausalIQActionProvider - Abstract base class for all action providers</li> <li>ActionInput - Type-safe input specification for action parameters</li> <li>ActionResult - Tuple type for action return values</li> <li>ActionValidationError - Exception for parameter validation failures</li> <li>ActionExecutionError - Exception for runtime execution failures</li> <li>TokenCache - SQLite-based caching infrastructure for workflow results</li> <li>JsonCompressor - JSON tokenisation for compact cache storage</li> </ul> <p>The API is organised into several key modules:</p>"},{"location":"api/overview/#core-components","title":"Core Components","text":""},{"location":"api/overview/#action-framework","title":"Action Framework","text":"<p>Base classes and interfaces for creating reusable workflow actions that follow GitHub Actions patterns.</p> <ul> <li>CausalIQActionProvider - Abstract base class for all workflow actions   (from causaliq-core)</li> <li>ActionInput - Type-safe input specifications (from causaliq-core)</li> <li>ActionResult - Standardised return type (from causaliq-core)</li> <li>ActionExecutionError/ActionValidationError - Exception handling (from   causaliq-core)</li> </ul>"},{"location":"api/overview/#action-registry","title":"Action Registry","text":"<p>Centralised discovery and execution system for workflow actions with plugin architecture support.</p> <ul> <li>ActionRegistry - Dynamic action discovery via entry points</li> <li>WorkflowContext - Complete workflow context for actions including cache</li> <li>ActionRegistryError - Registry-specific exceptions</li> </ul>"},{"location":"api/overview/#workflow-engine","title":"Workflow Engine","text":"<p>Powerful workflow parsing, validation, and execution engine with matrix expansion and templating.</p> <ul> <li>WorkflowExecutor - Main workflow processing engine</li> <li>WorkflowExecutionError - Workflow execution exceptions</li> <li>Template system - Variable substitution and validation</li> </ul>"},{"location":"api/overview/#workflow-cache","title":"Workflow Cache","text":"<p>SQLite-based caching for workflow step results, built on causaliq-core's TokenCache.</p> <ul> <li>WorkflowCache - High-level cache for workflow results</li> <li>CacheEntry - Entry model with metadata and named objects</li> <li>MatrixSchemaError - Cache consistency exceptions</li> <li>Export/Import - Convert cache entries to/from open formats</li> </ul>"},{"location":"api/overview/#schema-validation","title":"Schema Validation","text":"<p>Robust workflow validation against JSON schemas with detailed error reporting.</p> <ul> <li>validate_workflow - Schema-based workflow validation</li> <li>load_schema/load_workflow_file - File loading utilities</li> <li>WorkflowValidationError - Validation-specific exceptions</li> </ul>"},{"location":"api/overview/#status-system","title":"Status System","text":"<p>Comprehensive task execution status enumeration for workflow logging and monitoring.</p> <ul> <li>TaskStatus - Standardised status reporting for all task execution   outcomes</li> <li>Status properties - Categorisation helpers (success, error, execution,   dry-run)</li> <li>Status definitions - Complete coverage of execution, comparison, and   error statuses</li> </ul>"},{"location":"api/overview/#logging-system","title":"Logging System","text":"<p>Centralised logging infrastructure with multiple output destinations for workflow execution monitoring.</p> <ul> <li>WorkflowLogger - Multi-destination logging with file/terminal output   support</li> <li>LogLevel - Verbosity control (NONE, SUMMARY, ALL)</li> <li>Output configuration - Flexible logging destination management</li> </ul>"},{"location":"api/overview/#cli-interface","title":"CLI Interface","text":"<p>Command-line interface for workflow execution and management.</p> <ul> <li>cqflow run - Execute workflow files</li> <li>cqflow export_cache - Export cache entries to directory or zip</li> <li>cqflow import_cache - Import cache entries from directory or zip</li> </ul>"},{"location":"api/overview/#quick-start","title":"Quick Start","text":"<pre><code>from causaliq_workflow import WorkflowExecutor\n\n# Create executor and run workflow\nexecutor = WorkflowExecutor()\nworkflow = executor.parse_workflow(\"my_workflow.yml\")\nresults = executor.execute_workflow(workflow, mode=\"run\")\n</code></pre>"},{"location":"api/overview/#using-the-cache","title":"Using the Cache","text":"<pre><code>from causaliq_workflow.cache import WorkflowCache, CacheEntry\n\n# Store workflow results\nwith WorkflowCache(\"results.db\") as cache:\n    entry = CacheEntry()\n    entry.metadata[\"algorithm\"] = \"pc\"\n    entry.add_object(\"graph\", \"graphml\", \"&lt;graphml&gt;...&lt;/graphml&gt;\")\n\n    key = {\"network\": \"asia\", \"algorithm\": \"pc\"}\n    cache.put(key, entry)\n\n    # Retrieve later\n    result = cache.get(key)\n</code></pre>"},{"location":"api/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Usage Examples - Comprehensive code examples and patterns</li> <li>Action Framework - Learn how to create custom actions</li> <li>CLI Interface - Command-line usage and CI/CD integration</li> </ul> <p>For detailed examples and usage patterns, see the Usage Examples page.</p>"},{"location":"api/registry/","title":"Action Registry","text":"<p>The action registry provides centralised discovery, management, and execution of workflow actions with plugin architecture support.</p> <p>The registry uses entry points to discover action providers from installed packages, enabling a clean plugin architecture where actions can be distributed as separate packages.</p>"},{"location":"api/registry/#causaliq-core-integration","title":"causaliq-core Integration","text":"<p>The registry imports the following from causaliq-core:</p> <ul> <li>CausalIQActionProvider - Base class that all discovered actions must   implement</li> <li>ActionExecutionError - Exception type for action execution failures</li> </ul>"},{"location":"api/registry/#core-classes","title":"Core Classes","text":""},{"location":"api/registry/#causaliq_workflowregistry","title":"causaliq_workflow.registry","text":""},{"location":"api/registry/#causaliq_workflow.registry.ActionRegistry","title":"ActionRegistry","text":"<pre><code>ActionRegistry()\n</code></pre> <p>Registry for discovering and executing workflow actions dynamically.</p> <p>Uses import-time introspection to automatically discover actions when packages are imported. No configuration needed - just import the package and use 'uses: package-name' in workflows.</p> <p>Convention: Action packages should export a CausalIQActionProvider subclass named 'ActionProvider' in their init.py file to avoid namespace collisions.</p> <p>Attributes:</p> <ul> <li> <code>_instance</code>               (<code>Optional[ActionRegistry]</code>)           \u2013            <p>Singleton instance of the ActionRegistry</p> </li> <li> <code>_actions</code>               (<code>Dict[str, Type[CausalIQActionProvider]]</code>)           \u2013            <p>Dictionary mapping action names to CausalIQActionProvider classes</p> </li> <li> <code>_entry_points</code>               (<code>Dict[str, Any]</code>)           \u2013            <p>Dictionary of lazy-loadable entry points</p> </li> <li> <code>_discovery_errors</code>               (<code>List[str]</code>)           \u2013            <p>List of errors encountered during action discovery</p> </li> </ul> Initialises <p>_actions: Dictionary mapping action names to CausalIQActionProvider _entry_points: Dictionary of entry points for lazy loading _discovery_errors: List to collect any discovery errors</p> <p>Methods:</p> <ul> <li> <code>get_available_actions</code>             \u2013              <p>Get dictionary of available action names to classes.</p> </li> <li> <code>get_action_class</code>             \u2013              <p>Get action class by name, loading from entry point if needed.</p> </li> <li> <code>has_action</code>             \u2013              <p>Check if action is available.</p> </li> <li> <code>execute_action</code>             \u2013              <p>Execute action with inputs and workflow context.</p> </li> <li> <code>validate_workflow_actions</code>             \u2013              <p>Validate all actions in workflow exist and can run.</p> </li> </ul>"},{"location":"api/registry/#causaliq_workflow.registry.ActionRegistry.get_available_actions","title":"get_available_actions","text":"<pre><code>get_available_actions() -&gt; Dict[str, Type[CausalIQActionProvider]]\n</code></pre> <p>Get dictionary of available action names to classes.</p> <p>Note: Entry points that haven't been loaded yet will not appear in the returned dictionary. Use get_available_action_names() to get all available action names including lazy-loadable ones.</p> <p>Returns:</p> <ul> <li> <code>Dict[str, Type[CausalIQActionProvider]]</code>           \u2013            <p>Dictionary mapping action names to CausalIQActionProvider classes</p> </li> </ul>"},{"location":"api/registry/#causaliq_workflow.registry.ActionRegistry.get_action_class","title":"get_action_class","text":"<pre><code>get_action_class(name: str) -&gt; Type[CausalIQActionProvider]\n</code></pre> <p>Get action class by name, loading from entry point if needed.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Type[CausalIQActionProvider]</code>           \u2013            <p>CausalIQActionProvider class</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ActionRegistryError</code>             \u2013            <p>If action not found or fails to load</p> </li> </ul>"},{"location":"api/registry/#causaliq_workflow.registry.ActionRegistry.get_action_class(name)","title":"<code>name</code>","text":"(<code>str</code>)           \u2013            <p>Action name</p>"},{"location":"api/registry/#causaliq_workflow.registry.ActionRegistry.has_action","title":"has_action","text":"<pre><code>has_action(name: str) -&gt; bool\n</code></pre> <p>Check if action is available.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if action is available (loaded or lazy-loadable)</p> </li> </ul>"},{"location":"api/registry/#causaliq_workflow.registry.ActionRegistry.has_action(name)","title":"<code>name</code>","text":"(<code>str</code>)           \u2013            <p>Action name to check</p>"},{"location":"api/registry/#causaliq_workflow.registry.ActionRegistry.execute_action","title":"execute_action","text":"<pre><code>execute_action(\n    name: str, inputs: Dict[str, Any], context: WorkflowContext\n) -&gt; Dict[str, Any]\n</code></pre> <p>Execute action with inputs and workflow context.</p> <p>Extracts the 'action' key from inputs and passes it separately to the provider's run() method along with the remaining parameters.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Dict[str, Any]</code>           \u2013            <p>Action outputs dictionary with structure: - status: \"success\", \"skipped\", or \"error\" - **metadata: Execution metadata flattened into result - objects: List of object dicts</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ActionRegistryError</code>             \u2013            <p>If action not found or execution fails</p> </li> </ul>"},{"location":"api/registry/#causaliq_workflow.registry.ActionRegistry.execute_action(name)","title":"<code>name</code>","text":"(<code>str</code>)           \u2013            <p>Provider name (e.g., 'causaliq/knowledge')</p>"},{"location":"api/registry/#causaliq_workflow.registry.ActionRegistry.execute_action(inputs)","title":"<code>inputs</code>","text":"(<code>Dict[str, Any]</code>)           \u2013            <p>Action parameters including 'action' key</p>"},{"location":"api/registry/#causaliq_workflow.registry.ActionRegistry.execute_action(context)","title":"<code>context</code>","text":"(<code>WorkflowContext</code>)           \u2013            <p>Complete workflow context</p>"},{"location":"api/registry/#causaliq_workflow.registry.ActionRegistry.validate_workflow_actions","title":"validate_workflow_actions","text":"<pre><code>validate_workflow_actions(workflow: Dict[str, Any]) -&gt; List[str]\n</code></pre> <p>Validate all actions in workflow exist and can run.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List of validation errors (empty if valid)</p> </li> </ul>"},{"location":"api/registry/#causaliq_workflow.registry.ActionRegistry.validate_workflow_actions(workflow)","title":"<code>workflow</code>","text":"(<code>Dict[str, Any]</code>)           \u2013            <p>Parsed workflow dictionary</p>"},{"location":"api/registry/#causaliq_workflow.registry.WorkflowContext","title":"WorkflowContext  <code>dataclass</code>","text":"<pre><code>WorkflowContext(\n    mode: str,\n    matrix: Dict[str, List[Any]],\n    matrix_values: Dict[str, Any] = dict(),\n    cache: Optional[WorkflowCache] = None,\n)\n</code></pre> <p>Workflow context for action execution optimisation.</p> <p>Provides minimal context needed for actions to optimise across workflows. Actions receive specific data through inputs; context provides meta-information.</p> <p>Attributes:</p> <ul> <li> <code>mode</code>               (<code>str</code>)           \u2013            <p>Execution mode ('dry-run', 'run', 'compare')</p> </li> <li> <code>matrix</code>               (<code>Dict[str, List[Any]]</code>)           \u2013            <p>Complete matrix definition for cross-job optimisation</p> </li> <li> <code>matrix_values</code>               (<code>Dict[str, Any]</code>)           \u2013            <p>Current job's specific matrix variable values</p> </li> <li> <code>cache</code>               (<code>Optional[WorkflowCache]</code>)           \u2013            <p>Optional WorkflowCache for storing step results</p> </li> </ul>"},{"location":"api/registry/#causaliq_workflow.registry.WorkflowContext.matrix_key","title":"matrix_key  <code>property</code>","text":"<pre><code>matrix_key: str\n</code></pre> <p>Compute cache key from matrix values.</p> <p>Returns a truncated SHA-256 hash (16 hex characters) of the matrix variable values, suitable for use as a cache key.</p> <p>The hash is computed from JSON-serialised matrix_values with sorted keys for deterministic ordering.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Truncated hex hash string (16 characters), or empty string</p> </li> <li> <code>str</code>           \u2013            <p>if matrix_values is empty.</p> </li> </ul> Example <p>context = WorkflowContext( ...     mode=\"run\", ...     matrix={\"algorithm\": [\"pc\", \"ges\"]}, ...     matrix_values={\"algorithm\": \"pc\"} ... ) len(context.matrix_key) 16</p>"},{"location":"api/registry/#exception-handling","title":"Exception Handling","text":""},{"location":"api/registry/#causaliq_workflow.registry.ActionRegistryError","title":"ActionRegistryError","text":"<p>Raised when action registry operations fail.</p> <p>This exception is raised when: - Requested action is not found in the registry - Action discovery fails during module scanning - Action validation fails - Other registry-related errors occur</p>"},{"location":"api/registry/#usage-examples","title":"Usage Examples","text":""},{"location":"api/registry/#basic-registry-operations","title":"Basic Registry Operations","text":"<pre><code>from causaliq_workflow.registry import ActionRegistry, ActionRegistryError\n\n# Create registry instance\nregistry = ActionRegistry()\n\n# List all available actions\nactions = registry.get_available_actions()\nfor action_name, action_class in actions.items():\n    print(f\"Action: {action_name} (v{action_class.version})\")\n\n# Check if specific action exists\nif registry.has_action(\"causaliq-workflow\"):\n    action_class = registry.get_action_class(\"causaliq-workflow\")\n    print(f\"Found action: {action_class.description}\")\n\n# Execute action directly through registry\ntry:\n    result = registry.execute_action(\n        \"causaliq-workflow\",\n        action=\"echo\",\n        parameters={\"message\": \"Hello\", \"nodes\": 3},\n        mode=\"dry-run\",\n    )\n    print(f\"Execution result: {result}\")\nexcept ActionRegistryError as e:\n    print(f\"Registry error: {e}\")\n</code></pre>"},{"location":"api/registry/#entry-point-discovery","title":"Entry Point Discovery","text":"<p>Actions are discovered via Python entry points. Packages register their actions in <code>pyproject.toml</code>:</p> <pre><code>[project.entry-points.\"causaliq.actions\"]\nmy-package = \"my_package:ActionProvider\"\n</code></pre> <p>The registry discovers entry points at startup (metadata only) and loads them lazily on first use to avoid circular imports.</p> <pre><code># Entry points are loaded when first accessed\nregistry = ActionRegistry()\n\n# Check discovered entry points\nprint(f\"Available actions: {list(registry.get_available_actions().keys())}\")\n</code></pre>"},{"location":"api/registry/#workflow-context","title":"Workflow Context","text":"<pre><code>from causaliq_workflow.registry import WorkflowContext\n\n# Create workflow context for action execution\ncontext = WorkflowContext(\n    mode=\"run\",\n    matrix={\"dataset\": [\"asia\", \"cancer\"], \"algorithm\": [\"pc\", \"ges\"]},\n    matrix_values={\"dataset\": \"asia\", \"algorithm\": \"pc\"},\n)\n\n# Context provides execution metadata for action optimisation\nprint(f\"Execution mode: {context.mode}\")\nprint(f\"Matrix definition: {context.matrix}\")\nprint(f\"Current matrix values: {context.matrix_values}\")\n\n# Get cache key for current matrix combination\nprint(f\"Matrix key: {context.matrix_key}\")  # SHA-256 hash, 16 chars\n</code></pre>"},{"location":"api/registry/#architecture-notes","title":"Architecture Notes","text":"<p>The ActionRegistry uses Python's entry point system to automatically find and register actions. Actions are discovered by:</p> <ol> <li>Entry point scanning - Discovers <code>causaliq.actions</code> entry points from    installed packages</li> <li>Lazy loading - Entry points recorded at startup but loaded on first use</li> <li>Module fallback - Also scans imported modules for CausalIQActionProvider    subclasses</li> <li>Name-based lookup - Actions identified by their entry point name or    <code>name</code> class attribute</li> </ol> <p>This design enables a flexible plugin architecture where actions can be distributed as separate packages and automatically discovered at runtime, while avoiding circular import issues.</p> <p>\u2190 Previous: Actions | Back to API Overview | Next: Workflow Engine \u2192</p>"},{"location":"api/schema/","title":"Schema Validation","text":"<p>The schema validation system provides robust workflow validation against JSON schemas with detailed error reporting and custom schema support.</p>"},{"location":"api/schema/#core-functions","title":"Core Functions","text":""},{"location":"api/schema/#causaliq_workflowschema","title":"causaliq_workflow.schema","text":""},{"location":"api/schema/#causaliq_workflow.schema.validate_workflow","title":"validate_workflow","text":"<pre><code>validate_workflow(\n    workflow: Dict[str, Any], schema_path: Optional[Union[str, Path]] = None\n) -&gt; bool\n</code></pre> <p>Validate workflow against CausalIQ JSON Schema.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if workflow is valid</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>WorkflowValidationError</code>             \u2013            <p>If workflow validation fails</p> </li> </ul>"},{"location":"api/schema/#causaliq_workflow.schema.validate_workflow(workflow)","title":"<code>workflow</code>","text":"(<code>Dict[str, Any]</code>)           \u2013            <p>Workflow configuration dictionary</p>"},{"location":"api/schema/#causaliq_workflow.schema.validate_workflow(schema_path)","title":"<code>schema_path</code>","text":"(<code>Optional[Union[str, Path]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional path to custom schema file</p>"},{"location":"api/schema/#causaliq_workflow.schema.load_schema","title":"load_schema","text":"<pre><code>load_schema(schema_path: Optional[Union[str, Path]] = None) -&gt; Dict[str, Any]\n</code></pre> <p>Load CausalIQ workflow JSON Schema.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Dict[str, Any]</code>           \u2013            <p>Parsed JSON Schema dictionary</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>WorkflowValidationError</code>             \u2013            <p>If schema file cannot be loaded</p> </li> </ul>"},{"location":"api/schema/#causaliq_workflow.schema.load_schema(schema_path)","title":"<code>schema_path</code>","text":"(<code>Optional[Union[str, Path]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional path to custom schema file.         If None, loads default package schema.</p>"},{"location":"api/schema/#causaliq_workflow.schema.load_workflow_file","title":"load_workflow_file","text":"<pre><code>load_workflow_file(file_path: Union[str, Path]) -&gt; Dict[str, Any]\n</code></pre> <p>Load workflow from YAML file.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Dict[str, Any]</code>           \u2013            <p>Parsed workflow dictionary</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>WorkflowValidationError</code>             \u2013            <p>If file cannot be loaded</p> </li> </ul>"},{"location":"api/schema/#causaliq_workflow.schema.load_workflow_file(file_path)","title":"<code>file_path</code>","text":"(<code>Union[str, Path]</code>)           \u2013            <p>Path to workflow YAML file</p>"},{"location":"api/schema/#exception-handling","title":"Exception Handling","text":""},{"location":"api/schema/#causaliq_workflow.schema.WorkflowValidationError","title":"WorkflowValidationError","text":"<pre><code>WorkflowValidationError(message: str, schema_path: str = '')\n</code></pre> <p>Raised when workflow validation against JSON Schema fails.</p> <p>Parameters:</p>"},{"location":"api/schema/#causaliq_workflow.schema.WorkflowValidationError(message)","title":"<code>message</code>","text":"(<code>str</code>)           \u2013            <p>Validation error description</p>"},{"location":"api/schema/#causaliq_workflow.schema.WorkflowValidationError(schema_path)","title":"<code>schema_path</code>","text":"(<code>str</code>, default:                   <code>''</code> )           \u2013            <p>JSON Schema path where validation failed</p>"},{"location":"api/schema/#usage-examples","title":"Usage Examples","text":""},{"location":"api/schema/#basic-workflow-validation","title":"Basic Workflow Validation","text":"<pre><code>from causaliq_workflow.schema import validate_workflow, WorkflowValidationError\n\nworkflow_data = {\n    \"name\": \"My Experiment\",\n    \"id\": \"experiment-001\",\n    \"data_root\": \"/data\",\n    \"output_root\": \"/results\",\n    \"matrix\": {\n        \"dataset\": [\"asia\", \"cancer\"],\n        \"algorithm\": [\"pc\", \"ges\"],\n    },\n    \"steps\": [\n        {\n            \"name\": \"Structure Learning\",\n            \"uses\": \"my-custom-action\",\n            \"with\": {\n                \"alpha\": 0.05,\n            },\n        }\n    ],\n}\n\ntry:\n    result = validate_workflow(workflow_data)\n    print(\"Workflow validation passed!\")\n    print(f\"Validated workflow: {result['id']}\")\nexcept WorkflowValidationError as e:\n    print(f\"Validation failed: {e}\")\n    print(f\"Schema path: {e.schema_path}\")\n    if hasattr(e, 'validator'):\n        print(f\"Validation details: {e.validator}\")\n</code></pre>"},{"location":"api/schema/#loading-and-using-custom-schemas","title":"Loading and Using Custom Schemas","text":"<pre><code>from causaliq_workflow.schema import load_schema, validate_workflow\nfrom pathlib import Path\n\n# Load custom schema from file\nschema_path = Path(\"my-custom-schema.json\")\nschema = load_schema(schema_path)\n\n# Use custom schema with validation\ntry:\n    validate_workflow(workflow_data, schema)\n    print(\"Custom schema validation passed!\")\nexcept WorkflowValidationError as e:\n    print(f\"Custom validation failed: {e}\")\n</code></pre>"},{"location":"api/schema/#loading-workflow-files","title":"Loading Workflow Files","text":"<pre><code>from causaliq_workflow.schema import load_workflow_file\nfrom pathlib import Path\n\n# Load workflow from YAML or JSON file\nworkflow_path = Path(\"experiments/my-experiment.yml\")\nworkflow_data = load_workflow_file(workflow_path)\n\nprint(f\"Loaded workflow: {workflow_data['id']}\")\nprint(f\"Steps: {len(workflow_data.get('steps', []))}\")\n\n# File loading supports both YAML and JSON formats\njson_workflow = load_workflow_file(\"experiments/experiment.json\")\nyaml_workflow = load_workflow_file(\"experiments/experiment.yml\")\n</code></pre>"},{"location":"api/schema/#comprehensive-validation-pipeline","title":"Comprehensive Validation Pipeline","text":"<pre><code>from causaliq_workflow.schema import load_workflow_file, validate_workflow, WorkflowValidationError\nfrom pathlib import Path\n\ndef validate_workflow_file(file_path: Path) -&gt; dict:\n    \"\"\"Load and validate a workflow file with detailed error reporting.\"\"\"\n    try:\n        # Load workflow from file\n        workflow_data = load_workflow_file(file_path)\n        print(f\"\u2713 Loaded workflow from {file_path}\")\n\n        # Validate against schema\n        validated_workflow = validate_workflow(workflow_data)\n        print(f\"\u2713 Schema validation passed for workflow '{validated_workflow['id']}'\")\n\n        return validated_workflow\n\n    except FileNotFoundError:\n        print(f\"\u2717 Workflow file not found: {file_path}\")\n        raise\n    except WorkflowValidationError as e:\n        print(f\"\u2717 Schema validation failed:\")\n        print(f\"  Error: {e}\")\n        if hasattr(e, 'schema_path'):\n            print(f\"  Schema path: {e.schema_path}\")\n        raise\n    except Exception as e:\n        print(f\"\u2717 Unexpected error loading workflow: {e}\")\n        raise\n\n# Usage\nworkflow_files = [\n    Path(\"experiments/causal-discovery.yml\"),\n    Path(\"experiments/model-validation.json\"),\n    Path(\"experiments/parameter-sweep.yml\")\n]\n\nfor workflow_file in workflow_files:\n    try:\n        workflow = validate_workflow_file(workflow_file)\n        print(f\"Ready to execute: {workflow['id']}\\\\n\")\n    except Exception as e:\n        print(f\"Skipping invalid workflow: {workflow_file}\\\\n\")\n</code></pre>"},{"location":"api/schema/#schema-structure","title":"Schema Structure","text":"<p>The default workflow schema validates:</p>"},{"location":"api/schema/#required-fields","title":"Required Fields","text":"<ul> <li><code>id</code>: Unique workflow identifier</li> <li><code>steps</code>: Array of workflow steps</li> </ul>"},{"location":"api/schema/#optional-fields","title":"Optional Fields","text":"<ul> <li><code>name</code>: Human-readable workflow name</li> <li><code>description</code>: Workflow description</li> <li><code>matrix</code>: Parameter matrix for expansion</li> <li><code>data_root</code>: Base path for data files</li> <li><code>output_root</code>: Base path for output files</li> </ul>"},{"location":"api/schema/#step-schema","title":"Step Schema","text":"<p>Each step must include: - <code>uses</code>: Action identifier - <code>name</code> (optional): Human-readable step name - <code>with</code> (optional): Action parameters</p>"},{"location":"api/schema/#matrix-schema","title":"Matrix Schema","text":"<p>Matrix definitions support: - Simple arrays: <code>{\"param\": [\"value1\", \"value2\"]}</code> - Nested structures: Complex parameter combinations - Type validation: Ensures consistent parameter types</p>"},{"location":"api/schema/#error-reporting","title":"Error Reporting","text":"<p>WorkflowValidationError provides detailed information:</p> <pre><code>try:\n    validate_workflow(invalid_workflow)\nexcept WorkflowValidationError as e:\n    print(f\"Validation error: {e}\")\n    print(f\"Schema path: {e.schema_path}\")\n\n    # Access underlying jsonschema validation details\n    if hasattr(e, 'validator'):\n        print(f\"Failed constraint: {e.validator}\")\n        print(f\"Schema context: {e.schema_path}\")\n</code></pre>"},{"location":"api/schema/#custom-schema-development","title":"Custom Schema Development","text":"<pre><code># Example custom schema with additional constraints\ncustom_schema = {\n    \"type\": \"object\",\n    \"required\": [\"id\", \"version\", \"steps\"],\n    \"properties\": {\n        \"id\": {\"type\": \"string\", \"pattern\": \"^[a-z0-9-]+$\"},\n        \"version\": {\"type\": \"string\", \"pattern\": \"^\\\\d+\\\\.\\\\d+\\\\.\\\\d+$\"},\n        \"description\": {\"type\": \"string\", \"maxLength\": 200},\n        \"matrix\": {\n            \"type\": \"object\",\n            \"patternProperties\": {\n                \"^[a-z_]+$\": {\n                    \"type\": \"array\",\n                    \"minItems\": 1,\n                    \"items\": {\"type\": [\"string\", \"number\"]}\n                }\n            }\n        },\n        \"steps\": {\n            \"type\": \"array\",\n            \"minItems\": 1,\n            \"items\": {\n                \"type\": \"object\",\n                \"required\": [\"uses\"],\n                \"properties\": {\n                    \"name\": {\"type\": \"string\"},\n                    \"uses\": {\"type\": \"string\"},\n                    \"with\": {\"type\": \"object\"}\n                }\n            }\n        }\n    }\n}\n\n# Save and use custom schema\nimport json\nwith open(\"custom-workflow-schema.json\", \"w\") as f:\n    json.dump(custom_schema, f, indent=2)\n\ncustom_schema_obj = load_schema(\"custom-workflow-schema.json\")\nvalidate_workflow(workflow_data, custom_schema_obj)\n</code></pre> <p>\u2190 Previous: Cache | Back to API Overview | Next: Status System \u2192</p>"},{"location":"api/status/","title":"Status System API","text":"<p>The Status System provides standardized task execution status reporting for workflow logging, monitoring, and debugging. It supports all execution modes (run, dry-run, compare) with comprehensive error categorization.</p>"},{"location":"api/status/#taskstatus-enum","title":"TaskStatus Enum","text":"<p>The <code>TaskStatus</code> enum defines all possible task execution outcomes with standardized string values and helpful categorization properties.</p>"},{"location":"api/status/#causaliq_workflow.status.TaskStatus","title":"TaskStatus","text":"<p>Enumeration of all possible task execution statuses.</p> <p>Provides standardized status reporting for workflow task execution, supporting different execution modes and comprehensive error categorization.</p> Status Categories <ul> <li>Execution: EXECUTES, WOULD_EXECUTE, SKIPS, WOULD_SKIP</li> <li>Comparison: IDENTICAL, DIFFERENT</li> <li>Errors: INVALID_USES, INVALID_PARAMETER, FAILED, TIMED_OUT</li> </ul> <p>Attributes:</p> <ul> <li> <code>DIFFERENT</code>           \u2013            <p>Task re-executed, outputs differ from previous run.</p> </li> <li> <code>EXECUTES</code>           \u2013            <p>Task executed successfully, output files created/updated.</p> </li> <li> <code>FAILED</code>           \u2013            <p>Task execution threw unexpected exception.</p> </li> <li> <code>IDENTICAL</code>           \u2013            <p>Task re-executed, outputs identical to previous run.</p> </li> <li> <code>INVALID_PARAMETER</code>           \u2013            <p>Parameters in with: block are invalid for action.</p> </li> <li> <code>INVALID_USES</code>           \u2013            <p>Action package specified in uses: not found.</p> </li> <li> <code>SKIPS</code>           \u2013            <p>Task skipped because output files exist and are current.</p> </li> <li> <code>TIMED_OUT</code>           \u2013            <p>Task exceeded configured timeout.</p> </li> <li> <code>WOULD_EXECUTE</code>           \u2013            <p>Task would execute successfully if run (dry-run mode).</p> </li> <li> <code>WOULD_SKIP</code>           \u2013            <p>Task would be skipped because output files exist (dry-run mode).</p> </li> <li> <code>is_dry_run</code>               (<code>bool</code>)           \u2013            <p>Return True if status is for dry-run mode.</p> </li> <li> <code>is_error</code>               (<code>bool</code>)           \u2013            <p>Return True if status indicates an error condition.</p> </li> <li> <code>is_execution</code>               (<code>bool</code>)           \u2013            <p>Return True if status indicates actual execution occurred.</p> </li> <li> <code>is_success</code>               (<code>bool</code>)           \u2013            <p>Return True if status indicates successful execution.</p> </li> </ul>"},{"location":"api/status/#causaliq_workflow.status.TaskStatus.DIFFERENT","title":"DIFFERENT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DIFFERENT = 'DIFFERENT'\n</code></pre> <p>Task re-executed, outputs differ from previous run.</p>"},{"location":"api/status/#causaliq_workflow.status.TaskStatus.EXECUTES","title":"EXECUTES  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>EXECUTES = 'EXECUTES'\n</code></pre> <p>Task executed successfully, output files created/updated.</p>"},{"location":"api/status/#causaliq_workflow.status.TaskStatus.FAILED","title":"FAILED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FAILED = 'FAILED'\n</code></pre> <p>Task execution threw unexpected exception.</p>"},{"location":"api/status/#causaliq_workflow.status.TaskStatus.IDENTICAL","title":"IDENTICAL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IDENTICAL = 'IDENTICAL'\n</code></pre> <p>Task re-executed, outputs identical to previous run.</p>"},{"location":"api/status/#causaliq_workflow.status.TaskStatus.INVALID_PARAMETER","title":"INVALID_PARAMETER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>INVALID_PARAMETER = 'INVALID_PARAMETER'\n</code></pre> <p>Parameters in with: block are invalid for action.</p>"},{"location":"api/status/#causaliq_workflow.status.TaskStatus.INVALID_USES","title":"INVALID_USES  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>INVALID_USES = 'INVALID_USES'\n</code></pre> <p>Action package specified in uses: not found.</p>"},{"location":"api/status/#causaliq_workflow.status.TaskStatus.SKIPS","title":"SKIPS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SKIPS = 'SKIPS'\n</code></pre> <p>Task skipped because output files exist and are current.</p>"},{"location":"api/status/#causaliq_workflow.status.TaskStatus.TIMED_OUT","title":"TIMED_OUT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TIMED_OUT = 'TIMED_OUT'\n</code></pre> <p>Task exceeded configured timeout.</p>"},{"location":"api/status/#causaliq_workflow.status.TaskStatus.WOULD_EXECUTE","title":"WOULD_EXECUTE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>WOULD_EXECUTE = 'WOULD_EXECUTE'\n</code></pre> <p>Task would execute successfully if run (dry-run mode).</p>"},{"location":"api/status/#causaliq_workflow.status.TaskStatus.WOULD_SKIP","title":"WOULD_SKIP  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>WOULD_SKIP = 'WOULD_SKIP'\n</code></pre> <p>Task would be skipped because output files exist (dry-run mode).</p>"},{"location":"api/status/#causaliq_workflow.status.TaskStatus.is_dry_run","title":"is_dry_run  <code>property</code>","text":"<pre><code>is_dry_run: bool\n</code></pre> <p>Return True if status is for dry-run mode.</p>"},{"location":"api/status/#causaliq_workflow.status.TaskStatus.is_error","title":"is_error  <code>property</code>","text":"<pre><code>is_error: bool\n</code></pre> <p>Return True if status indicates an error condition.</p>"},{"location":"api/status/#causaliq_workflow.status.TaskStatus.is_execution","title":"is_execution  <code>property</code>","text":"<pre><code>is_execution: bool\n</code></pre> <p>Return True if status indicates actual execution occurred.</p>"},{"location":"api/status/#causaliq_workflow.status.TaskStatus.is_success","title":"is_success  <code>property</code>","text":"<pre><code>is_success: bool\n</code></pre> <p>Return True if status indicates successful execution.</p>"},{"location":"api/status/#status-categories","title":"Status Categories","text":""},{"location":"api/status/#core-execution-statuses","title":"Core Execution Statuses","text":"<ul> <li>EXECUTES - Task executed successfully, output files created/updated</li> <li>WOULD_EXECUTE - Task would execute successfully if run (dry-run mode)</li> <li>SKIPS - Task skipped because output files exist and are current</li> <li>WOULD_SKIP - Task would be skipped because output files exist (dry-run mode)</li> </ul>"},{"location":"api/status/#compare-mode-statuses","title":"Compare Mode Statuses","text":"<ul> <li>IDENTICAL - Task re-executed, outputs identical to previous run</li> <li>DIFFERENT - Task re-executed, outputs differ from previous run</li> </ul>"},{"location":"api/status/#error-statuses","title":"Error Statuses","text":"<ul> <li>INVALID_USES - Action package specified in <code>uses:</code> not found</li> <li>INVALID_PARAMETER - Parameters in <code>with:</code> block are invalid for action</li> <li>FAILED - Task execution threw unexpected exception</li> <li>TIMED_OUT - Task exceeded configured timeout</li> </ul>"},{"location":"api/status/#properties","title":"Properties","text":""},{"location":"api/status/#is_success","title":"is_success","text":"<p>Returns <code>True</code> if status indicates successful execution: - <code>EXECUTES</code>, <code>WOULD_EXECUTE</code>, <code>SKIPS</code>, <code>WOULD_SKIP</code>, <code>IDENTICAL</code>, <code>DIFFERENT</code></p>"},{"location":"api/status/#is_error","title":"is_error","text":"<p>Returns <code>True</code> if status indicates an error condition: - <code>INVALID_USES</code>, <code>INVALID_PARAMETER</code>, <code>FAILED</code>, <code>TIMED_OUT</code></p>"},{"location":"api/status/#is_execution","title":"is_execution","text":"<p>Returns <code>True</code> if status indicates actual execution occurred: - <code>EXECUTES</code>, <code>IDENTICAL</code>, <code>DIFFERENT</code></p>"},{"location":"api/status/#is_dry_run","title":"is_dry_run","text":"<p>Returns <code>True</code> if status is for dry-run mode: - <code>WOULD_EXECUTE</code>, <code>WOULD_SKIP</code></p>"},{"location":"api/status/#usage-examples","title":"Usage Examples","text":""},{"location":"api/status/#basic-status-checking","title":"Basic Status Checking","text":"<pre><code>from causaliq_workflow import TaskStatus\n\n# Check if a task completed successfully\nif status == TaskStatus.EXECUTES:\n    print(\"Task executed successfully\")\n\n# Categorize status\nif status.is_success:\n    print(\"Task completed successfully\")\nelif status.is_error:\n    print(f\"Task failed with error: {status.value}\")\n</code></pre>"},{"location":"api/status/#status-based-control-flow","title":"Status-Based Control Flow","text":"<pre><code># Handle different execution outcomes\nmatch status:\n    case TaskStatus.EXECUTES | TaskStatus.IDENTICAL:\n        log_successful_execution(task_name, runtime)\n    case TaskStatus.SKIPS | TaskStatus.WOULD_SKIP:\n        log_skipped_task(task_name, reason=\"outputs exist\")\n    case TaskStatus.DIFFERENT:\n        log_output_changes(task_name, diff_summary)\n    case TaskStatus.FAILED:\n        log_task_failure(task_name, exception_info)\n    case _:\n        log_other_status(task_name, status)\n</code></pre>"},{"location":"api/status/#filtering-and-aggregation","title":"Filtering and Aggregation","text":"<pre><code># Count successful vs failed tasks\nsuccessful_tasks = [s for s in task_statuses if s.is_success]\nfailed_tasks = [s for s in task_statuses if s.is_error]\n\nprint(f\"Success rate: {len(successful_tasks)/len(task_statuses):.1%}\")\n\n# Find tasks that actually executed\nexecuted_tasks = [s for s in task_statuses if s.is_execution]\nprint(f\"Executed {len(executed_tasks)} tasks\")\n</code></pre>"},{"location":"api/status/#dry-run-vs-run-mode","title":"Dry-Run vs Run Mode","text":"<pre><code>def analyze_workflow_plan(statuses):\n    \"\"\"Analyze what a workflow would do in dry-run mode.\"\"\"\n    would_execute = [s for s in statuses if s == TaskStatus.WOULD_EXECUTE]\n    would_skip = [s for s in statuses if s == TaskStatus.WOULD_SKIP]\n\n    print(f\"Would execute: {len(would_execute)} tasks\")\n    print(f\"Would skip: {len(would_skip)} tasks\")\n\n    return len(would_execute) &gt; 0  # True if work needed\n</code></pre>"},{"location":"api/status/#integration-points","title":"Integration Points","text":""},{"location":"api/status/#workflow-logger-integration","title":"Workflow Logger Integration","text":"<p>The TaskStatus enum is designed for integration with the upcoming WorkflowLogger system:</p> <pre><code># Future logging integration example\nlogger.log_task(\n    action_name=\"causal-discovery\",\n    status=TaskStatus.EXECUTES,\n    message=\"learn graph\",\n    runtime=2.3,\n    outputs=\"/results/graph.xml\"\n)\n</code></pre>"},{"location":"api/status/#action-development","title":"Action Development","text":"<p>Actions will report their execution status using TaskStatus values:</p> <pre><code># Action integration example\ndef run(\n    self,\n    action: str,\n    parameters: Dict[str, Any],\n    mode: str = \"dry-run\",\n    context=None,\n    logger=None,\n) -&gt; Dict[str, Any]:\n    if mode == \"dry-run\":\n        if self._outputs_exist(parameters):\n            return {\"status\": TaskStatus.WOULD_SKIP}\n        else:\n            return {\"status\": TaskStatus.WOULD_EXECUTE}\n\n    # Actual execution logic...\n    return {\"status\": TaskStatus.EXECUTES, \"outputs\": results}\n</code></pre>"},{"location":"api/status/#design-principles","title":"Design Principles","text":""},{"location":"api/status/#status-completeness","title":"Status Completeness","text":"<p>The enum covers all possible task execution outcomes across different modes, ensuring comprehensive status reporting without gaps.</p>"},{"location":"api/status/#categorization-properties","title":"Categorization Properties","text":"<p>Helper properties (<code>is_success</code>, <code>is_error</code>, etc.) enable easy filtering and aggregation without hardcoding status lists.</p>"},{"location":"api/status/#string-values","title":"String Values","text":"<p>All enum values use string representations matching their names, ensuring clear, readable log output.</p>"},{"location":"api/status/#mode-awareness","title":"Mode Awareness","text":"<p>Distinct statuses for dry-run vs run modes enable accurate workflow planning and execution reporting.</p> <p>\u2190 Previous: Schema Validation | Back to API Overview | Next: Logging System \u2192</p>"},{"location":"api/workflow/","title":"Workflow Engine","text":"<p>The workflow execution engine provides powerful workflow parsing, validation, and execution with matrix expansion and template variable support.</p>"},{"location":"api/workflow/#core-classes","title":"Core Classes","text":""},{"location":"api/workflow/#causaliq_workflowworkflow","title":"causaliq_workflow.workflow","text":""},{"location":"api/workflow/#causaliq_workflow.workflow.WorkflowExecutor","title":"WorkflowExecutor","text":"<pre><code>WorkflowExecutor()\n</code></pre> <p>Parse and execute GitHub Actions-style workflows with matrix expansion.</p> <p>This class handles the parsing of YAML workflow files and expansion of matrix strategies into individual experiment jobs. It provides the foundation for executing multi-step causal discovery workflows with parameterised experiments using flexible action parameter templating.</p> <p>Methods:</p> <ul> <li> <code>parse_workflow</code>             \u2013              <p>Parse workflow YAML file with validation.</p> </li> <li> <code>expand_matrix</code>             \u2013              <p>Expand matrix variables into individual job configurations.</p> </li> <li> <code>execute_workflow</code>             \u2013              <p>Execute complete workflow with matrix expansion.</p> </li> </ul>"},{"location":"api/workflow/#causaliq_workflow.workflow.WorkflowExecutor.parse_workflow","title":"parse_workflow","text":"<pre><code>parse_workflow(\n    workflow_path: Union[str, Path], mode: str = \"dry-run\"\n) -&gt; Dict[str, Any]\n</code></pre> <p>Parse workflow YAML file with validation.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Dict[str, Any]</code>           \u2013            <p>Parsed and validated workflow dictionary</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>WorkflowExecutionError</code>             \u2013            <p>If workflow parsing or validation fails</p> </li> </ul>"},{"location":"api/workflow/#causaliq_workflow.workflow.WorkflowExecutor.parse_workflow(workflow_path)","title":"<code>workflow_path</code>","text":"(<code>Union[str, Path]</code>)           \u2013            <p>Path to workflow YAML file</p>"},{"location":"api/workflow/#causaliq_workflow.workflow.WorkflowExecutor.parse_workflow(mode)","title":"<code>mode</code>","text":"(<code>str</code>, default:                   <code>'dry-run'</code> )           \u2013            <p>Execution mode for action validation</p>"},{"location":"api/workflow/#causaliq_workflow.workflow.WorkflowExecutor.expand_matrix","title":"expand_matrix","text":"<pre><code>expand_matrix(matrix: Dict[str, List[Any]]) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Expand matrix variables into individual job configurations.</p> <p>Generates all combinations from matrix variables using cartesian product. Each combination becomes a separate job configuration.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>List[Dict[str, Any]]</code>           \u2013            <p>List of job configurations with matrix variables expanded</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>WorkflowExecutionError</code>             \u2013            <p>If matrix expansion fails</p> </li> </ul>"},{"location":"api/workflow/#causaliq_workflow.workflow.WorkflowExecutor.expand_matrix(matrix)","title":"<code>matrix</code>","text":"(<code>Dict[str, List[Any]]</code>)           \u2013            <p>Dictionary mapping variable names to lists of values</p>"},{"location":"api/workflow/#causaliq_workflow.workflow.WorkflowExecutor.execute_workflow","title":"execute_workflow","text":"<pre><code>execute_workflow(\n    workflow: Dict[str, Any],\n    mode: str = \"dry-run\",\n    cli_params: Optional[Dict[str, Any]] = None,\n    step_logger: Optional[Callable[[str, str, str], None]] = None,\n) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Execute complete workflow with matrix expansion.</p> <p>Caching is controlled at the step level via the 'output' parameter in each step's 'with' block. Each step can write to its own cache.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>List[Dict[str, Any]]</code>           \u2013            <p>List of job results from matrix expansion</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>WorkflowExecutionError</code>             \u2013            <p>If workflow execution fails</p> </li> </ul>"},{"location":"api/workflow/#causaliq_workflow.workflow.WorkflowExecutor.execute_workflow(workflow)","title":"<code>workflow</code>","text":"(<code>Dict[str, Any]</code>)           \u2013            <p>Parsed workflow dictionary</p>"},{"location":"api/workflow/#causaliq_workflow.workflow.WorkflowExecutor.execute_workflow(mode)","title":"<code>mode</code>","text":"(<code>str</code>, default:                   <code>'dry-run'</code> )           \u2013            <p>Execution mode ('dry-run', 'run', 'compare')</p>"},{"location":"api/workflow/#causaliq_workflow.workflow.WorkflowExecutor.execute_workflow(cli_params)","title":"<code>cli_params</code>","text":"(<code>Optional[Dict[str, Any]]</code>, default:                   <code>None</code> )           \u2013            <p>Additional parameters from CLI</p>"},{"location":"api/workflow/#causaliq_workflow.workflow.WorkflowExecutor.execute_workflow(step_logger)","title":"<code>step_logger</code>","text":"(<code>Optional[Callable[[str, str, str], None]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional function to log step execution</p>"},{"location":"api/workflow/#exception-handling","title":"Exception Handling","text":""},{"location":"api/workflow/#causaliq_workflow.workflow.WorkflowExecutionError","title":"WorkflowExecutionError","text":"<p>Raised when workflow execution fails.</p>"},{"location":"api/workflow/#usage-examples","title":"Usage Examples","text":""},{"location":"api/workflow/#basic-workflow-execution","title":"Basic Workflow Execution","text":"<pre><code>from causaliq_workflow import WorkflowExecutor, WorkflowExecutionError\nfrom causaliq_workflow.registry import ActionRegistry\n\n# Create executor instance\nexecutor = WorkflowExecutor()\n\ntry:\n    # Parse and validate workflow (includes template variable validation)\n    workflow = executor.parse_workflow(\"experiment.yml\")\n    print(f\"Workflow ID: {workflow['id']}\")\n    print(f\"Description: {workflow['description']}\")\n\n    # Execute the complete workflow\n    results = executor.execute_workflow(workflow, mode=\"run\")\n    print(f\"Workflow completed successfully\")\n\nexcept WorkflowExecutionError as e:\n    print(f\"Workflow execution failed: {e}\")\n</code></pre>"},{"location":"api/workflow/#matrix-expansion","title":"Matrix Expansion","text":"<pre><code>from causaliq_workflow import WorkflowExecutor\n\nexecutor = WorkflowExecutor()\n\n# Define matrix for parameter sweeps\nmatrix = {\n    \"algorithm\": [\"pc\", \"ges\", \"lingam\"],\n    \"dataset\": [\"asia\", \"cancer\"],\n    \"alpha\": [0.01, 0.05]\n}\n\n# Expand into individual parameter combinations\njobs = executor.expand_matrix(matrix)\nprint(f\"Generated {len(jobs)} jobs from matrix\")  # Results in 12 jobs (3 \u00d7 2 \u00d7 2)\n\nfor i, job in enumerate(jobs):\n    print(f\"Job {i}: Algorithm={job['algorithm']}, Dataset={job['dataset']}, Alpha={job['alpha']}\")\n</code></pre>"},{"location":"api/workflow/#template-variable-system","title":"Template Variable System","text":"<pre><code># The WorkflowExecutor automatically validates template variables during parsing\n# Template variables ({{variable}}) are checked against available context\n\n# Example: Valid template usage\nvalid_workflow = {\n    \"id\": \"test-001\",\n    \"description\": \"Template validation example\", \n    \"matrix\": {\"dataset\": [\"asia\"], \"algorithm\": [\"pc\"]},\n    \"steps\": [{\n        \"uses\": \"my-custom-action\",\n        \"with\": {\n            \"output\": \"/results/{{id}}/{{dataset}}_{{algorithm}}.xml\",\n            \"description\": \"Processing {{dataset}} with {{algorithm}}\"\n        }\n    }]\n}\n\ntry:\n    workflow = executor.parse_workflow_dict(valid_workflow)\n    print(\"Template validation passed!\")\nexcept WorkflowExecutionError as e:\n    if \"Unknown template variables\" in str(e):\n        print(f\"Template validation failed: {e}\")\n        # Example: \"Unknown template variables: missing_var. Available context: id, dataset, algorithm\"\n    else:\n        print(f\"Workflow execution failed: {e}\")\n</code></pre>"},{"location":"api/workflow/#advanced-workflow-features","title":"Advanced Workflow Features","text":"<pre><code># Example workflow YAML showing flexible action parameters\nworkflow_yaml = \\\"\\\"\\\"\nid: \"experiment-001\"\ndescription: \"Flexible causal discovery experiment\"\nmatrix:\n  dataset: [\"asia\", \"cancer\"]\n  algorithm: [\"pc\", \"ges\"]\n  alpha: [0.01, 0.05]\n\nsteps:\n  - name: \"Structure Learning\"\n    uses: \"my-structure-learner\"\n    with:\n      data_path: \"/experiments/data/{{dataset}}.csv\"\n      output_dir: \"/experiments/results/{{id}}/{{algorithm}}/\"\n      alpha: \"{{alpha}}\"\n      max_iter: 1000\n\n  - name: \"Validation\"\n    uses: \"validate-graph\"\n    with:\n      graph_path: \"/experiments/results/{{id}}/{{algorithm}}/graph.graphml\"\n      metrics_output: \"/experiments/results/{{id}}/{{algorithm}}/metrics.json\"\n\\\"\\\"\\\"\n\n# Save and parse the workflow\nwith open(\"experiment.yml\", \"w\") as f:\n    f.write(workflow_yaml)\n\nworkflow = executor.parse_workflow(\"experiment.yml\")\n\n# Matrix expansion creates jobs with substituted variables\nif \"matrix\" in workflow:\n    jobs = executor.expand_matrix(workflow[\"matrix\"])\n    # Creates 8 jobs (2\u00d72\u00d72) with customizable file paths\n\n    for job in jobs:\n        print(f\"Job: {job}\")\n        # Example output: {'dataset': 'asia', 'algorithm': 'pc', 'alpha': 0.01}\n</code></pre>"},{"location":"api/workflow/#template-variable-context","title":"Template Variable Context","text":"<p>Template variables can reference:</p> <ol> <li>Workflow properties: <code>{{id}}</code>, <code>{{description}}</code>, <code>{{name}}</code></li> <li>Matrix variables: Any variables defined in the <code>matrix</code> section</li> <li>Step context: Variables available during step execution</li> <li>File paths: Dynamic path generation using workflow context</li> </ol>"},{"location":"api/workflow/#available-template-variables","title":"Available Template Variables","text":"Context Variables Example Workflow <code>id</code>, <code>description</code>, <code>name</code> <code>{{id}}</code> Matrix User-defined matrix vars <code>{{dataset}}</code>, <code>{{algorithm}}</code> Paths <code>data_root</code>, <code>output_root</code> <code>{{output_root}}/results</code>"},{"location":"api/workflow/#error-handling","title":"Error Handling","text":"<p>The WorkflowExecutor provides detailed error reporting for:</p> <ul> <li>Parse errors: Invalid YAML/JSON syntax</li> <li>Schema validation: Workflow structure validation</li> <li>Template errors: Unknown or invalid template variables</li> <li>Action errors: Action execution failures</li> <li>Matrix errors: Invalid matrix definitions</li> </ul> <p>\u2190 Previous: Registry | Back to API Overview | Next: Cache \u2192</p>"},{"location":"architecture/action_architecture_design/","title":"Action Architecture","text":""},{"location":"architecture/action_architecture_design/#overview","title":"Overview","text":"<p>The action architecture provides reusable, automatically-discoverable workflow components following GitHub Actions patterns. Actions are zero-configuration plugins that become available immediately upon installation, with no registry files or manual setup required.</p>"},{"location":"architecture/action_architecture_design/#auto-discovery-action-framework","title":"Auto-Discovery Action Framework","text":""},{"location":"architecture/action_architecture_design/#how-actions-are-found-and-used","title":"How Actions Are Found and Used","text":""},{"location":"architecture/action_architecture_design/#the-discovery-lifecycle","title":"The Discovery Lifecycle","text":"<ol> <li>Installation Phase: Developer installs action package    (<code>pip install my-action</code>)</li> <li>Discovery Phase: Framework discovers entry points at startup (metadata    only)</li> <li>Lazy Loading Phase: Action classes are loaded on first use</li> <li>Execution Phase: Workflows reference actions by name    (<code>uses: \"my-action\"</code>)</li> </ol>"},{"location":"architecture/action_architecture_design/#entry-point-based-action-registration","title":"Entry Point-Based Action Registration","text":"<p>Actions register themselves using Python entry points in <code>pyproject.toml</code>:</p> <pre><code># pyproject.toml\n[project.entry-points.\"causaliq.actions\"]\nmy-action = \"my_action:ActionProvider\"\n</code></pre> <p>The action class implementation:</p> <pre><code># my_action/__init__.py\nfrom causaliq_workflow.action import BaseActionProvider\n\n\nclass ActionProvider(BaseActionProvider):\n    name = \"my-action\"\n    version = \"1.0.0\"\n    description = \"Performs custom analysis\"\n\n    def run(self, inputs, **kwargs):\n        # Implementation here\n        return {\"status\": \"complete\"}\n</code></pre>"},{"location":"architecture/action_architecture_design/#base-action-interface","title":"Base Action Interface","text":"<pre><code>from abc import ABC, abstractmethod\nfrom typing import Dict, Any, Optional\nfrom dataclasses import dataclass\nimport semantic_version\n\n@dataclass\nclass ActionInput:\n    \"\"\"Define action input specification.\"\"\"\n    name: str\n    description: str\n    required: bool = False\n    default: Any = None\n    type_hint: str = \"Any\"\n\n@dataclass\nclass ActionOutput:\n    \"\"\"Define action output specification.\"\"\"\n    name: str\n    description: str\n    value: Any\n\nclass BaseActionProvider(ABC):\n    \"\"\"Base class for all workflow actions.\"\"\"\n\n    # Action metadata\n    name: str = \"\"\n    version: str = \"1.0.0\"\n    description: str = \"\"\n    author: str = \"\"\n\n    # Input/output specifications\n    inputs: Dict[str, ActionInput] = {}\n    outputs: Dict[str, str] = {}  # name -&gt; description mapping\n\n    @abstractmethod\n    def run(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Execute action with validated inputs, return outputs.\"\"\"\n        pass\n\n    def validate_inputs(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Validate and process input values.\"\"\"\n        validated = {}\n\n        for input_name, input_spec in self.inputs.items():\n            if input_spec.required and input_name not in inputs:\n                raise ValueError(f\"Required input '{input_name}' missing for action {self.name}\")\n\n            value = inputs.get(input_name, input_spec.default)\n            validated[input_name] = value\n\n        return validated\n\n    def format_outputs(self, raw_outputs: Dict[str, Any]) -&gt; Dict[str, ActionOutput]:\n        \"\"\"Format raw outputs with metadata.\"\"\"\n        formatted = {}\n\n        for name, value in raw_outputs.items():\n            description = self.outputs.get(name, f\"Output from {self.name}\")\n            formatted[name] = ActionOutput(\n                name=name,\n                description=description,\n                value=value\n            )\n\n        return formatted\n</code></pre>"},{"location":"architecture/action_architecture_design/#auto-discovery-action-registry","title":"Auto-Discovery Action Registry","text":"<p>The registry automatically discovers and manages actions without configuration:</p> <pre><code>import pkgutil\nimport importlib\nfrom typing import Dict, Type, Any\n\nclass ActionRegistry:\n    \"\"\"Automatically discover and manage workflow actions.\"\"\"\n\n    def __init__(self):\n        self._actions: Dict[str, Type[BaseActionProvider]] = {}\n        self._discover_actions()  # Automatic discovery on initialization\n\n    def _discover_actions(self):\n        \"\"\"Scan Python environment for action packages.\"\"\"\n\n        # Iterate through all importable modules\n        for finder, module_name, ispkg in pkgutil.iter_modules():\n            try:\n                # Attempt to import the module\n                module = importlib.import_module(module_name)\n\n                # Check if module exports an 'ActionProvider' class\n                if hasattr(module, 'ActionProvider'):\n                    action_class = getattr(module, 'ActionProvider')\n\n                    # Verify it's a proper BaseActionProvider subclass\n                    if (isinstance(action_class, type) and \n                        issubclass(action_class, BaseActionProvider) and \n                        action_class != BaseActionProvider):\n\n                        # Register using module name as action identifier\n                        self._actions[module_name] = action_class\n\n            except ImportError:\n                # Skip modules that can't be imported\n                continue\n\n    def get_available_actions(self) -&gt; Dict[str, Type[BaseActionProvider]]:\n        \"\"\"Return copy of available actions.\"\"\"\n        return self._actions.copy()\n\n    def get_action_class(self, action_name: str) -&gt; Type[BaseActionProvider]:\n        \"\"\"Get action class by name.\"\"\"\n        if action_name not in self._actions:\n            raise ActionRegistryError(f\"Action '{action_name}' not found. Available actions: {list(self._actions.keys())}\")\n        return self._actions[action_name]\n</code></pre>"},{"location":"architecture/action_architecture_design/#how-discovery-works-step-by-step","title":"How Discovery Works Step-by-Step","text":"<ol> <li>Registry Initialization: When <code>ActionRegistry()</code> is created, discovery starts automatically</li> <li>Module Scanning: Uses <code>pkgutil.iter_modules()</code> to iterate through all Python modules</li> <li>Safe Import: Attempts to import each module, skipping those that fail</li> <li>Convention Check: Looks for a class named 'ActionProvider' in each module</li> <li>Validation: Ensures the ActionProvider class inherits from the base BaseActionProvider class</li> <li>Registration: Maps module name to ActionProvider class for workflow lookup</li> </ol>"},{"location":"architecture/action_architecture_design/#action-package-development-workflow","title":"Action Package Development Workflow","text":"<p>Step 1: Create Standard Python Package</p> <pre><code>mkdir my_custom_action\ncd my_custom_action\n</code></pre> <p>Step 2: Define Package Structure</p> <pre><code>my_custom_action/\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 my_custom_action/\n\u2502   \u2514\u2500\u2500 __init__.py  # Must export 'ActionProvider' class\n\u2514\u2500\u2500 README.md\n</code></pre> <p>Step 3: Implement Action Convention</p> <pre><code># my_custom_action/__init__.py\nfrom causaliq_workflow.action import BaseActionProvider\n\nclass ActionProvider(BaseActionProvider):  # Must be named 'ActionProvider'\n    name = \"my-custom-action\"\n    version = \"1.0.0\" \n    description = \"Custom analysis action\"\n\n    def run(self, inputs):\n        # Action implementation\n        result = self.perform_analysis(inputs['data'])\n        return {\"analysis_result\": result}\n</code></pre> <p>Step 4: Install and Use Immediately</p> <pre><code>pip install my_custom_action\ncausaliq-workflow my-experiment.yml  # Action automatically available\n</code></pre> <pre><code>\n## Auto-Discovery Action Examples\n\n### Example 1: Simple Analysis Action\n\n**Package: causaliq_analysis**\n\n```python\n# causaliq_analysis/__init__.py\nfrom causaliq_workflow.action import BaseActionProvider\nimport pandas as pd\nimport networkx as nx\n\nclass ActionProvider(BaseActionProvider):  # Auto-discovered by this name\n    name = \"causaliq-analysis\"\n    version = \"1.0.0\"\n    description = \"Basic causal graph analysis\"\n\n    def run(self, inputs):\n        \"\"\"Analyze causal graph structure.\"\"\"\n        graph_path = inputs['graph_path']\n        graph = nx.read_graphml(graph_path)\n\n        analysis = {\n            \"nodes\": len(graph.nodes),\n            \"edges\": len(graph.edges), \n            \"density\": nx.density(graph),\n            \"is_dag\": nx.is_directed_acyclic_graph(graph)\n        }\n\n        return {\"analysis\": analysis}\n</code></pre> <p>Usage in Workflow:</p> <pre><code>steps:\n  - name: \"Analyze Graph\"\n    uses: \"causaliq_analysis\"  # Automatically discovered\n    with:\n      graph_path: \"/results/learned_graph.xml\"\n</code></pre>"},{"location":"architecture/action_architecture_design/#example-2-data-loading-action","title":"Example 2: Data Loading Action","text":"<p>Package: causaliq_data</p> <pre><code># causaliq_data/__init__.py  \nfrom causaliq_workflow.action import BaseActionProvider\nimport pandas as pd\nfrom pathlib import Path\n\nclass ActionProvider(BaseActionProvider):\n    name = \"causaliq-data\"\n    version = \"2.1.0\"\n    description = \"Load and preprocess causal datasets\"\n\n    def run(self, inputs):\n        \"\"\"Load dataset with optional preprocessing.\"\"\"\n        dataset_name = inputs['dataset']\n        sample_size = inputs.get('sample_size')\n\n        # Load from standard datasets\n        if dataset_name == \"asia\":\n            data = self._load_asia_network()\n        elif dataset_name == \"cancer\":\n            data = self._load_cancer_network()\n        else:\n            # Load from file path\n            data = pd.read_csv(dataset_name)\n\n        # Apply sampling if requested\n        if sample_size and sample_size &lt; len(data):\n            data = data.sample(n=sample_size, random_state=42)\n\n        output_path = inputs['output_path']\n        data.to_csv(f\"{output_path}/data.csv\", index=False)\n\n        return {\n            \"data_path\": f\"{output_path}/data.csv\",\n            \"rows\": len(data),\n            \"columns\": len(data.columns)\n        }\n</code></pre>"},{"location":"architecture/action_architecture_design/#example-3-algorithm-bridge-action","title":"Example 3: Algorithm Bridge Action","text":"<p>Package: causaliq_pc_algorithm</p> <pre><code># causaliq_pc_algorithm/__init__.py\nfrom causaliq_workflow.action import BaseActionProvider\nimport pandas as pd\nimport networkx as nx\n\nclass ActionProvider(BaseActionProvider):\n    name = \"causaliq-pc-algorithm\" \n    version = \"1.5.2\"\n    description = \"PC algorithm for causal structure learning\"\n\n    def run(self, inputs):\n        \"\"\"Execute PC algorithm.\"\"\"\n        data_path = inputs['data_path']\n        alpha = inputs.get('alpha', 0.05)\n        output_path = inputs['output_path']\n\n        # Load data\n        data = pd.read_csv(data_path)\n\n        # Run PC algorithm (implementation details omitted)\n        graph = self._execute_pc_algorithm(data, alpha)\n\n        # Save results\n        nx.write_graphml(graph, f\"{output_path}/graph.xml\")\n\n        # Generate metadata\n        metadata = {\n            \"algorithm\": \"pc\",\n            \"alpha\": alpha,\n            \"nodes\": len(graph.nodes),\n            \"edges\": len(graph.edges)\n        }\n\n        with open(f\"{output_path}/metadata.json\", 'w') as f:\n            json.dump(metadata, f, indent=2)\n\n        return {\n            \"graph_path\": f\"{output_path}/graph.xml\",\n            \"metadata_path\": f\"{output_path}/metadata.json\",\n            \"edge_count\": len(graph.edges)\n        }\n            columns = dataset.columns.tolist()\n            np.random.shuffle(columns)\n            randomised = dataset[columns]\n            transformation_log.append(f\"Shuffled column order: {' -&gt; '.join(columns)}\")\n\n        elif strategy == \"subsample\":\n            subsample_size = min(len(dataset) // 2, 1000)\n            randomised = dataset.sample(n=subsample_size).reset_index(drop=True)\n            transformation_log.append(f\"Subsampled to {subsample_size} rows\")\n\n        elif strategy == \"bootstrap\":\n            randomised = dataset.sample(n=len(dataset), replace=True).reset_index(drop=True)\n            transformation_log.append(\"Bootstrap resampling applied\")\n\n        else:\n            raise ValueError(f\"Unknown randomisation strategy: {strategy}\")\n\n        return {\n            \"randomised_dataset\": randomised,\n            \"transformation_log\": transformation_log\n        }\n</code></pre>"},{"location":"architecture/action_architecture_design/#algorithm-execution-actions","title":"Algorithm Execution Actions","text":"<pre><code>import networkx as nx\n\nclass CausalDiscoveryAction(BaseActionProvider):\n    \"\"\"Execute causal discovery algorithm from various packages.\"\"\"\n\n    name = \"causal-discovery\"\n    version = \"1.0.0\"\n    description = \"Run causal discovery algorithm with automatic package detection\"\n\n    inputs = {\n        \"algorithm\": ActionInput(\"algorithm\", \"Algorithm name (pc, ges, lingam, etc.)\", required=True),\n        \"package\": ActionInput(\"package\", \"Algorithm package (bnlearn, tetrad, causal-learn, auto)\", default=\"auto\"),\n        \"data\": ActionInput(\"data\", \"Input dataset\", required=True),\n        \"parameters\": ActionInput(\"parameters\", \"Algorithm-specific parameters\", default={})\n    }\n\n    outputs = {\n        \"learned_graph\": \"Learned causal graph as NetworkX DiGraph\",\n        \"algorithm_info\": \"Information about algorithm execution\",\n        \"performance_metrics\": \"Execution time, memory usage, convergence info\"\n    }\n\n    def run(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Execute causal discovery algorithm.\"\"\"\n        algorithm = inputs[\"algorithm\"].lower()\n        package = inputs[\"package\"]\n        data = inputs[\"data\"]\n        parameters = inputs[\"parameters\"]\n\n        # Auto-detect package if needed\n        if package == \"auto\":\n            package = self._detect_best_package(algorithm)\n\n        # Execute algorithm\n        start_time = time.time()\n\n        if package == \"bnlearn\":\n            learned_graph, algo_info = self._execute_bnlearn(algorithm, data, parameters)\n        elif package == \"tetrad\":\n            learned_graph, algo_info = self._execute_tetrad(algorithm, data, parameters)\n        elif package == \"causal-learn\":\n            learned_graph, algo_info = self._execute_causal_learn(algorithm, data, parameters)\n        else:\n            raise ValueError(f\"Unsupported package: {package}\")\n\n        execution_time = time.time() - start_time\n\n        performance_metrics = {\n            \"execution_time_seconds\": execution_time,\n            \"algorithm\": algorithm,\n            \"package\": package,\n            \"num_variables\": len(data.columns),\n            \"num_samples\": len(data),\n            \"num_edges\": learned_graph.number_of_edges()\n        }\n\n        return {\n            \"learned_graph\": learned_graph,\n            \"algorithm_info\": algo_info,\n            \"performance_metrics\": performance_metrics\n        }\n\n    def _detect_best_package(self, algorithm: str) -&gt; str:\n        \"\"\"Detect best available package for algorithm.\"\"\"\n        algorithm_packages = {\n            \"pc\": [\"bnlearn\", \"causal-learn\", \"tetrad\"],\n            \"ges\": [\"causal-learn\", \"tetrad\"], \n            \"lingam\": [\"causal-learn\"],\n            \"iamb\": [\"bnlearn\"],\n            \"gs\": [\"bnlearn\"]\n        }\n\n        preferred_packages = algorithm_packages.get(algorithm, [\"causal-learn\"])\n\n        # Check availability and return first available\n        for package in preferred_packages:\n            if self._is_package_available(package):\n                return package\n\n        raise RuntimeError(f\"No available package found for algorithm: {algorithm}\")\n\n    def _execute_bnlearn(self, algorithm: str, data: pd.DataFrame, \n                        parameters: Dict) -&gt; tuple:\n        \"\"\"Execute algorithm using R bnlearn.\"\"\"\n        try:\n            import rpy2.robjects as ro\n            from rpy2.robjects import pandas2ri\n        }\n</code></pre>"},{"location":"architecture/action_architecture_design/#benefits-of-auto-discovery-architecture","title":"Benefits of Auto-Discovery Architecture","text":""},{"location":"architecture/action_architecture_design/#for-action-developers","title":"For Action Developers","text":""},{"location":"architecture/action_architecture_design/#zero-configuration-setup","title":"Zero Configuration Setup","text":"<ul> <li>No registry management: No need to maintain configuration files or plugin registries</li> <li>Standard Python patterns: Use familiar <code>pyproject.toml</code>, <code>pip install</code>, and package structure</li> <li>Immediate availability: Actions become available as soon as the package is installed</li> <li>Simple convention: Just export a class named 'Action' from the package</li> </ul>"},{"location":"architecture/action_architecture_design/#development-workflow","title":"Development Workflow","text":"<ol> <li>Create package: Standard Python package with <code>pyproject.toml</code></li> <li>Implement action: Export 'Action' class following the interface</li> <li>Test locally: <code>pip install -e .</code> for development testing</li> <li>Publish: Standard PyPI publishing or GitHub releases</li> <li>Use immediately: Actions available in all workflows without restart</li> </ol>"},{"location":"architecture/action_architecture_design/#for-workflow-authors","title":"For Workflow Authors","text":""},{"location":"architecture/action_architecture_design/#seamless-integration","title":"Seamless Integration","text":"<ul> <li>Familiar syntax: Uses standard GitHub Actions-style <code>uses: \"action-name\"</code></li> <li>No configuration: No need to declare or configure actions before use</li> <li>Version management: Standard semantic versioning through package versions</li> <li>Dependency handling: Python's pip handles all dependencies automatically</li> </ul>"},{"location":"architecture/action_architecture_design/#ecosystem-growth","title":"Ecosystem Growth","text":"<ul> <li>Organic discovery: New actions become available automatically</li> <li>Community contributions: Easy for community to create and share actions</li> <li>Quality assurance: Actions are regular Python packages with standard testing</li> <li>Documentation: Standard Python documentation tools apply</li> </ul>"},{"location":"architecture/action_architecture_design/#for-the-framework","title":"For the Framework","text":""},{"location":"architecture/action_architecture_design/#architectural-benefits","title":"Architectural Benefits","text":"<ul> <li>Reduced complexity: No registry files, configuration, or plugin management code</li> <li>Robustness: Discovery failures don't break the system (graceful degradation)  </li> <li>Performance: Lazy loading and one-time discovery minimize overhead</li> <li>Maintainability: Less framework code means easier maintenance</li> </ul>"},{"location":"architecture/action_architecture_design/#ecosystem-integration","title":"Ecosystem Integration","text":"<ul> <li>Standard distribution: Uses PyPI and standard Python packaging</li> <li>Cross-platform: Works wherever Python works</li> <li>Version compatibility: Standard semantic versioning for compatibility management</li> <li>Testing integration: Actions can be tested with standard Python testing tools</li> </ul>"},{"location":"architecture/action_architecture_design/#auto-discovery-implementation-patterns","title":"Auto-Discovery Implementation Patterns","text":""},{"location":"architecture/action_architecture_design/#cross-language-bridges","title":"Cross-Language Bridges","text":"<p>Actions can bridge to R, Java, and other languages:</p> <pre><code># causaliq_bnlearn/__init__.py\nfrom causaliq_workflow.action import BaseActionProvider\nimport rpy2.robjects as ro\n\nclass ActionProvider(BaseActionProvider):\n    name = \"causaliq-bnlearn\"\n\n    def __init__(self):\n        # Initialize R environment once\n        ro.r('library(bnlearn)')\n\n    def run(self, inputs):\n        # Bridge to R bnlearn package\n        algorithm = inputs['algorithm']  # 'pc', 'gs', 'iamb', etc.\n        ro.globalenv['data'] = inputs['data']\n        ro.r(f'result &lt;- {algorithm}(data)')\n        return {\"graph\": self._convert_to_networkx(ro.r('result'))}\n</code></pre>"},{"location":"architecture/action_architecture_design/#algorithm-collections","title":"Algorithm Collections","text":"<p>Single packages can provide multiple related algorithms:</p> <pre><code># causaliq_constraint_based/__init__.py\nclass ActionProvider(BaseActionProvider):\n    name = \"causaliq-constraint-based\"\n\n    def run(self, inputs):\n        algorithm = inputs['algorithm']\n\n        if algorithm == 'pc':\n            return self._run_pc(inputs)\n        elif algorithm == 'fci':\n            return self._run_fci(inputs)\n        elif algorithm == 'cfci':\n            return self._run_cfci(inputs)\n        else:\n            raise ValueError(f\"Unknown algorithm: {algorithm}\")\n</code></pre> <p>This auto-discovery architecture creates a vibrant, extensible ecosystem where actions can be developed, shared, and used with minimal friction while maintaining the robustness and reliability needed for scientific workflows.</p>"},{"location":"architecture/action_auto_discovery_design/","title":"Action Auto-Discovery Design","text":""},{"location":"architecture/action_auto_discovery_design/#overview","title":"Overview","text":"<p>The auto-discovery system implements a plugin architecture using Python entry points where actions are automatically found and registered without any configuration files. This design eliminates the need for manual registry management while providing a seamless plugin ecosystem for causal discovery workflows.</p> <p>The system uses lazy loading to avoid circular import issues - entry points are discovered at startup (metadata only), but the actual action classes are only loaded when first used.</p>"},{"location":"architecture/action_auto_discovery_design/#how-auto-discovery-works-the-complete-journey","title":"How Auto-Discovery Works: The Complete Journey","text":""},{"location":"architecture/action_auto_discovery_design/#phase-1-system-initialisation","title":"Phase 1: System Initialisation","text":""},{"location":"architecture/action_auto_discovery_design/#step-1-workflow-engine-starts","title":"Step 1: Workflow Engine Starts","text":"<p>When a user runs <code>causaliq-workflow experiment.yml</code>:</p> <ol> <li>WorkflowExecutor initialisation: Creates a new <code>WorkflowExecutor</code>    instance</li> <li>ActionRegistry creation: Instantiates <code>ActionRegistry()</code> which triggers    discovery</li> <li>Entry point discovery: The registry scans for <code>causaliq.actions</code> entry    points</li> </ol>"},{"location":"architecture/action_auto_discovery_design/#step-2-entry-point-discovery-lazy-loading","title":"Step 2: Entry Point Discovery (Lazy Loading)","text":"<p>The system discovers actions without importing them:</p> <ol> <li>Entry point enumeration: Uses <code>importlib.metadata.entry_points()</code> to    find all packages that declare <code>causaliq.actions</code> entry points</li> <li>Metadata recording: Stores entry point metadata (name, module path)    without loading</li> <li>Deferred loading: Actual imports happen only when an action is first    used</li> </ol> <p>This lazy loading approach is critical for avoiding circular imports, since action packages (like <code>causaliq-knowledge</code>) depend on <code>causaliq-workflow</code>.</p>"},{"location":"architecture/action_auto_discovery_design/#phase-2-action-loading-on-demand","title":"Phase 2: Action Loading (On Demand)","text":""},{"location":"architecture/action_auto_discovery_design/#step-3-first-use-triggers-loading","title":"Step 3: First Use Triggers Loading","text":"<p>When a workflow references an action:</p> <pre><code>steps:\n  - name: \"Generate Graph\"\n    uses: \"causaliq-knowledge\"  # Triggers lazy load\n    with:\n      action: \"generate_graph\"\n</code></pre> <p>The system:</p> <ol> <li>Entry point lookup: Finds the entry point for \"causaliq-knowledge\"</li> <li>Class loading: Calls <code>entry_point.load()</code> to import the action class</li> <li>Type validation: Verifies it's a valid <code>BaseActionProvider</code> subclass</li> <li>Caching: Stores the loaded class for future use</li> </ol>"},{"location":"architecture/action_auto_discovery_design/#step-4-action-execution","title":"Step 4: Action Execution","text":"<p>Once loaded:</p> <ol> <li>Parameter preparation: Collects parameters from the <code>with</code> block</li> <li>Input validation: Action validates its own inputs using schemas</li> <li>Execution: Calls <code>action.run(inputs)</code> with validated parameters</li> <li>Output handling: Processes and stores action outputs</li> </ol>"},{"location":"architecture/action_auto_discovery_design/#phase-3-fallback-module-scanning","title":"Phase 3: Fallback Module Scanning","text":"<p>For backwards compatibility and development scenarios, the registry also supports module scanning:</p> <ol> <li>Module enumeration: Scans <code>sys.modules</code> for already-imported modules</li> <li>Convention detection: Looks for modules exporting <code>ActionProvider</code></li> <li>Registration: Adds discovered actions to the registry</li> </ol>"},{"location":"architecture/action_auto_discovery_design/#the-plugin-developer-experience","title":"The Plugin Developer Experience","text":""},{"location":"architecture/action_auto_discovery_design/#creating-an-action-package-step-by-step","title":"Creating an Action Package: Step-by-Step","text":""},{"location":"architecture/action_auto_discovery_design/#step-1-standard-python-package-setup","title":"Step 1: Standard Python Package Setup","text":"<p>Developers start with familiar Python packaging:</p> <pre><code>mkdir causaliq-pc-algorithm\ncd causaliq-pc-algorithm\n</code></pre> <p>File: pyproject.toml</p> <pre><code>[build-system]\nrequires = [\"setuptools&gt;=61.0\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"causaliq-pc-algorithm\"\nversion = \"1.0.0\"\ndescription = \"PC algorithm implementation for CausalIQ workflows\"\ndependencies = [\n    \"causaliq-workflow&gt;=0.1.1\",\n    \"networkx&gt;=2.8.0\",\n    \"pandas&gt;=1.5.0\"\n]\n\n# CRITICAL: Register your action as an entry point\n[project.entry-points.\"causaliq.actions\"]\ncausaliq-pc-algorithm = \"causaliq_pc_algorithm:ActionProvider\"\n</code></pre> <p>The entry point declaration tells <code>causaliq-workflow</code> where to find your action class. The format is:</p> <pre><code>action-name = \"module.path:ClassName\"\n</code></pre>"},{"location":"architecture/action_auto_discovery_design/#step-2-implement-the-action-class","title":"Step 2: Implement the Action Class","text":"<p>File: src/causaliq_pc_algorithm/init.py</p> <pre><code>\"\"\"PC algorithm action for CausalIQ workflows.\"\"\"\n\nfrom causaliq_workflow.action import BaseActionProvider\nimport pandas as pd\nimport networkx as nx\n\n\n# The class name can be anything, but must match the entry point\nclass ActionProvider(BaseActionProvider):\n    \"\"\"PC algorithm for causal structure learning.\"\"\"\n\n    name = \"causaliq-pc-algorithm\"\n    version = \"1.0.0\"\n    description = \"PC algorithm for causal structure learning\"\n\n    def run(self, inputs, **kwargs):\n        \"\"\"Execute the PC algorithm.\n\n        Args:\n            inputs: Dictionary containing:\n                - dataset: Path to CSV data file\n                - alpha: Significance level (default: 0.05)\n                - output_path: Where to save results\n\n        Returns:\n            Dictionary with graph_path, nodes, and edges count.\n        \"\"\"\n        # Read data\n        data = pd.read_csv(inputs[\"dataset\"])\n\n        # Run PC algorithm\n        alpha = inputs.get(\"alpha\", 0.05)\n        graph = self._pc_algorithm(data, alpha)\n\n        # Save results\n        output_path = inputs[\"output_path\"]\n        nx.write_graphml(graph, f\"{output_path}/graph.xml\")\n\n        return {\n            \"graph_path\": f\"{output_path}/graph.xml\",\n            \"nodes\": len(graph.nodes),\n            \"edges\": len(graph.edges),\n        }\n\n    def _pc_algorithm(self, data, alpha):\n        \"\"\"PC algorithm implementation.\"\"\"\n        # Implementation here\n        pass\n</code></pre>"},{"location":"architecture/action_auto_discovery_design/#step-3-installation-and-immediate-availability","title":"Step 3: Installation and Immediate Availability","text":"<pre><code># Install the package (editable mode for development)\npip install -e .\n\n# Action is immediately available in workflows\ncausaliq-workflow my-experiment.yml\n</code></pre> <p>File: my-experiment.yml</p> <pre><code>description: \"PC Algorithm Test\"\nid: pc-test\n\nsteps:\n  - name: \"Learn Structure\"\n    uses: \"causaliq-pc-algorithm\"  # Automatically discovered via entry point\n    with:\n      dataset: \"/data/asia.csv\"\n      alpha: 0.01\n      output_path: \"/results/pc\"\n</code></pre>"},{"location":"architecture/action_auto_discovery_design/#entry-point-discovery-implementation","title":"Entry Point Discovery Implementation","text":""},{"location":"architecture/action_auto_discovery_design/#the-lazy-loading-pattern","title":"The Lazy Loading Pattern","text":"<pre><code>import sys\nfrom importlib.metadata import entry_points\nfrom typing import Dict, Optional, Type\n\nclass ActionRegistry:\n    def __init__(self):\n        self._actions: Dict[str, Type[BaseActionProvider]] = {}\n        self._entry_points: Dict[str, Any] = {}  # Metadata only\n        self._discover_entry_points()\n\n    def _discover_entry_points(self) -&gt; None:\n        \"\"\"Discover entry points WITHOUT importing them.\"\"\"\n        # Python 3.10+ API\n        if sys.version_info &gt;= (3, 10):\n            eps = entry_points(group=\"causaliq.actions\")\n        else:\n            # Python 3.9 compatibility\n            all_eps = entry_points()\n            eps = all_eps.get(\"causaliq.actions\", [])\n\n        for ep in eps:\n            # Store metadata only - no imports yet!\n            self._entry_points[ep.name] = ep\n\n    def _load_entry_point(self, name: str) -&gt; Optional[Type[BaseActionProvider]]:\n        \"\"\"Load an entry point on first use.\"\"\"\n        if name not in self._entry_points:\n            return None\n\n        ep = self._entry_points[name]\n        action_class = ep.load()  # Import happens here\n\n        # Validate and cache\n        if issubclass(action_class, BaseActionProvider):\n            self._actions[name] = action_class\n            return action_class\n        return None\n\n    def get_action_class(self, name: str) -&gt; Type[BaseActionProvider]:\n        \"\"\"Get action class, loading from entry point if needed.\"\"\"\n        # Return cached action\n        if name in self._actions:\n            return self._actions[name]\n\n        # Try lazy loading from entry point\n        if name in self._entry_points:\n            action_class = self._load_entry_point(name)\n            if action_class:\n                return action_class\n\n        raise ActionRegistryError(f\"Action '{name}' not found\")\n</code></pre>"},{"location":"architecture/action_auto_discovery_design/#why-lazy-loading","title":"Why Lazy Loading?","text":"<p>Lazy loading solves a critical circular import problem:</p> <pre><code>causaliq-workflow (provides BaseActionProvider base class)\n       \u2191\n       \u2514\u2500\u2500 causaliq-knowledge (imports BaseActionProvider, exports action)\n</code></pre> <p>If <code>causaliq-workflow</code> eagerly imported <code>causaliq-knowledge</code> during registry initialisation, it would fail because <code>causaliq-knowledge</code> needs to import from <code>causaliq-workflow</code> first.</p> <p>By deferring the import until the action is actually used, we ensure: 1. <code>causaliq-workflow</code> initialises completely first 2. <code>causaliq-knowledge</code> can import from it safely 3. The action class is then loaded on demand</p>"},{"location":"architecture/action_auto_discovery_design/#ecosystem-integration-patterns","title":"Ecosystem Integration Patterns","text":""},{"location":"architecture/action_auto_discovery_design/#distribution-strategy","title":"Distribution Strategy","text":"<p>Action packages follow standard Python distribution:</p> <ol> <li>PyPI publishing: <code>pip install causaliq-tetrad-bridge</code></li> <li>GitHub releases: Direct installation from repositories</li> <li>Local development: <code>pip install -e .</code> for development packages</li> <li>Version management: Standard semantic versioning</li> </ol>"},{"location":"architecture/action_auto_discovery_design/#dependency-management","title":"Dependency Management","text":"<p>Actions declare their dependencies naturally:</p> <pre><code># Action package dependencies\ndependencies = [\n    \"causaliq-workflow&gt;=0.1.1\",    # Framework dependency\n    \"rpy2&gt;=3.5.0\",                 # R interface (if needed)\n    \"jpype1&gt;=1.4.0\",               # Java interface (if needed)\n    \"scikit-learn&gt;=1.2.0\"          # Algorithm dependencies\n]\n\n# REQUIRED: Entry point registration\n[project.entry-points.\"causaliq.actions\"]\nmy-action = \"my_action:ActionProvider\"\n</code></pre>"},{"location":"architecture/action_auto_discovery_design/#cross-language-bridge-pattern","title":"Cross-Language Bridge Pattern","text":"<p>For R and Java integrations:</p> <pre><code>class RBridgeAction(BaseActionProvider):\n    \"\"\"Bridge to R-based causal discovery algorithms.\"\"\"\n\n    name = \"r-bridge-action\"\n    version = \"1.0.0\"\n    description = \"Execute R algorithms via rpy2\"\n\n    def __init__(self):\n        super().__init__()\n        # Initialise R environment\n        self.r_session = self._setup_r_environment()\n\n    def run(self, inputs, **kwargs):\n        # Execute R code through rpy2\n        result = self.r_session.run_algorithm(inputs)\n        return self._convert_r_output(result)\n</code></pre>"},{"location":"architecture/action_auto_discovery_design/#benefits-of-entry-point-discovery","title":"Benefits of Entry Point Discovery","text":""},{"location":"architecture/action_auto_discovery_design/#for-users","title":"For Users","text":"<ol> <li>Zero configuration: Install and use immediately</li> <li>No registry management: No config files to maintain</li> <li>Standard workflow: Familiar <code>pip install</code> \u2192 use pattern</li> <li>Automatic updates: New action versions available immediately</li> </ol>"},{"location":"architecture/action_auto_discovery_design/#for-developers","title":"For Developers","text":"<ol> <li>Standard Python packaging: Use pyproject.toml entry points</li> <li>Simple registration: One line in pyproject.toml</li> <li>Full Python ecosystem: Use any Python dependencies</li> <li>Independent development: No coordination with core framework needed</li> <li>No circular imports: Lazy loading handles dependencies correctly</li> </ol>"},{"location":"architecture/action_auto_discovery_design/#for-the-ecosystem","title":"For the Ecosystem","text":"<ol> <li>Organic growth: Easy to create and share actions</li> <li>Version management: Standard semantic versioning</li> <li>Quality control: Actions are regular Python packages with testing</li> <li>Documentation: Standard Python documentation tools apply</li> </ol>"},{"location":"architecture/action_auto_discovery_design/#quick-reference-creating-a-new-action","title":"Quick Reference: Creating a New Action","text":""},{"location":"architecture/action_auto_discovery_design/#minimal-example","title":"Minimal Example","text":"<p>pyproject.toml:</p> <pre><code>[build-system]\nrequires = [\"setuptools&gt;=61.0\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"my-causaliq-action\"\nversion = \"1.0.0\"\ndependencies = [\"causaliq-workflow&gt;=0.1.1\"]\n\n[project.entry-points.\"causaliq.actions\"]\nmy-causaliq-action = \"my_causaliq_action:ActionProvider\"\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n</code></pre> <p>src/my_causaliq_action/init.py:</p> <pre><code>from causaliq_workflow.action import BaseActionProvider\n\n\nclass ActionProvider(BaseActionProvider):\n    name = \"my-causaliq-action\"\n    version = \"1.0.0\"\n    description = \"My custom action\"\n\n    def run(self, inputs, **kwargs):\n        # Your implementation here\n        return {\"status\": \"complete\", \"result\": inputs.get(\"value\", 0) * 2}\n</code></pre> <p>Usage in workflow.yaml:</p> <pre><code>description: \"Test my action\"\nid: test-my-action\n\nsteps:\n  - name: \"Run my action\"\n    uses: \"my-causaliq-action\"\n    with:\n      value: 42\n</code></pre>"},{"location":"architecture/action_auto_discovery_design/#checklist","title":"Checklist","text":"<ul> <li>[ ] Create <code>pyproject.toml</code> with entry point registration</li> <li>[ ] Implement action class inheriting from <code>BaseActionProvider</code></li> <li>[ ] Set <code>name</code>, <code>version</code>, <code>description</code> class attributes</li> <li>[ ] Implement <code>run(self, inputs, **kwargs)</code> method</li> <li>[ ] Return a dictionary from <code>run()</code></li> <li>[ ] Install with <code>pip install -e .</code> for development</li> <li>[ ] Test with <code>causaliq-workflow --dry-run workflow.yaml</code></li> </ul>"},{"location":"architecture/logging_architecture_design/","title":"Logging &amp; Task Status Architecture Design","text":"<p>Design Document Version: 1.0 Date: 2025-11-18 Status: Draft - Implementation Planned</p>"},{"location":"architecture/logging_architecture_design/#overview","title":"Overview","text":"<p>CausalIQ Workflow requires a comprehensive logging and task status system to provide visibility into workflow execution, enable debugging, and support different execution modes (dry-run, run, compare). This document outlines the architecture for task-level logging with standardized status reporting.</p>"},{"location":"architecture/logging_architecture_design/#requirements","title":"Requirements","text":""},{"location":"architecture/logging_architecture_design/#functional-requirements","title":"Functional Requirements","text":""},{"location":"architecture/logging_architecture_design/#fr1-multi-destination-logging","title":"FR1: Multi-Destination Logging","text":"<ul> <li>Requirement: Log messages to file and/or terminal simultaneously</li> <li>Rationale: Terminal output for interactive use, file output for audit trails and debugging</li> <li>Implementation: Configurable output destinations via CLI parameters</li> </ul>"},{"location":"architecture/logging_architecture_design/#fr2-test-capturable-output","title":"FR2: Test-Capturable Output","text":"<ul> <li>Requirement: Output must be capturable and verifiable in automated tests</li> <li>Rationale: Essential for testing workflow execution and validating status reporting</li> <li>Implementation: Structured logging interface that tests can intercept</li> </ul>"},{"location":"architecture/logging_architecture_design/#fr3-task-level-granularity","title":"FR3: Task-Level Granularity","text":"<ul> <li>Requirement: Messages produced at individual task level (each action execution)</li> <li>Rationale: Actions may execute multiple internal tasks (e.g., learning hundreds of graphs)</li> <li>Implementation: Actions control their own message granularity and frequency</li> </ul>"},{"location":"architecture/logging_architecture_design/#fr4-standardized-message-format","title":"FR4: Standardized Message Format","text":"<ul> <li>Requirement: Consistent, parseable message format across all actions</li> <li>Format: <code>YYYY-MM-DD HH:MM:SS [action-name] STATUS task description (details)</code></li> <li>Example: <code>2025-06-23 12:03:23 [causal-discovery] EXECUTES learn graph in 2.3s</code></li> <li>Rationale: Enables monitoring, parsing, and integration with external tools</li> </ul>"},{"location":"architecture/logging_architecture_design/#fr5-comprehensive-status-system","title":"FR5: Comprehensive Status System","text":"<ul> <li>Requirement: Task execution status covers all workflow execution modes</li> <li>Statuses: See Task Status Definitions</li> <li>Rationale: Provides complete visibility into workflow execution state</li> </ul>"},{"location":"architecture/logging_architecture_design/#fr6-summary-reporting","title":"FR6: Summary Reporting","text":"<ul> <li>Requirement: Aggregate status counts and resource consumption</li> <li>Includes: Task counts per status, total runtime, estimated resource usage</li> <li>Rationale: High-level workflow execution overview for monitoring</li> </ul>"},{"location":"architecture/logging_architecture_design/#fr7-progress-indication","title":"FR7: Progress Indication","text":"<ul> <li>Requirement: Real-time progress feedback during execution</li> <li>Implementation: Click-based progress bars with estimated completion</li> <li>Rationale: User experience during long-running workflows</li> </ul>"},{"location":"architecture/logging_architecture_design/#non-functional-requirements","title":"Non-Functional Requirements","text":""},{"location":"architecture/logging_architecture_design/#nfr1-performance","title":"NFR1: Performance","text":"<ul> <li>Requirement: Logging overhead &lt; 5% of total execution time</li> <li>Implementation: Efficient logging with minimal I/O blocking</li> </ul>"},{"location":"architecture/logging_architecture_design/#nfr2-flexibility","title":"NFR2: Flexibility","text":"<ul> <li>Requirement: Actions have complete control over message frequency/content</li> <li>Implementation: Optional logger interface, actions decide granularity</li> </ul>"},{"location":"architecture/logging_architecture_design/#nfr3-backward-compatibility","title":"NFR3: Backward Compatibility","text":"<ul> <li>Requirement: Existing actions work without modification</li> <li>Implementation: Logger parameter is optional, graceful degradation</li> </ul>"},{"location":"architecture/logging_architecture_design/#task-status-definitions","title":"Task Status Definitions","text":""},{"location":"architecture/logging_architecture_design/#core-execution-statuses","title":"Core Execution Statuses","text":""},{"location":"architecture/logging_architecture_design/#executes","title":"EXECUTES","text":"<ul> <li>Mode: run, compare</li> <li>Condition: Task executed successfully, output files created/updated</li> <li>Message: Includes actual runtime, input/output file sizes</li> <li>Example: <code>EXECUTES learn graph in 2.3s \u2192 inputs: /data/asia.csv (1.2MB) \u2192 outputs: /results/graph.xml (0.8MB)</code></li> </ul>"},{"location":"architecture/logging_architecture_design/#would_execute","title":"WOULD_EXECUTE","text":"<ul> <li>Mode: dry-run</li> <li>Condition: Task would execute successfully if run, output files absent</li> <li>Message: Includes estimated runtime from action</li> <li>Example: <code>WOULD_EXECUTE learn graph in ~2.1s \u2192 inputs: /data/asia.csv (1.2MB) \u2192 outputs: /results/graph.xml (estimated)</code></li> </ul>"},{"location":"architecture/logging_architecture_design/#skips","title":"SKIPS","text":"<ul> <li>Mode: run</li> <li>Condition: Task skipped because output files exist and are current</li> <li>Message: Includes existing file information</li> <li>Example: <code>SKIPS learn graph \u2192 outputs: /results/graph.xml (exists, 0.8MB)</code></li> <li>Special Case: For append-semantics files (e.g., metadata.json), only skips if action's expected contribution already exists</li> </ul>"},{"location":"architecture/logging_architecture_design/#would_skip","title":"WOULD_SKIP","text":"<ul> <li>Mode: dry-run  </li> <li>Condition: Task would be skipped because output files exist</li> <li>Message: Includes existing file information</li> <li>Example: <code>WOULD_SKIP learn graph \u2192 outputs: /results/graph.xml (exists, 0.8MB)</code></li> <li>Special Case: For append-semantics files, evaluates whether action's contribution would be added</li> </ul>"},{"location":"architecture/logging_architecture_design/#compare-mode-statuses","title":"Compare Mode Statuses","text":""},{"location":"architecture/logging_architecture_design/#identical","title":"IDENTICAL","text":"<ul> <li>Mode: compare</li> <li>Condition: Task re-executed, outputs identical to previous run</li> <li>Message: Includes comparison details</li> <li>Example: <code>IDENTICAL learn graph in 2.4s \u2192 outputs: /results/graph.xml (unchanged)</code></li> <li>Special Case: For append-semantics files, compares only the action's specific contribution</li> </ul>"},{"location":"architecture/logging_architecture_design/#different","title":"DIFFERENT","text":"<ul> <li>Mode: compare</li> <li>Condition: Task re-executed, outputs differ from previous run</li> <li>Message: Includes difference summary</li> <li>Example: <code>DIFFERENT learn graph in 2.3s \u2192 outputs: /results/graph.xml (5 edge changes)</code></li> <li>Special Case: For append-semantics files, reports differences in the action's contribution</li> </ul>"},{"location":"architecture/logging_architecture_design/#error-statuses","title":"Error Statuses","text":""},{"location":"architecture/logging_architecture_design/#invalid_uses","title":"INVALID_USES","text":"<ul> <li>Mode: All</li> <li>Condition: Action package specified in <code>uses:</code> not found</li> <li>Message: Available actions listed</li> <li>Example: <code>INVALID_USES unknown-action \u2192 Available: [causal-discovery, visualization]</code></li> </ul>"},{"location":"architecture/logging_architecture_design/#invalid_parameter","title":"INVALID_PARAMETER","text":"<ul> <li>Mode: All (detected in dry-run)</li> <li>Condition: Parameters in <code>with:</code> block are invalid for action</li> <li>Message: Specific parameter validation errors</li> <li>Example: <code>INVALID_PARAMETER learn graph \u2192 missing required: data_path, invalid: alpha=-0.1</code></li> </ul>"},{"location":"architecture/logging_architecture_design/#failed","title":"FAILED","text":"<ul> <li>Mode: run, compare</li> <li>Condition: Task execution threw unexpected exception</li> <li>Message: Exception details with traceback reference</li> <li>Example: <code>FAILED learn graph after 1.2s \u2192 FileNotFoundError: /data/missing.csv (see log line 1234)</code></li> </ul>"},{"location":"architecture/logging_architecture_design/#timed_out","title":"TIMED_OUT","text":"<ul> <li>Mode: run, compare  </li> <li>Condition: Task exceeded configured timeout</li> <li>Message: Timeout duration and partial results</li> <li>Example: <code>TIMED_OUT learn graph after 300s \u2192 timeout: 300s, partial outputs may exist</code></li> </ul>"},{"location":"architecture/logging_architecture_design/#architecture-design","title":"Architecture Design","text":""},{"location":"architecture/logging_architecture_design/#component-overview","title":"Component Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  WorkflowExecutor \u2502\u2500\u2500\u2500\u2192\u2502  ActionExecutor  \u2502\u2500\u2500\u2500\u2192\u2502     Action      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                       \u2502                       \u2502\n         \u25bc                       \u25bc                       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  WorkflowLogger  \u2502\u2190\u2500\u2500\u2500\u2502   StatusTracker  \u2502    \u2502   FileManager   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                       \u2502                       \n         \u25bc                       \u25bc                       \n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \n\u2502ProgressReporter  \u2502    \u2502  Summary Report  \u2502              \n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \n</code></pre>"},{"location":"architecture/logging_architecture_design/#core-classes","title":"Core Classes","text":""},{"location":"architecture/logging_architecture_design/#workflowlogger","title":"WorkflowLogger","text":"<pre><code>class WorkflowLogger:\n    \"\"\"Centralized logging with multiple output destinations.\"\"\"\n\n    def __init__(self, \n                 terminal: bool = True,\n                 log_file: Optional[Path] = None,\n                 log_level: LogLevel = LogLevel.SUMMARY):\n\n    def log_task(self, \n                action_name: str, \n                status: TaskStatus,\n                message: str,\n                runtime: Optional[float] = None,\n                inputs: Optional[Dict[str, Any]] = None,\n                outputs: Optional[str] = None,\n                job_info: Optional[str] = None) -&gt; None:\n\n    def start_progress(self, total_jobs: int, estimated_runtime: float) -&gt; None:\n    def update_progress(self, completed: int, current_job: str) -&gt; None:\n    def finish_progress(self) -&gt; None:\n</code></pre>"},{"location":"architecture/logging_architecture_design/#taskstatus","title":"TaskStatus","text":"<pre><code>class TaskStatus(Enum):\n    \"\"\"Enumeration of all possible task execution statuses.\"\"\"\n    EXECUTES = \"EXECUTES\"\n    WOULD_EXECUTE = \"WOULD_EXECUTE\"\n    SKIPS = \"SKIPS\"\n    WOULD_SKIP = \"WOULD_SKIP\"\n    IDENTICAL = \"IDENTICAL\"\n    DIFFERENT = \"DIFFERENT\"\n    INVALID_USES = \"INVALID_USES\"\n    INVALID_PARAMETER = \"INVALID_PARAMETER\"\n    FAILED = \"FAILED\"\n    TIMED_OUT = \"TIMED_OUT\"\n</code></pre>"},{"location":"architecture/logging_architecture_design/#actionexecutor","title":"ActionExecutor","text":"<pre><code>class ActionExecutor:\n    \"\"\"Handles action execution with logging and status determination.\"\"\"\n\n    def __init__(self, logger: WorkflowLogger, file_manager: FileManager):\n\n    def execute_action(self, \n                      action_name: str,\n                      action_class: Type[Action], \n                      inputs: Dict[str, Any],\n                      mode: str,\n                      context: WorkflowContext) -&gt; Dict[str, Any]:\n        \"\"\"Execute action with comprehensive status logging.\"\"\"\n\n    def should_skip(self, action_class: Type[Action], inputs: Dict[str, Any]) -&gt; bool:\n        \"\"\"Determine if action should be skipped based on existing outputs.\"\"\"\n\n    def estimate_runtime(self, action_class: Type[Action], inputs: Dict[str, Any]) -&gt; float:\n        \"\"\"Get runtime estimate from action for progress calculation.\"\"\"\n</code></pre>"},{"location":"architecture/logging_architecture_design/#modified-action-interface","title":"Modified Action Interface","text":"<pre><code>class Action(ABC):\n    \"\"\"Base class with optional logging support.\"\"\"\n\n    @abstractmethod\n    def run(self, \n           inputs: Dict[str, Any], \n           mode: str = \"dry-run\",\n           context: Optional[WorkflowContext] = None,\n           logger: Optional[WorkflowLogger] = None) -&gt; Dict[str, Any]:\n        \"\"\"Execute with optional logging capability.\"\"\"\n\n    def estimate_runtime(self, inputs: Dict[str, Any]) -&gt; float:\n        \"\"\"Provide runtime estimate for progress calculation.\"\"\"\n        return 1.0  # Default: 1 second estimate\n\n    def get_output_files(self, inputs: Dict[str, Any]) -&gt; List[str]:\n        \"\"\"Return list of output files this action will create.\"\"\"\n        return []  # Default: no specific outputs\n\n    def get_output_contribution_key(self, inputs: Dict[str, Any]) -&gt; Optional[str]:\n        \"\"\"Return key identifying this action's contribution in append-semantics files.\n\n        Used for files like metadata.json where multiple actions append content\n        rather than replacing the entire file. Return None for traditional files.\n        \"\"\"\n        return None  # Default: traditional replace-semantics\n\n    def has_existing_contribution(self, file_path: str, inputs: Dict[str, Any]) -&gt; bool:\n        \"\"\"Check if this action's contribution already exists in append-semantics file.\n\n        Only called when get_output_contribution_key() returns non-None.\n        Used to determine if action can skip execution.\n        \"\"\"\n        return False  # Default: not applicable\n</code></pre>"},{"location":"architecture/logging_architecture_design/#implementation-phases","title":"Implementation Phases","text":""},{"location":"architecture/logging_architecture_design/#phase-1-core-logging-infrastructure","title":"Phase 1: Core Logging Infrastructure","text":"<p>Target: Basic logging working with actions</p> <p>Components: - <code>TaskStatus</code> enum with all status definitions - <code>WorkflowLogger</code> class with file/terminal output - Modified <code>Action.run()</code> signature with optional logger parameter - Basic status logging from within actions</p> <p>Benefits: - Actions can immediately start logging task-level messages - Foundation for all subsequent logging features - No breaking changes (logger parameter optional)</p> <p>Example Usage:</p> <pre><code># Inside action.run()\nif logger:\n    logger.log_task(\n        action_name=self.name,\n        status=TaskStatus.EXECUTES,\n        message=\"learn graph\",\n        runtime=2.3,\n        inputs={\"dataset\": \"/data/asia.csv\"},\n        outputs=\"/results/graph.xml\"\n    )\n</code></pre>"},{"location":"architecture/logging_architecture_design/#phase-2-smart-execution-logic","title":"Phase 2: Smart Execution Logic","text":"<p>Target: Add output detection and skip logic</p> <p>Components: - <code>FileManager</code> class for output file detection - <code>ActionExecutor</code> class with skip logic - Runtime estimation interface - SKIP/WOULD_SKIP status implementation</p> <p>Benefits: - Intelligent workflow execution avoiding duplicate work - Progress estimation foundation - Proper dry-run vs run mode differentiation</p>"},{"location":"architecture/logging_architecture_design/#phase-3-cli-integration","title":"Phase 3: CLI Integration","text":"<p>Target: Complete user interface with logging</p> <p>Components: - CLI parameters: <code>--log-file</code>, <code>--log-level</code> - <code>ProgressReporter</code> with Click integration - Summary reports with status counts - User-friendly error display</p> <p>Benefits: - Real-world testing of logging design - Complete user experience - Feedback for design refinement</p>"},{"location":"architecture/logging_architecture_design/#phase-4-advanced-features","title":"Phase 4: Advanced Features","text":"<p>Target: Compare mode and monitoring</p> <p>Components: - File comparison for IDENTICAL/DIFFERENT - Resource monitoring (memory, CPU) - Timeout handling with TIMED_OUT - Enhanced progress displays</p>"},{"location":"architecture/logging_architecture_design/#design-decisions","title":"Design Decisions","text":""},{"location":"architecture/logging_architecture_design/#append-semantics-output-files","title":"Append-Semantics Output Files","text":""},{"location":"architecture/logging_architecture_design/#the-metadata-challenge","title":"The Metadata Challenge","text":"<p>Structure learning experiments often produce a <code>metadata.json</code> file that has append semantics rather than replace semantics:</p> <ul> <li>Initial Creation: Structure learning action creates metadata with algorithm details, runtime, parameters</li> <li>Subsequent Additions: Analysis actions add scores, metrics, validation results to the same file</li> <li>Incremental Growth: File grows over workflow execution rather than being replaced</li> </ul>"},{"location":"architecture/logging_architecture_design/#impact-on-task-status-determination","title":"Impact on Task Status Determination","text":"<p>Traditional Files (e.g., <code>graph.xml</code>): - File exists \u2192 task can skip - File missing \u2192 task must execute - Compare mode: entire file comparison</p> <p>Append-Semantics Files (e.g., <code>metadata.json</code>): - File exists \u2192 check if this action's contribution already present - Action's section missing \u2192 must execute (even if file exists) - Compare mode: compare only this action's contribution</p>"},{"location":"architecture/logging_architecture_design/#implementation-strategy","title":"Implementation Strategy","text":"<p>Action Interface Extension:</p> <pre><code>class Action(ABC):\n    def get_output_contribution_key(self, inputs: Dict[str, Any]) -&gt; Optional[str]:\n        \"\"\"Return key identifying this action's contribution in append-semantics files.\"\"\"\n        return None  # Most actions don't use append semantics\n\n    def has_existing_contribution(self, file_path: str, inputs: Dict[str, Any]) -&gt; bool:\n        \"\"\"Check if this action's contribution already exists in append-semantics file.\"\"\"\n        return False  # Default: not applicable\n</code></pre> <p>FileManager Enhancement:</p> <pre><code>class FileManager:\n    def should_skip_action(self, action: Action, inputs: Dict[str, Any]) -&gt; bool:\n        output_files = action.get_output_files(inputs)\n        for file_path in output_files:\n            if not os.path.exists(file_path):\n                return False  # Missing file, must execute\n\n            # Check append-semantics files\n            contrib_key = action.get_output_contribution_key(inputs)\n            if contrib_key and not action.has_existing_contribution(file_path, inputs):\n                return False  # Action's contribution missing\n\n        return True  # All outputs present (traditional or append-semantics)\n</code></pre> <p>Comparison Logic:</p> <pre><code>class FileManager:\n    def compare_outputs(self, action: Action, inputs: Dict[str, Any], \n                       old_run_dir: str, new_run_dir: str) -&gt; ComparisonResult:\n        \"\"\"Compare outputs, handling append-semantics files appropriately.\"\"\"\n\n        output_files = action.get_output_files(inputs)\n        for file_path in output_files:\n            contrib_key = action.get_output_contribution_key(inputs)\n            if contrib_key:\n                # Compare only this action's contribution\n                return self._compare_contribution(old_run_dir, new_run_dir, \n                                                file_path, contrib_key)\n            else:\n                # Traditional full-file comparison\n                return self._compare_full_file(old_run_dir, new_run_dir, file_path)\n</code></pre>"},{"location":"architecture/logging_architecture_design/#example-structure-learning-analysis-pipeline","title":"Example: Structure Learning + Analysis Pipeline","text":"<p>Workflow:</p> <pre><code>jobs:\n  structure_learning:\n    uses: causal-discovery\n    with: \n      algorithm: pc\n      data: data/asia.csv\n\n  analysis:\n    uses: graph-analysis  \n    with:\n      graph: ${{ jobs.structure_learning.outputs.graph }}\n</code></pre> <p>Execution Sequence:</p> <ol> <li>Structure Learning (first run):</li> <li>Creates: <code>/results/graph.xml</code>, <code>/results/metadata.json</code></li> <li><code>metadata.json</code>: <code>{\"structure_learning\": {\"algorithm\": \"pc\", \"runtime\": 2.3, ...}}</code></li> <li> <p>Status: <code>EXECUTES</code></p> </li> <li> <p>Analysis (first run):</p> </li> <li>Reads: <code>/results/graph.xml</code></li> <li>Appends to: <code>/results/metadata.json</code> </li> <li><code>metadata.json</code>: <code>{\"structure_learning\": {...}, \"analysis\": {\"scores\": {...}, \"metrics\": {...}}}</code></li> <li> <p>Status: <code>EXECUTES</code></p> </li> <li> <p>Structure Learning (second run):</p> </li> <li><code>metadata.json</code> exists but missing <code>structure_learning</code> section (analysis cleared it)</li> <li> <p>Status: <code>EXECUTES</code> (recreates structure learning contribution)</p> </li> <li> <p>Analysis (second run):</p> </li> <li><code>metadata.json</code> exists with <code>structure_learning</code> but missing <code>analysis</code> section</li> <li> <p>Status: <code>EXECUTES</code> (adds analysis contribution)</p> </li> <li> <p>Structure Learning (third run):</p> </li> <li><code>metadata.json</code> exists with <code>structure_learning</code> section present</li> <li>Status: <code>SKIPS</code> (contribution already exists)</li> </ol>"},{"location":"architecture/logging_architecture_design/#design-benefits","title":"Design Benefits","text":"<ul> <li>Precise Skip Logic: Actions only skip when their specific contribution exists</li> <li>Accurate Comparison: Compare mode focuses on action's actual output changes</li> <li>Incremental Execution: Workflows can build up metadata files progressively</li> <li>Clear Status Reporting: Users understand exactly what each action contributed</li> </ul>"},{"location":"architecture/logging_architecture_design/#why-task-level-logging","title":"Why Task-Level Logging?","text":"<ul> <li>Actions control granularity: Each action knows its internal complexity</li> <li>Flexible reporting: Single chart vs. hundreds of graphs handled appropriately  </li> <li>Real-time feedback: Users see progress as work happens</li> <li>Debugging capability: Specific task failures easily identified</li> </ul>"},{"location":"architecture/logging_architecture_design/#why-status-based-execution","title":"Why Status-Based Execution?","text":"<ul> <li>Mode-aware behavior: Same action behaves correctly in dry-run vs run</li> <li>Smart skipping: Avoid duplicate work, enable incremental execution</li> <li>Compare mode support: Foundation for regression testing workflows</li> <li>Clear error categorization: Different error types handled appropriately</li> </ul>"},{"location":"architecture/logging_architecture_design/#why-optional-logger-parameter","title":"Why Optional Logger Parameter?","text":"<ul> <li>Backward compatibility: Existing actions continue working unchanged</li> <li>Gradual adoption: Actions can add logging over time</li> <li>Testing simplicity: Tests can inject mock loggers easily</li> <li>Performance: No overhead for actions that don't use logging</li> </ul>"},{"location":"architecture/logging_architecture_design/#integration-points","title":"Integration Points","text":""},{"location":"architecture/logging_architecture_design/#workflowexecutor-integration","title":"WorkflowExecutor Integration","text":"<ul> <li>WorkflowExecutor creates WorkflowLogger based on CLI parameters</li> <li>Passes logger to ActionExecutor for all action executions</li> <li>Coordinates progress reporting across matrix jobs</li> <li>Handles summary report generation</li> </ul>"},{"location":"architecture/logging_architecture_design/#actionregistry-integration","title":"ActionRegistry Integration","text":"<ul> <li>Validates actions support logging interface during discovery</li> <li>Provides action metadata for runtime estimation</li> <li>Enables error status reporting for INVALID_USES</li> </ul>"},{"location":"architecture/logging_architecture_design/#exception-handling-integration","title":"Exception Handling Integration","text":"<ul> <li>All action exceptions caught by ActionExecutor</li> <li>Converted to appropriate FAILED status messages</li> <li>Stack traces logged with reference line numbers</li> <li>Graceful workflow continuation after failures</li> </ul>"},{"location":"architecture/logging_architecture_design/#testing-strategy","title":"Testing Strategy","text":""},{"location":"architecture/logging_architecture_design/#unit-testing","title":"Unit Testing","text":"<ul> <li>Mock logger captures for status verification</li> <li>FileManager testing with temporary directories</li> <li>ActionExecutor testing with mock actions</li> <li>Status transition testing for all scenarios</li> </ul>"},{"location":"architecture/logging_architecture_design/#integration-testing","title":"Integration Testing","text":"<ul> <li>End-to-end logging through WorkflowExecutor</li> <li>CLI parameter integration testing</li> <li>File output format validation</li> <li>Progress reporter integration testing</li> </ul>"},{"location":"architecture/logging_architecture_design/#performance-testing","title":"Performance Testing","text":"<ul> <li>Logging overhead measurement</li> <li>Large matrix workflow testing</li> <li>Memory usage monitoring</li> <li>File I/O performance validation</li> </ul>"},{"location":"architecture/logging_architecture_design/#future-extensions","title":"Future Extensions","text":""},{"location":"architecture/logging_architecture_design/#advanced-features","title":"Advanced Features","text":"<ul> <li>Log parsing tools: Analysis scripts for workflow execution logs</li> <li>Monitoring integration: Prometheus/Grafana metrics export</li> <li>Notification system: Slack/email alerts for workflow completion</li> <li>Dashboard integration: Web UI for workflow execution monitoring</li> </ul>"},{"location":"architecture/logging_architecture_design/#action-enhancements","title":"Action Enhancements","text":"<ul> <li>Detailed progress: Sub-task progress reporting within actions</li> <li>Resource prediction: More sophisticated runtime/memory estimation</li> <li>Parallel logging: Thread-safe logging for parallel action execution</li> <li>Custom status types: Action-specific status extensions</li> </ul>"},{"location":"architecture/logging_architecture_design/#conclusion","title":"Conclusion","text":"<p>This logging architecture provides: - Complete visibility into workflow execution at appropriate granularity - User-friendly experience with progress indication and clear error reporting - Developer-friendly integration with optional, backward-compatible interface - Foundation for advanced features like compare mode and monitoring - Testing-ready design with mockable interfaces and captured output</p> <p>The phased implementation approach allows for incremental development with real-world feedback at each stage, ensuring the final design meets actual user needs while maintaining system reliability and performance.</p>"},{"location":"architecture/matrix_expansion_design/","title":"CI Workflow Matrix Strategy Implementation","text":""},{"location":"architecture/matrix_expansion_design/#overview","title":"Overview","text":"<p>The matrix strategy implementation provides the GitHub Actions-style <code>strategy.matrix</code> functionality within the unified CI workflow engine. This is not a separate matrix expansion system, but rather a core component of the CI workflow architecture that handles the complex logic of experiment combination generation.</p>"},{"location":"architecture/matrix_expansion_design/#github-actions-matrix-strategy","title":"GitHub Actions Matrix Strategy","text":""},{"location":"architecture/matrix_expansion_design/#core-matrix-features","title":"Core Matrix Features","text":"<p>The implementation supports all GitHub Actions matrix strategy features:</p> <pre><code>strategy:\n  matrix:\n    algorithm: [\"PC\", \"GES\", \"LINGAM\"]\n    network: [\"asia\", \"sachs\"]\n    sample_size: [100, 500]\n\n  exclude:\n    - algorithm: \"LINGAM\"\n      network: \"alarm\"  # Remove specific combinations\n\n  include:\n    - algorithm: \"PC\"\n      network: \"asia\"\n      alpha: 0.01      # Add combinations with extra parameters\n\n  fail_fast: false      # Continue other jobs if one fails\n  max_parallel: 8       # Limit concurrent execution\n</code></pre>"},{"location":"architecture/matrix_expansion_design/#matrix-strategy-implementation","title":"Matrix Strategy Implementation","text":"<pre><code>import itertools\nfrom typing import Dict, List, Any\n\nclass CIMatrixStrategy:\n    \"\"\"Implement GitHub Actions matrix strategy within CI workflow engine.\"\"\"\n\n    def expand_matrix_strategy(self, strategy_config: Dict) -&gt; List[Dict]:\n        \"\"\"\n        Generate all job combinations from strategy.matrix configuration.\n\n        This is part of the CI workflow engine, not a separate expansion system.\n\n        Input:\n          strategy:\n            matrix:\n              algorithm: [\"PC\", \"GES\", \"LINGAM\"]\n              network: [\"asia\", \"sachs\"]\n              sample_size: [100, 500]\n\n        Output:\n          12 jobs with all combinations of algorithm \u00d7 network \u00d7 sample_size\n        \"\"\"\n        matrix_config = strategy_config.get(\"matrix\", {})\n        exclude_rules = strategy_config.get(\"exclude\", [])\n        include_rules = strategy_config.get(\"include\", [])\n\n        # Generate base cartesian product\n        base_jobs = self._generate_base_combinations(matrix_config)\n\n        # Apply exclude rules\n        filtered_jobs = self._apply_exclude_rules(base_jobs, exclude_rules)\n\n        # Apply include rules  \n        final_jobs = self._apply_include_rules(filtered_jobs, include_rules)\n\n        return final_jobs\n</code></pre>"},{"location":"architecture/matrix_expansion_design/#advanced-matrix-features","title":"Advanced Matrix Features","text":"<pre><code>def apply_exclude_rules(self, jobs: List[Dict], exclude_rules: List[Dict]) -&gt; List[Dict]:\n    \"\"\"\n    Remove jobs matching exclude patterns.\n\n    Example:\n      exclude:\n        - algorithm: \"LINGAM\"\n          network: \"alarm\"  # Remove LINGAM + alarm combination\n        - sample_size: 100\n          algorithm: \"GES\"   # Remove GES + 100 sample size combination\n    \"\"\"\n    filtered_jobs = []\n\n    for job in jobs:\n        should_exclude = False\n\n        for exclude_rule in exclude_rules:\n            if self._job_matches_pattern(job, exclude_rule):\n                should_exclude = True\n                break\n\n        if not should_exclude:\n            filtered_jobs.append(job)\n\n    return filtered_jobs\n\ndef apply_include_rules(self, jobs: List[Dict], include_rules: List[Dict]) -&gt; List[Dict]:\n    \"\"\"\n    Add specific job combinations even if not in matrix.\n\n    Example:\n      include:\n        - algorithm: \"PC\"\n          network: \"asia\"\n          sample_size: 50    # Add PC + asia + 50 even though 50 not in matrix\n          alpha: 0.01        # Add extra parameter for this combination\n    \"\"\"\n    extended_jobs = jobs.copy()\n\n    for include_rule in include_rules:\n        # Check if this combination already exists\n        if not any(self._job_matches_pattern(job, include_rule) for job in jobs):\n            extended_jobs.append(include_rule.copy())\n\n    return extended_jobs\n\ndef _job_matches_pattern(self, job: Dict, pattern: Dict) -&gt; bool:\n    \"\"\"Check if job matches the given pattern (all pattern keys must match).\"\"\"\n    for key, value in pattern.items():\n        if key not in job or job[key] != value:\n            return False\n    return True\n</code></pre>"},{"location":"architecture/matrix_expansion_design/#template-substitution-engine","title":"Template Substitution Engine","text":""},{"location":"architecture/matrix_expansion_design/#jinja2-integration-for-variable-substitution","title":"Jinja2 Integration for Variable Substitution","text":"<pre><code>import jinja2\nfrom typing import Dict, Any\n\nclass TemplateProcessor:\n    \"\"\"Handle GitHub Actions-style template substitution.\"\"\"\n\n    def __init__(self):\n        # Configure Jinja2 with GitHub Actions syntax\n        self.env = jinja2.Environment(\n            variable_start_string=\"${{\",\n            variable_end_string=\"}}\",\n            trim_blocks=True,\n            lstrip_blocks=True\n        )\n\n    def substitute_job_variables(self, template: str, job_context: Dict) -&gt; str:\n        \"\"\"\n        Process template with job-specific variables.\n\n        Example:\n          template: \"network_${{ matrix.network }}_${{ matrix.sample_size }}.csv\"\n          job_context: {\"matrix\": {\"network\": \"asia\", \"sample_size\": 100}}\n          result: \"network_asia_100.csv\"\n        \"\"\"\n        jinja_template = self.env.from_string(template)\n        return jinja_template.render(**job_context)\n\n    def build_job_context(self, job: Dict, workflow_env: Dict, \n                         step_outputs: Dict = None) -&gt; Dict:\n        \"\"\"\n        Build complete context for template substitution.\n\n        Available variables:\n        - matrix.*: Matrix variables for current job\n        - env.*: Environment variables\n        - steps.*: Outputs from previous steps\n        - github.*: GitHub Actions compatibility variables\n        \"\"\"\n        context = {\n            \"matrix\": job,\n            \"env\": workflow_env,\n            \"github\": {\n                \"workspace\": \"/tmp/causaliq-workspace\",  # Configurable\n                \"run_id\": \"12345\",  # Generated\n                \"run_number\": \"1\"\n            }\n        }\n\n        if step_outputs:\n            context[\"steps\"] = step_outputs\n\n        return context\n</code></pre>"},{"location":"architecture/matrix_expansion_design/#integration-with-dask-task-graphs","title":"Integration with DASK Task Graphs","text":""},{"location":"architecture/matrix_expansion_design/#job-dask-task-conversion","title":"Job \u2192 DASK Task Conversion","text":"<pre><code>import dask\nfrom typing import Dict, List, Callable\n\nclass DaskTaskGraphBuilder:\n    \"\"\"Convert matrix jobs into DASK computation graph.\"\"\"\n\n    def build_workflow_graph(self, jobs: List[Dict], \n                            workflow_definition: Dict) -&gt; Dict:\n        \"\"\"\n        Convert expanded matrix jobs into DASK task graph.\n\n        Each job becomes a series of DASK tasks corresponding to workflow steps.\n        Dependencies between steps are handled through DASK delayed mechanism.\n        \"\"\"\n        graph = {}\n\n        for job_idx, job in enumerate(jobs):\n            job_id = f\"job_{job_idx}\"\n            step_results = {}\n\n            # Process each step in the workflow\n            for step_idx, step in enumerate(workflow_definition[\"steps\"]):\n                step_id = f\"{job_id}_step_{step_idx}\"\n                step_name = step.get(\"name\", f\"step_{step_idx}\")\n\n                # Build step task with dependencies\n                step_task = self._build_step_task(\n                    step=step,\n                    job_context=job,\n                    previous_results=step_results,\n                    workflow_env=workflow_definition.get(\"env\", {})\n                )\n\n                graph[step_id] = step_task\n                step_results[step_name] = step_id\n\n        return graph\n\n    def _build_step_task(self, step: Dict, job_context: Dict, \n                        previous_results: Dict, workflow_env: Dict) -&gt; tuple:\n        \"\"\"\n        Build individual DASK task for workflow step.\n\n        Returns tuple: (function, *args) suitable for DASK graph\n        \"\"\"\n        action_name = step[\"uses\"]\n        action_inputs = step.get(\"with\", {})\n\n        # Substitute templates in action inputs\n        template_processor = TemplateProcessor()\n        full_context = template_processor.build_job_context(\n            job=job_context,\n            workflow_env=workflow_env,\n            step_outputs=previous_results\n        )\n\n        substituted_inputs = {}\n        for key, value in action_inputs.items():\n            if isinstance(value, str):\n                substituted_inputs[key] = template_processor.substitute_job_variables(\n                    value, full_context\n                )\n            else:\n                substituted_inputs[key] = value\n\n        # Return DASK task tuple\n        return (\n            self._execute_action,\n            action_name,\n            substituted_inputs,\n            job_context,\n            previous_results\n        )\n\n    def _execute_action(self, action_name: str, inputs: Dict, \n                       job_context: Dict, previous_results: Dict) -&gt; Any:\n        \"\"\"Execute workflow action with proper error handling.\"\"\"\n        from causaliq_workflow.actions import ActionRegistry\n\n        registry = ActionRegistry()\n        return registry.execute_action(action_name, inputs)\n</code></pre>"},{"location":"architecture/matrix_expansion_design/#performance-optimisations","title":"Performance Optimisations","text":""},{"location":"architecture/matrix_expansion_design/#intelligent-job-batching","title":"Intelligent Job Batching","text":"<pre><code>class MatrixOptimiser:\n    \"\"\"Optimise matrix job execution for performance.\"\"\"\n\n    def batch_similar_jobs(self, jobs: List[Dict], max_batch_size: int = 10) -&gt; List[List[Dict]]:\n        \"\"\"\n        Group jobs by similarity for efficient resource usage.\n\n        Jobs using same algorithm/package can share initialisation costs.\n        Jobs using same dataset can share data loading costs.\n        \"\"\"\n        batches = []\n\n        # Group by algorithm package for shared initialisation\n        algorithm_groups = {}\n        for job in jobs:\n            algorithm = job.get(\"algorithm\", \"unknown\")\n            if algorithm not in algorithm_groups:\n                algorithm_groups[algorithm] = []\n            algorithm_groups[algorithm].append(job)\n\n        # Further group by dataset within each algorithm\n        for algorithm, algo_jobs in algorithm_groups.items():\n            dataset_groups = {}\n            for job in algo_jobs:\n                dataset = job.get(\"network\", \"unknown\")\n                if dataset not in dataset_groups:\n                    dataset_groups[dataset] = []\n                dataset_groups[dataset].append(job)\n\n            # Create batches within each dataset group\n            for dataset, dataset_jobs in dataset_groups.items():\n                for i in range(0, len(dataset_jobs), max_batch_size):\n                    batch = dataset_jobs[i:i + max_batch_size]\n                    batches.append(batch)\n\n        return batches\n\n    def estimate_job_resources(self, job: Dict) -&gt; Dict[str, Any]:\n        \"\"\"\n        Estimate resource requirements for job.\n\n        Used for intelligent DASK worker allocation.\n        \"\"\"\n        algorithm = job.get(\"algorithm\", \"\")\n        sample_size = job.get(\"sample_size\", 100)\n        network_size = self._estimate_network_complexity(job.get(\"network\", \"\"))\n\n        # Simple heuristics - can be refined with benchmarking data\n        memory_mb = max(500, sample_size * network_size * 0.1)\n        cpu_time_minutes = max(1, (sample_size * network_size) / 10000)\n\n        return {\n            \"memory_mb\": memory_mb,\n            \"cpu_time_minutes\": cpu_time_minutes,\n            \"io_intensive\": algorithm.lower() in [\"ges\", \"gies\"],  # Score-based algorithms\n            \"cross_language\": self._requires_cross_language_bridge(algorithm)\n        }\n</code></pre>"},{"location":"architecture/matrix_expansion_design/#integration-with-ci-workflow-engine","title":"Integration with CI Workflow Engine","text":""},{"location":"architecture/matrix_expansion_design/#unified-architecture","title":"Unified Architecture","text":"<p>The matrix strategy implementation is a core component of the unified CI workflow engine, not a separate system:</p> <pre><code>class CIWorkflowEngine:\n    \"\"\"Unified CI workflow engine with integrated matrix strategy support.\"\"\"\n\n    def __init__(self):\n        self.matrix_strategy = CIMatrixStrategy()\n        self.template_processor = TemplateProcessor()\n        self.action_registry = ActionRegistry()\n        self.dask_builder = DaskTaskGraphBuilder()\n\n    def execute_workflow(self, workflow_yaml: str) -&gt; WorkflowResult:\n        \"\"\"Execute complete CI workflow with matrix strategy expansion.\"\"\"\n        workflow_def = self.parse_workflow(workflow_yaml)\n\n        # Expand matrix strategy if present\n        if \"strategy\" in workflow_def:\n            jobs = self.matrix_strategy.expand_matrix_strategy(workflow_def[\"strategy\"])\n        else:\n            jobs = [{}]  # Single job with empty context\n\n        # Build DASK task graph for all jobs\n        task_graph = self.dask_builder.build_workflow_graph(jobs, workflow_def)\n\n        # Execute with DASK\n        return self._execute_dask_graph(task_graph)\n</code></pre>"},{"location":"architecture/matrix_expansion_design/#key-design-principles","title":"Key Design Principles","text":"<ol> <li>Unified System: Matrix strategy is part of CI workflow engine, not separate component</li> <li>GitHub Actions Compatibility: Full compatibility with GitHub Actions matrix syntax</li> <li>DASK Integration: Matrix jobs convert directly to DASK task graph</li> <li>Template Support: Full Jinja2 template substitution with <code>${{ matrix.variable }}</code> syntax</li> <li>Resource Management: Matrix features like <code>max_parallel</code> map to DASK execution controls</li> </ol> <p>This implementation provides the sophisticated matrix capabilities of GitHub Actions while maintaining the unified architecture of the CI workflow system for causal discovery research.</p>"},{"location":"architecture/overview/","title":"CausalIQ Workflow - Technical Architecture","text":""},{"location":"architecture/overview/#architectural-vision-configuration-free-research-reproducibility-platform","title":"Architectural Vision: Configuration-Free Research Reproducibility Platform","text":""},{"location":"architecture/overview/#core-architecture-principles","title":"Core Architecture Principles","text":"<p>CausalIQ Workflow is designed as a zero-configuration workflow orchestration engine that enables reproducible causal discovery research through:</p> <ol> <li>Entry Point-Based Action Discovery: Actions are automatically discovered    via Python entry points - no configuration files required</li> <li>Lazy Loading: Action classes are loaded on first use, avoiding circular    import issues</li> <li>Sequential Steps with Matrix Expansion: Simple, predictable workflow    execution with powerful parameterisation</li> <li>Conservative Execution: Actions skip work if outputs already exist,    enabling safe workflow restart and efficient re-runs</li> <li>Mode-Based Operation: <code>--mode=dry-run|run|compare</code> provides validation,    execution, and functional testing capabilities</li> <li>Plugin Architecture: Third-party packages can register actions via    entry points. Results are passed between packages using open standard    formats e.g. JSON and .graphml</li> <li>Implicit Parameter Passing: CLI parameters flow through workflows    without formal definitions</li> <li>Action-Level Validation: Each action validates its own inputs    (integrated with dry-run capability)</li> <li>Workflow Composition: Workflows can call other workflows via <code>cqflow</code>    commands, enabling complex research workflows</li> <li>Workflow Cache Storage: Results stored in SQLite-based Workflow Caches     with matrix values as keys, enabling compact storage and fast lookup</li> </ol>"},{"location":"architecture/overview/#entry-point-based-auto-discovery","title":"Entry Point-Based Auto-Discovery","text":"<p>How Action Discovery Works: When a workflow runs, the system discovers actions through Python entry points. Packages register their actions in <code>pyproject.toml</code>:</p> <pre><code>[project.entry-points.\"causaliq.actions\"]\nmy-action = \"my_package:ActionProvider\"\n</code></pre> <p>Actions are discovered at startup (metadata only) and loaded lazily on first use. This avoids circular imports since action packages depend on <code>causaliq-workflow</code>.</p>"},{"location":"architecture/overview/#research-reproducibility-pattern","title":"Research Reproducibility Pattern","text":"<p>Paper Reproduction = Workflow-of-Workflows where: - Top-level workflow defines paper reproduction strategy - Component workflows handle specific analyses (structure learning,   visualisation, etc.) - causaliq-papers processes workflow dependencies to generate targeted   execution plans - causaliq-workflow executes the optimised workflow graph</p>"},{"location":"architecture/overview/#example-simplified-workflow-architecture","title":"Example: Simplified Workflow Architecture","text":"<pre><code># paper-reproduction.yml (top-level workflow)\nid: \"peters2023causal-reproduction\"\nmatrix:\n  model: [\"asia\", \"cancer\"]\n  algorithm: [\"pc\", \"ges\", \"fci\"]\n\n# Workflow Cache stores all results - matrix values used as cache keys\nworkflow_cache: \"results/{{id}}_cache.db\"\n\nsteps:\n  - name: \"Structure Learning\"\n    uses: \"causaliq-discovery\"\n    with:\n      algorithm: \"{{algorithm}}\"\n      model: \"models/{{model}}/{{model}}.json\"\n      # No output path needed - results written to workflow_cache\n      # Cache key derived from matrix: {network: \"asia\", algorithm: \"pc\"}\n\n  - name: \"Analysis\"\n    uses: \"causaliq-analysis\"\n    with:\n      operation: \"evaluate-graph\"\n      # Graph retrieved from workflow_cache using matrix key\n\n  - name: \"Generate Figures\"\n    uses: \"causaliq-visualisation\"\n    with:\n      output: \"results/{{id}}/figures\"  # Figures still go to filesystem\n</code></pre>"},{"location":"architecture/overview/#conservative-execution-mode-control","title":"Conservative Execution &amp; Mode Control","text":"<pre><code># CLI execution modes\ncqflow workflow.yml --mode=dry-run    # Default: validate and preview\ncqflow workflow.yml --mode=run        # Execute workflow (skip if outputs exist)\ncqflow workflow.yml --mode=compare    # Re-execute and compare with existing\n</code></pre>"},{"location":"architecture/overview/#action-intelligence-efficiency","title":"Action Intelligence &amp; Efficiency","text":"<pre><code># Actions support robust execution patterns with validation\naction.run(inputs, dry_run=True)    # (a) Validate and preview execution\naction.run(inputs, force=False)     # (b) Skip if output exists (conservative)\naction.compare(inputs)              # (c) Regenerate and compare with filesystem\n\n# Implicit parameter passing - no formal definitions needed\nclass StructureLearnerAction(BaseActionProvider):\n    def run(self, inputs, matrix_job=None, dry_run=False, **kwargs):\n        # Action handles its own validation\n        self.validate_inputs(inputs)\n\n        # Conservative execution: skip if outputs exist\n        if not inputs.force and self.outputs_exist(inputs):\n            return self.load_existing_outputs(inputs)\n\n        if dry_run:\n            return self.simulate_execution(inputs)\n        return self.learn_structure(inputs)\n</code></pre>"},{"location":"architecture/overview/#system-overview","title":"System Overview","text":"<p>The causaliq-workflow serves as the orchestration layer within the CausalIQ ecosystem, coordinating causal discovery experiments through GitHub Actions-inspired YAML workflows. This architecture models causal discovery experiments as familiar CI/CD workflows, providing unprecedented flexibility while leveraging proven workflow patterns.</p>"},{"location":"architecture/overview/#auto-discovery-architecture-how-it-works","title":"Auto-Discovery Architecture: How It Works","text":""},{"location":"architecture/overview/#the-discovery-process-step-by-step","title":"The Discovery Process: Step-by-Step","text":""},{"location":"architecture/overview/#1-workflow-execution-begins","title":"1. Workflow Execution Begins","text":"<p>When you run <code>causaliq-workflow my-experiment.yml</code>, the system: - Creates a new <code>ActionRegistry</code> instance - Triggers the automatic discovery process - Scans all installed Python packages for actions</p>"},{"location":"architecture/overview/#2-package-scanning-phase","title":"2. Package Scanning Phase","text":"<p>The registry uses Python's module introspection to: - Iterate through all importable modules using <code>pkgutil.iter_modules()</code> - Attempt to import each module safely (catching import errors) - Look for modules that export an 'Action' class</p>"},{"location":"architecture/overview/#3-convention-based-registration","title":"3. Convention-Based Registration","text":"<p>For each discovered module, the system: - Checks if the module has an 'ActionProvider' attribute - Verifies that it's a subclass of <code>causaliq_workflow.action.BaseActionProvider</code> - Registers the action using the module name as the action identifier - Builds a runtime lookup table: <code>{action_name: ActionProvider_class}</code></p>"},{"location":"architecture/overview/#4-workflow-resolution","title":"4. Workflow Resolution","text":"<p>When a workflow step specifies <code>uses: \"my-custom-action\"</code>: - The registry looks up \"my-custom-action\" in the registered actions - Instantiates the corresponding ActionProvider class - Passes workflow parameters to the action's <code>run()</code> method</p>"},{"location":"architecture/overview/#zero-configuration-plugin-pattern","title":"Zero-Configuration Plugin Pattern","text":""},{"location":"architecture/overview/#creating-a-new-action-package","title":"Creating a New Action Package","text":"<p>Developers create action packages by following a simple convention:</p> <p>Step 1: Package Structure</p> <pre><code>my-custom-action/\n\u251c\u2500\u2500 pyproject.toml           # Standard Python package config\n\u251c\u2500\u2500 my_custom_action/        # Package directory  \n\u2502   \u2514\u2500\u2500 __init__.py         # Must export 'ActionProvider' class\n\u2514\u2500\u2500 README.md\n</code></pre> <p>Step 2: Action Implementation</p> <pre><code># my_custom_action/__init__.py\nfrom causaliq_core import CausalIQActionProvider\n\nclass ActionProvider(CausalIQActionProvider):  # Must be named 'ActionProvider'\n    name = \"my-custom-action\"\n    description = \"Performs custom analysis\"\n\n    def run(self, action, parameters, mode=\"dry-run\", context=None, logger=None):\n        # Action logic here\n        return (\"success\", {\"result\": \"analysis complete\"}, [])\n</code></pre> <p>Step 3: Installation &amp; Discovery</p> <pre><code>pip install my-custom-action    # Install the package\ncausaliq-workflow my-workflow.yml  # Action automatically discovered\n</code></pre>"},{"location":"architecture/overview/#why-this-works","title":"Why This Works","text":"<ul> <li>No configuration files: No registry.json, no plugin.xml, no setup scripts</li> <li>Standard Python packaging: Uses familiar pyproject.toml and pip install</li> <li>Immediate availability: Actions become available as soon as the package is installed</li> <li>Namespace safety: 'ActionProvider' avoids conflicts with generic 'Action' classes in other packages</li> <li>Version management: Standard semantic versioning through package versions</li> </ul>"},{"location":"architecture/overview/#workflow-caches","title":"Workflow Caches","text":"<p>Workflow caches are filesystem objects which store results from workflow actions. They are SQLite-based with matrix values as keys, enabling compact storage and fast lookup.</p> <p>Workflow caches are built on causaliq-core's <code>TokenCache</code> infrastructure, with JSON tokenisation via <code>JsonCompressor</code> for compact storage.</p> <p>Workflow actions typically write their outputs as entries in workflow caches. Each entry consists of:</p> <ul> <li>metadata describing the action, for example, the <code>algorithm</code> used in a   structure learning action, or the <code>llm_model</code> used in an LLM graph generate   action. The JSON is tokenised when stored in the cache for compactness.</li> <li>optionally, the entry may contain one or more data objects, for example, a   graph produced by structure learning or generated by an LLM. The objects in   an LLM entry are listed in a \"manifest\" section of the metadata and specify   the ActionProvider used to create each data object, e.g. \"causaliq-discovery\"   and the data object type, e.g. \"graph\".</li> </ul> <p>DataProvider actions check if their output is already present in the workflow cache and if so do not re-generate the output. This provides the basis for the conservative execution and reproducibility that CausalIQ Workflows offer.</p> <p>Workflow caches often contain the results from a large series of experiments - for example, structure learning over a range of models (networks), algorithms and sample sizes. They may therefore contain tens of thousands of entries or more.</p> <p>Workflow actions can read entries from workflow caches too. Thus, they provide the mechanism for workflow actions to provide their output as the input to other actions, a typical example is that a <code>causaliq-discovery</code> action might write an entry with a learned graph object, and then a subsequent <code>causaliq-analysis</code> action might read that graph object and evaluate its structural accuracy, adding, for example the F1 value to the entry's metadata.</p> <p>The design focus for workflow caches is speed and compactness, and because of this, their contents are not human readable. However, given that transparency is another goal of the CausalIQ ecosystem, CausalIQ ActionProviders are required to implement two special methods:</p> <ul> <li><code>serialise(data_type, data)</code> which converts internal data objects to open-standard format strings such as GraphML.</li> <li><code>deserialise(data_type, content)</code> which converts open-standard format strings back to internal data objects.</li> </ul> <p>The <code>causaliq-workflow</code> CLI provides an <code>export_cache</code> command which exports the whole contents of a workflow cache to a directory structure of standard-format files which are human-readable and processable by third party tools. It is this open-standards export of the workflow cache that would be stored on Zenodo for example. Similarly, an <code>import_cache</code> command provides the capability to convert these open-standard format files back into workflow caches, ready to participate in CausalIQ workflows.</p> <p>Importantly, the <code>serialise</code> and <code>deserialise</code> methods work with string content rather than files, which provides the means by which ActionProviders can read workflow cache entries without understanding the compressed internal format of the entry cache. Third-parties writing actions therefore need only be able to process standards-based formats as inputs, and need not, for example, use any CausalIQ code, as long as they implement the BaseActionProvider interface.</p> <p>The <code>causaliq-workflow</code> package itself is responsible for:</p> <ul> <li>converting internal format cache entries to and from open-standards format which ActionProviders consume and produce. It does this by looking at the entry's metadata, and using the appropriate packages and data object types to convert the internal format to the open-standards that any ActionProvider can consume.</li> <li>managing conservative execution</li> </ul>"},{"location":"architecture/overview/#serialise-and-deserialise-method-specifications","title":"Serialise and Deserialise Method Specifications","text":"<p>ActionProviders that produce data objects (e.g., graphs) must implement <code>serialise()</code> and <code>deserialise()</code> methods. These methods convert between internal data objects and open-standard format strings. The format used is determined by the provider implementation (e.g., GraphML for graphs).</p>"},{"location":"architecture/overview/#serialise-method","title":"serialise Method","text":"<p>Converts internal data objects to open-standard format strings.</p> <p>Parameters: - <code>data_type</code> (str): Type of data object (e.g., \"graph\") - <code>data</code> (Any): The data object to serialise</p> <p>Returns: - <code>str</code>: Open-standard format string representation</p> <p>Example:</p> <pre><code>from causaliq_knowledge import ActionProvider\n\nprovider = ActionProvider()\ngraph = provider.run(\"generate_graph\", params, mode=\"run\")[\"graph\"]\n\n# Serialise to GraphML string\ngraphml_str = provider.serialise(\"graph\", graph)\n</code></pre>"},{"location":"architecture/overview/#deserialise-method","title":"deserialise Method","text":"<p>Converts open-standard format strings to internal data objects.</p> <p>Parameters: - <code>data_type</code> (str): Type of data object (e.g., \"graph\") - <code>content</code> (str): Open-standard format string</p> <p>Returns: - The deserialised data object</p> <p>Example:</p> <pre><code># Read GraphML from file\nwith open(\"graph.graphml\") as f:\n    graphml_content = f.read()\n\n# Deserialise to internal object\ngraph = provider.deserialise(\"graph\", graphml_content)\n</code></pre>"},{"location":"architecture/overview/#in-memory-data-flow","title":"In-Memory Data Flow","text":"<p>When <code>causaliq-workflow</code> needs to pass data between actions from different providers, it uses these methods:</p> <pre><code># Producer action writes graph to cache\n# Consumer action from different package needs the graph...\n\n# 1. Workflow retrieves graph object from producer action result\ngraph = producer_result[\"graph\"]\n\n# 2. Serialise to open-standard format string\ngraph_str = producer_provider.serialise(\"graph\", graph)\n\n# 3. Consumer deserialises to its internal format (if different)\nconsumer_graph = consumer_provider.deserialise(\"graph\", graph_str)\n\n# 4. Consumer processes the graph\nconsumer_provider.run(\n    action=\"analyse\",\n    parameters={\"graph\": consumer_graph, ...}\n)\n</code></pre> <p>This architecture ensures: - Decoupling: Consumers don't need producer's internal format knowledge - Open standards: All data exchanged as JSON, GraphML, etc. - Third-party friendly: External packages only implement standard I/O</p>"},{"location":"architecture/overview/#core-architectural-decisions","title":"Core Architectural Decisions","text":""},{"location":"architecture/overview/#github-actions-foundation","title":"GitHub Actions Foundation","text":"<p>The architecture is built on GitHub Actions workflow patterns, adapted for causal discovery:</p> <pre><code>name: \"Causal Discovery Experiment\"\nid: \"asia-comparison-001\"\ndata_root: \"data\"\nworkflow_cache: \"results/{{id}}_cache.db\"  # All results stored here\n\nmatrix:\n  dataset: [\"asia\", \"cancer\"]  \n  algorithm: [\"pc\", \"ges\"]\n\nsteps:\n  - name: \"Structure Learning\"\n    uses: \"dummy-structure-learner\"\n    with:\n      alpha: 0.05\n      max_iter: 1000\n      # Results cached with key: {dataset, algorithm}\n</code></pre>"},{"location":"architecture/overview/#action-based-components","title":"Action-Based Components","text":"<p>Actions are reusable workflow components with semantic versioning. All actions inherit from <code>CausalIQActionProvider</code> defined in causaliq-core:</p> <pre><code>from causaliq_core import CausalIQActionProvider\n\nclass Action(CausalIQActionProvider):\n    \"\"\"Base class for all workflow actions.\"\"\"\n\n    name: str                    # Action identifier\n    version: str                 # Semantic version\n    description: str             # Human description\n    inputs: Dict[str, ActionInput]   # Type-safe inputs\n    outputs: Dict[str, str]      # Output descriptions\n\n    def run(\n        self,\n        action: str,\n        parameters: Dict[str, Any],\n        mode: str = \"dry-run\",\n        context: Optional[Any] = None,\n        logger: Optional[Any] = None,\n    ) -&gt; ActionResult:\n        \"\"\"Execute the action with given inputs.\"\"\"\n</code></pre>"},{"location":"architecture/overview/#integration-with-causaliq-ecosystem","title":"Integration with CausalIQ Ecosystem","text":""},{"location":"architecture/overview/#package-coordination","title":"Package Coordination","text":"<ul> <li>causaliq-discovery: Core algorithms integrated as package plugins</li> <li>causaliq-knowledge: Knowledge provision via action-based architecture</li> <li>causaliq-analysis: Statistical analysis actions and workflow post-processing</li> <li>causaliq-research: Configuration and result storage with CI workflow metadata</li> </ul>"},{"location":"architecture/overview/#development-standards","title":"Development Standards","text":"<ul> <li>GitHub Actions schema compliance: Official JSON schema for validation</li> <li>Action versioning: Semantic versioning for all reusable actions</li> <li>CausalIQ integration standards: Plugin architecture, result standardisation</li> <li>79-character line limit: All code adheres to CausalIQ formatting standards</li> <li>Type safety: Full MyPy type checking with strict configuration</li> </ul>"},{"location":"architecture/overview/#design-patterns","title":"Design Patterns","text":""},{"location":"architecture/overview/#yaml-first-configuration","title":"YAML-First Configuration","text":"<ul> <li>All workflow functionality expressible through YAML</li> <li>External workflow definitions without code changes</li> <li>Clear, actionable error messages for configuration issues</li> </ul>"},{"location":"architecture/overview/#package-level-plugin-architecture","title":"Package-Level Plugin Architecture","text":"<ul> <li>Dynamic discovery and registration of algorithm packages</li> <li>Cross-language bridge management at package level</li> <li>Preference resolution for algorithm conflicts</li> </ul>"},{"location":"architecture/overview/#action-based-composability","title":"Action-Based Composability","text":"<ul> <li>Reusable, versioned workflow components</li> <li>Standardised input/output interfaces</li> <li>Community potential for shared actions</li> </ul> <p>This architecture transforms causal discovery workflow definition from domain-specific patterns into familiar CI/CD workflows, dramatically reducing the learning curve while providing enterprise-grade features for research.</p>"},{"location":"architecture/workflow_cache_design/","title":"Workflow Cache Design","text":""},{"location":"architecture/workflow_cache_design/#overview","title":"Overview","text":"<p>Workflow Caches provide persistent storage for workflow step results in a compact, fast SQLite database. This enables conservative execution (skipping work if results exist) and supports reproducibility of research over many years.</p> <p>The cache is built on common infrastructure (<code>TokenCache</code>, <code>JsonEncoder</code>) shared with LLM response caching. This common infrastructure currently resides in causaliq-knowledge but will migrate to causaliq-core.</p>"},{"location":"architecture/workflow_cache_design/#design-goals","title":"Design Goals","text":"Goal Description Compact storage SQLite with tokenised blobs, not unwieldy file trees Fast lookup Quick existence checks for conservative execution Flexibility Multiple entry types without schema changes Open export Convert to GraphML, JSON, CSV for archival Reproducibility Results persist and can be replicated years later"},{"location":"architecture/workflow_cache_design/#architecture-decisions","title":"Architecture Decisions","text":""},{"location":"architecture/workflow_cache_design/#cache-entry-structure","title":"Cache Entry Structure","text":"<p>Each cache entry consists of a metadata blob and multiple data blobs:</p> <p>| Component | Storage | Purpose | |-----------|---------|---------|| | Metadata blob | Tokenised JSON | Provenance, metrics, type-specific attributes | | Data blobs | Type-specific encoded objects | One or more result objects (e.g., graph, trace) |</p> <p>For example, a structure learning result might include:</p> <ul> <li>A <code>graph</code> data blob (the learned SDG)</li> <li>A <code>trace</code> data blob (iteration-by-iteration execution trace)</li> <li>Metadata (algorithm, timing, scores, hyperparameters)</li> </ul> <p>This design:</p> <ul> <li>Supports composite results with multiple artefacts</li> <li>Avoids schema changes when adding new result types</li> <li>Keeps object-level attributes (e.g., edge confidences) in metadata</li> <li>Allows flexible metadata structure per entry type</li> </ul>"},{"location":"architecture/workflow_cache_design/#cache-key-strategy","title":"Cache Key Strategy","text":""},{"location":"architecture/workflow_cache_design/#schema-binding","title":"Schema Binding","text":"<p>A Workflow Cache is bound to its matrix variable structure (the set of variable names), not to specific values. This means:</p> Change Permitted Reason Add new value to existing variable \u2713 Yes New entries created, existing entries unchanged Remove value from variable \u2713 Yes Existing entries remain accessible Add new matrix variable \u2717 No Changes key structure, requires new cache Remove matrix variable \u2717 No Changes key structure, requires new cache Rename matrix variable \u2717 No Changes key structure, requires new cache <p>Example - permitted change:</p> <pre><code># Original\nmatrix:\n  algorithm: [pc, ges]\n\n# Extended (same cache works)\nmatrix:\n  algorithm: [pc, ges, fci, tabu]  # Added values\n</code></pre> <p>Example - requires new cache:</p> <pre><code># Original\nmatrix:\n  algorithm: [pc, ges]\n\n# Extended with new dimension (new cache needed)\nmatrix:\n  algorithm: [pc, ges]\n  network: [asia, cancer]  # New variable\n</code></pre> <p>This constraint keeps the implementation simple. Future versions may support schema migration if needed.</p>"},{"location":"architecture/workflow_cache_design/#key-derivation","title":"Key Derivation","text":"<p>The cache key is a SHA-256 hash (truncated to 16 hex characters) of the workflow matrix variable values for that step execution:</p> <pre><code># Example: workflow with matrix expansion\nmatrix_values = {\n    \"network\": \"cancer\",\n    \"llm_model\": \"groq/llama-3.1-8b\",\n    \"prompt_detail\": \"standard\"\n}\nkey = sha256(json.dumps(matrix_values, sort_keys=True))[:16]\n</code></pre> <p>Rationale: Matrix values capture the experimental design - they define what distinguishes one result from another. This aligns cache keys with research intent rather than implementation details.</p> <p>Deferred: Complex key strategies (action params, step names) can be added if the simple approach proves insufficient.</p>"},{"location":"architecture/workflow_cache_design/#edge-confidences","title":"Edge Confidences","text":"<p>Edge confidences from LLM graph generation are stored in metadata JSON, not as SDG edge attributes:</p> <pre><code>{\n  \"provenance\": {\n    \"generator\": \"llm\",\n    \"model\": \"groq/llama-3.1-8b-instant\",\n    \"timestamp\": \"2026-02-04T10:30:00Z\"\n  },\n  \"edge_confidences\": {\n    \"A-&gt;B\": 0.95,\n    \"B-&gt;C\": 0.72\n  },\n  \"evaluation\": {\n    \"shd\": 3,\n    \"precision\": 0.85\n  }\n}\n</code></pre> <p>Rationale: Edge confidences are method-specific (LLM output), not intrinsic graph properties. Keeping them in metadata maintains consistency across diverse result types (graphs, traces, junction trees, etc.).</p>"},{"location":"architecture/workflow_cache_design/#sdg-changes-minimal","title":"SDG Changes (Minimal)","text":"<p>The SDG class requires only:</p> <ul> <li><code>compress()</code> / <code>decompress()</code> methods for compact blob representation</li> <li><code>to_graphml()</code> / <code>from_graphml()</code> for open format export</li> </ul> <p>Edge attributes are not added to SDG in this release.</p>"},{"location":"architecture/workflow_cache_design/#graph-encoding-format","title":"Graph Encoding Format","text":"<p>The <code>SDG.compress()</code> method produces a compact binary representation that leverages the Workflow Cache's shared token dictionary for variable names:</p> <p>Header (4 bytes):</p> <p>| Bytes | Content | |-------|---------|| | 0-1 | Node count (uint16, max 65,535 nodes) | | 2-3 | Edge count (uint16, max 65,535 edges) |</p> <p>Node table (2 bytes per node):</p> <p>Each node is a token ID (uint16) referencing the token dictionary. Variable names like \"BMI\", \"Age\", \"Smoking\" are stored once in the token dictionary and referenced by ID.</p> <p>Edge list (5 bytes per edge):</p> <p>Each edge is packed into 36 bits (padded to 5 bytes):</p> <p>| Bits | Content | |------|---------|| | 0-15 | Source node token ID (uint16) | | 16-31 | Target node token ID (uint16) | | 32-33 | Source endpoint type (2 bits: <code>-</code>, <code>&gt;</code>, <code>o</code>) | | 34-35 | Target endpoint type (2 bits: <code>-</code>, <code>&gt;</code>, <code>o</code>) |</p> <p>Example: A graph with 20 nodes and 25 edges:</p> <ul> <li>Header: 4 bytes</li> <li>Nodes: 20 \u00d7 2 = 40 bytes</li> <li>Edges: 25 \u00d7 5 = 125 bytes</li> <li>Total: 169 bytes (vs ~2KB for JSON representation)</li> </ul> <p>The token dictionary is shared across all cache entries, so common variable names are stored only once regardless of how many graphs reference them.</p>"},{"location":"architecture/workflow_cache_design/#sqlite-schema","title":"SQLite Schema","text":"<pre><code>-- Shared token dictionary for compression (includes variable names)\nCREATE TABLE tokens (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    token TEXT UNIQUE NOT NULL,\n    frequency INTEGER DEFAULT 1\n);\n\n-- Cache entries with metadata\nCREATE TABLE cache_entries (\n    hash TEXT NOT NULL,\n    seq INTEGER NOT NULL DEFAULT 0,  -- Collision sequence (0 = first entry)\n    key_json TEXT NOT NULL,          -- Original key for collision detection\n    metadata BLOB,\n    created_at TEXT NOT NULL,\n    hit_count INTEGER DEFAULT 0,\n    last_accessed_at TEXT,\n    PRIMARY KEY (hash, seq)\n);\n\n-- Data blobs associated with cache entries\nCREATE TABLE cache_data (\n    hash TEXT NOT NULL,\n    seq INTEGER NOT NULL DEFAULT 0,\n    data_type TEXT NOT NULL,        -- 'graph', 'trace', etc.\n    data BLOB NOT NULL,\n    PRIMARY KEY (hash, seq, data_type),\n    FOREIGN KEY (hash, seq) REFERENCES cache_entries(hash, seq) ON DELETE CASCADE\n);\n\nCREATE INDEX idx_created_at ON cache_entries(created_at);\nCREATE INDEX idx_data_type ON cache_data(data_type);\n</code></pre>"},{"location":"architecture/workflow_cache_design/#hash-collision-handling","title":"Hash Collision Handling","text":"<p>With truncated SHA-256 hashes (16 hex chars = 64 bits), collisions are rare but possible. The <code>seq</code> column handles multiple entries with the same hash:</p> <pre><code>def put(self, key_data: dict, data: dict[str, bytes], metadata: dict) -&gt; None:\n    hash = self._compute_hash(key_data)\n    key_json = json.dumps(key_data, sort_keys=True)\n\n    # Find existing entry with same key, or next available seq\n    rows = self.conn.execute(\n        \"SELECT seq, key_json FROM cache_entries WHERE hash = ? ORDER BY seq\",\n        (hash,)\n    ).fetchall()\n\n    seq = 0\n    for row_seq, row_key_json in rows:\n        if row_key_json == key_json:\n            # Exact match - update existing entry\n            seq = row_seq\n            self._delete_entry(hash, seq)  # Remove old data blobs\n            break\n        seq = row_seq + 1  # Collision - use next sequence number\n\n    # Insert entry and data blobs\n    self.conn.execute(\n        \"INSERT INTO cache_entries (hash, seq, key_json, metadata, created_at) \"\n        \"VALUES (?, ?, ?, ?, ?)\",\n        (hash, seq, key_json, self._encode_metadata(metadata), datetime.utcnow())\n    )\n    for data_type, blob in data.items():\n        self.conn.execute(\n            \"INSERT INTO cache_data (hash, seq, data_type, data) VALUES (?, ?, ?, ?)\",\n            (hash, seq, data_type, blob)\n        )\n    self.conn.commit()\n\n\ndef get(self, key_data: dict, data_type: str) -&gt; bytes | None:\n    hash = self._compute_hash(key_data)\n    key_json = json.dumps(key_data, sort_keys=True)\n\n    # Find entry matching both hash and key\n    rows = self.conn.execute(\n        \"SELECT seq, key_json FROM cache_entries WHERE hash = ?\", (hash,)\n    ).fetchall()\n\n    for seq, row_key_json in rows:\n        if row_key_json == key_json:\n            # Found matching entry - fetch data blob\n            data_row = self.conn.execute(\n                \"SELECT data FROM cache_data \"\n                \"WHERE hash = ? AND seq = ? AND data_type = ?\",\n                (hash, seq, data_type)\n            ).fetchone()\n            return data_row[0] if data_row else None\n\n    return None  # No matching entry found\n</code></pre> <p>In practice, collisions are extremely rare with 64-bit hashes (birthday problem suggests ~4 billion entries before 50% collision probability), but the schema handles them correctly when they occur.</p>"},{"location":"architecture/workflow_cache_design/#entry-types","title":"Entry Types","text":"Type Data Blob Metadata Encoder <code>graph</code> Encoded SDG Provenance, edge confidences, scores <code>GraphEntryEncoder</code> <code>trace</code> Encoded DataFrame Algorithm, iterations, timing <code>TraceEntryEncoder</code> <code>llm</code> Tokenised JSON Provider, tokens, cost <code>LLMEntryEncoder</code> <p>New types can be added by implementing <code>EntryEncoder</code> - no schema changes.</p>"},{"location":"architecture/workflow_cache_design/#metadata-mutability-and-cache-queries","title":"Metadata Mutability and Cache Queries","text":""},{"location":"architecture/workflow_cache_design/#mutable-metadata","title":"Mutable Metadata","text":"<p>While matrix keys are immutable (they define entry identity), metadata can be updated after entry creation. This enables analysis workflows that enrich cached results:</p> <pre><code># Update metadata for an existing entry\ncache.update_metadata(\n    key={\"algorithm\": \"pc\", \"network\": \"asia\"},\n    metadata_updates={\"bic_score\": -1523.4, \"evaluated_at\": \"2026-02-04\"}\n)\n</code></pre> <p>Use cases: - Scoring workflows that evaluate cached graphs - Adding benchmark results to existing entries - Annotating entries with review status or notes</p>"},{"location":"architecture/workflow_cache_design/#cache-as-workflow-input","title":"Cache as Workflow Input","text":"<p>Workflow steps can use a Workflow Cache as an input source, selecting entries via predicates:</p> <pre><code>name: \"Score cached graphs\"\nsteps:\n  - action: score_graphs\n    cache_input:\n      source: \"results/discovery_cache.db\"\n      select:\n        # Matrix key predicates (indexed lookup)\n        algorithm: [pc, ges]\n        # Metadata predicates (may require scan)\n        bic_score: null            # Not yet scored\n    cache_output: \"results/discovery_cache.db\"  # Update same cache\n</code></pre>"},{"location":"architecture/workflow_cache_design/#entry-selection-predicates","title":"Entry Selection Predicates","text":"<p>Predicates can filter on matrix keys or metadata:</p> Predicate Type Index Example Matrix key equality \u2713 Hash lookup <code>algorithm: pc</code> Matrix key in list \u2713 Multiple lookups <code>algorithm: [pc, ges]</code> Metadata equality \u2717 Full scan <code>evaluated: true</code> Metadata comparison \u2717 Full scan <code>bic_score: {gt: -1000}</code> Metadata null check \u2717 Full scan <code>bic_score: null</code> <pre><code>def select_entries(\n    self,\n    matrix_predicates: dict[str, Any] | None = None,\n    metadata_predicates: dict[str, Any] | None = None,\n) -&gt; Iterator[CacheEntry]:\n    \"\"\"Select entries matching predicates.\n\n    Args:\n        matrix_predicates: Filter on matrix key values (indexed).\n        metadata_predicates: Filter on metadata fields (scan).\n\n    Yields:\n        Matching cache entries.\n    \"\"\"\n    if matrix_predicates:\n        # Build candidate set from matrix key lookups\n        candidates = self._lookup_by_matrix(matrix_predicates)\n    else:\n        # No matrix filter - full cache scan\n        candidates = self._scan_all_entries()\n\n    # Apply metadata filters\n    for entry in candidates:\n        if self._matches_metadata(entry, metadata_predicates):\n            yield entry\n</code></pre>"},{"location":"architecture/workflow_cache_design/#query-optimisation","title":"Query Optimisation","text":"<p>When matrix predicates are provided, selection is efficient:</p> <pre><code># Indexed: O(k) where k = number of matching matrix combinations\nselect(matrix_predicates={\"algorithm\": \"pc\", \"network\": \"asia\"})\n\n# Scan: O(n) where n = total entries\nselect(metadata_predicates={\"bic_score\": {\"gt\": -1000}})\n\n# Hybrid: O(k) lookups + filter\nselect(\n    matrix_predicates={\"algorithm\": [\"pc\", \"ges\"]},\n    metadata_predicates={\"bic_score\": null}\n)\n</code></pre> <p>Future optimisation: Add optional indexes on frequently-queried metadata fields if scan performance becomes a bottleneck.</p>"},{"location":"architecture/workflow_cache_design/#importexport","title":"Import/Export","text":""},{"location":"architecture/workflow_cache_design/#export-to-open-formats","title":"Export to Open Formats","text":"<pre><code>cqflow cache export workflow.db --output ./exported/\n</code></pre> <p>The export creates a hierarchical directory structure mirroring the matrix parameters, making results human-navigable:</p> <pre><code>exported/\n\u251c\u2500\u2500 pc/                          # algorithm (1st matrix variable)\n\u2502   \u251c\u2500\u2500 asia/                    # network (2nd matrix variable)\n\u2502   \u2502   \u251c\u2500\u2500 graph.graphml\n\u2502   \u2502   \u251c\u2500\u2500 trace.csv            # (if present)\n\u2502   \u2502   \u2514\u2500\u2500 metadata.json\n\u2502   \u2514\u2500\u2500 cancer/\n\u2502       \u251c\u2500\u2500 graph.graphml\n\u2502       \u2514\u2500\u2500 metadata.json\n\u251c\u2500\u2500 ges/\n\u2502   \u251c\u2500\u2500 asia/\n\u2502   \u2502   \u251c\u2500\u2500 graph.graphml\n\u2502   \u2502   \u2514\u2500\u2500 metadata.json\n\u2502   \u2514\u2500\u2500 cancer/\n\u2502       \u251c\u2500\u2500 graph.graphml\n\u2502       \u2514\u2500\u2500 metadata.json\n\u2514\u2500\u2500 manifest.json                # Index with full matrix key mappings\n</code></pre> <p>The directory hierarchy follows the order of matrix variables as defined in the workflow. The <code>manifest.json</code> provides a complete index mapping directory paths to cache hashes and full matrix key values:</p> <pre><code>{\n  \"matrix_variables\": [\"algorithm\", \"network\"],\n  \"entries\": [\n    {\n      \"path\": \"pc/asia\",\n      \"hash\": \"a3f7b2c1e9d4f8a2\",\n      \"key\": {\"algorithm\": \"pc\", \"network\": \"asia\"},\n      \"data_types\": [\"graph\", \"trace\"]\n    },\n    {\n      \"path\": \"pc/cancer\",\n      \"hash\": \"b4e8c3d2f1a5e9b3\",\n      \"key\": {\"algorithm\": \"pc\", \"network\": \"cancer\"},\n      \"data_types\": [\"graph\"]\n    }\n  ]\n}\n</code></pre>"},{"location":"architecture/workflow_cache_design/#import-from-open-formats","title":"Import from Open Formats","text":"<pre><code>cqflow cache import ./exported/ --into results.db\n</code></pre> <p>Useful for:</p> <ul> <li>Populating test fixtures</li> <li>Sharing results between researchers</li> <li>Restoring from Zenodo archives</li> </ul>"},{"location":"architecture/workflow_cache_design/#integration-with-workflows","title":"Integration with Workflows","text":""},{"location":"architecture/workflow_cache_design/#writing-results","title":"Writing Results","text":"<p>Actions write results via the workflow context:</p> <pre><code>def run(self, inputs, mode, context, logger):\n    # ... generate graph and trace ...\n\n    if context and context.cache:\n        context.cache.put(\n            key_data=context.matrix_values,  # Original key dict\n            data={                           # Multiple data blobs\n                \"graph\": graph,\n                \"trace\": trace,              # Optional\n            },\n            metadata={\n                \"provenance\": {...},\n                \"edge_confidences\": {...}\n            }\n        )\n</code></pre>"},{"location":"architecture/workflow_cache_design/#conservative-execution","title":"Conservative Execution","text":"<p>The workflow executor checks cache before running steps:</p> <pre><code>if cache.exists(context.matrix_values):\n    logger.info(\"Skipping: result already cached\")\n    return cache.get(context.matrix_values, \"graph\")\n</code></pre>"},{"location":"architecture/workflow_cache_design/#cross-package-dependencies","title":"Cross-Package Dependencies","text":"<p>Workflow Caches span three packages:</p> Package Version Responsibility causaliq-core v0.4.0 TokenCache, JsonEncoder, SDG encode/decode/GraphML causaliq-knowledge v0.5.0 GraphEntryEncoder, update generate_graph action causaliq-workflow v0.2.0 WorkflowCache class, CLI commands, workflow integration <p>Implementation order: core \u2192 knowledge \u2192 workflow (each depends on prior)</p>"},{"location":"architecture/workflow_cache_design/#future-considerations","title":"Future Considerations","text":"<p>Deferred to later releases:</p> <ul> <li>Metadata indexing for complex queries</li> <li>SDG edge attributes</li> <li>Cache key strategies beyond matrix values</li> <li>Cache comparison and diff tools</li> </ul>"},{"location":"architecture/workflow_executor_design/","title":"WorkflowExecutor Implementation Design","text":""},{"location":"architecture/workflow_executor_design/#overview","title":"Overview","text":"<p>The <code>WorkflowExecutor</code> class provides the foundation for parsing and executing GitHub Actions-style YAML workflows with matrix expansion support. This implementation focuses on the parsing and preparation phase, establishing the infrastructure for workflow execution.</p>"},{"location":"architecture/workflow_executor_design/#implementation-architecture","title":"Implementation Architecture","text":""},{"location":"architecture/workflow_executor_design/#class-structure","title":"Class Structure","text":"<pre><code>class WorkflowExecutor:\n    \"\"\"Parse and execute GitHub Actions-style workflows with matrix expansion.\"\"\"\n\n    def parse_workflow(self, workflow_path: Union[str, Path]) -&gt; Dict[str, Any]\n    def expand_matrix(self, matrix: Dict[str, List[Any]]) -&gt; List[Dict[str, Any]]\n    def construct_paths(self, job: Dict[str, Any], data_root: str, \n                       output_root: str, workflow_id: str) -&gt; Dict[str, str]\n</code></pre>"},{"location":"architecture/workflow_executor_design/#key-design-decisions","title":"Key Design Decisions","text":""},{"location":"architecture/workflow_executor_design/#1-integration-with-existing-schema-validation","title":"1. Integration with Existing Schema Validation","text":"<p>The <code>WorkflowExecutor</code> leverages the existing <code>causaliq_workflow.schema</code> module for workflow validation:</p> <pre><code>def parse_workflow(self, workflow_path):\n    workflow = load_workflow_file(workflow_path)  # Existing function\n    validate_workflow(workflow)                   # Existing function\n    return workflow\n</code></pre> <p>Rationale: Reuse proven validation logic, maintain single source of truth for workflow structure validation.</p>"},{"location":"architecture/workflow_executor_design/#2-matrix-expansion-strategy","title":"2. Matrix Expansion Strategy","text":"<p>Matrix expansion uses cartesian product generation:</p> <pre><code>def expand_matrix(self, matrix):\n    variables = list(matrix.keys())\n    value_lists = list(matrix.values())\n    combinations = list(itertools.product(*value_lists))\n\n    jobs = []\n    for combination in combinations:\n        job = dict(zip(variables, combination))\n        jobs.append(job)\n    return jobs\n</code></pre> <p>Rationale:  - Simple, predictable algorithm matching GitHub Actions behaviour - Easy to understand and debug - Supports arbitrary matrix dimensions - Deterministic ordering for reproducible results</p>"},{"location":"architecture/workflow_executor_design/#3-path-construction-pattern","title":"3. Path Construction Pattern","text":"<p>Paths follow the established pattern from the examples:</p> <pre><code># Input: {data_root}/{dataset}/input.csv\n# Output: {output_root}/{workflow_id}/{dataset}_{algorithm}/\n</code></pre> <p>Rationale: - Consistent with documentation examples - Organises outputs by workflow and experiment parameters - Supports hierarchical result organisation - Compatible with existing action framework expectations</p>"},{"location":"architecture/workflow_executor_design/#4-error-handling-strategy","title":"4. Error Handling Strategy","text":"<p>All methods use consistent error propagation:</p> <pre><code>try:\n    # Core logic\nexcept Exception as e:\n    raise WorkflowExecutionError(f\"Operation failed: {e}\") from e\n</code></pre> <p>Rationale: - Maintains error chain for debugging - Provides consistent error interface - Follows established patterns in the codebase</p>"},{"location":"architecture/workflow_executor_design/#implementation-phases","title":"Implementation Phases","text":""},{"location":"architecture/workflow_executor_design/#phase-1-parsing-foundation-complete","title":"Phase 1: Parsing Foundation (Complete \u2705)","text":"<p>Scope: Basic workflow parsing and matrix expansion - Parse and validate YAML workflow files - Expand matrix variables into job configurations - Construct file paths from matrix variables - Comprehensive error handling</p> <p>Test Coverage: 100% with edge case coverage - Unit tests with mocked dependencies - Functional tests with real YAML files - Exception handling verification</p>"},{"location":"architecture/workflow_executor_design/#phase-2-execution-engine-future","title":"Phase 2: Execution Engine (Future)","text":"<p>Scope: Step execution and orchestration - Execute workflow steps with action coordination - Environment variable management - Conditional execution (<code>if:</code> conditions) - Step output handling and dependencies</p>"},{"location":"architecture/workflow_executor_design/#phase-3-advanced-features-future","title":"Phase 3: Advanced Features (Future)","text":"<p>Scope: Enterprise workflow features - DASK task graph integration - Progress monitoring and status reporting - Resource management and limits - Workflow queue management</p>"},{"location":"architecture/workflow_executor_design/#integration-points","title":"Integration Points","text":""},{"location":"architecture/workflow_executor_design/#with-action-framework","title":"With Action Framework","text":"<pre><code># Future integration pattern\nfor step in workflow[\"steps\"]:\n    action = ActionRegistry.get_action(step[\"uses\"])\n    inputs = construct_action_inputs(step[\"with\"], job_context)\n    result = action.run(inputs)\n</code></pre>"},{"location":"architecture/workflow_executor_design/#with-schema-validation","title":"With Schema Validation","text":"<pre><code># Current integration\nworkflow = load_workflow_file(path)     # schema.py\nvalidate_workflow(workflow)             # schema.py\njobs = executor.expand_matrix(workflow[\"matrix\"])  # workflow.py\n</code></pre>"},{"location":"architecture/workflow_executor_design/#with-dask-future","title":"With DASK (Future)","text":"<pre><code># Planned integration\njobs = executor.expand_matrix(matrix)\ntask_graph = DaskBuilder.build_workflow_graph(jobs, workflow)\nresults = executor.execute_dask_graph(task_graph)\n</code></pre>"},{"location":"architecture/workflow_executor_design/#testing-strategy","title":"Testing Strategy","text":""},{"location":"architecture/workflow_executor_design/#unit-tests-isolated-logic","title":"Unit Tests (Isolated Logic)","text":"<ul> <li>Matrix expansion algorithm verification</li> <li>Path construction with various inputs</li> <li>Exception handling edge cases</li> <li>Mocked dependencies for isolation</li> </ul>"},{"location":"architecture/workflow_executor_design/#functional-tests-real-operations","title":"Functional Tests (Real Operations)","text":"<ul> <li>YAML file parsing with real workflow files</li> <li>Filesystem operations for temporary test files</li> <li>Integration with schema validation</li> <li>End-to-end workflow parsing scenarios</li> </ul>"},{"location":"architecture/workflow_executor_design/#edge-case-coverage","title":"Edge Case Coverage","text":"<ul> <li>Exception handling in matrix expansion (<code>itertools.product</code> failures)</li> <li>Empty matrices and default value handling</li> <li>Invalid workflow file scenarios</li> <li>Missing matrix variable handling</li> </ul>"},{"location":"architecture/workflow_executor_design/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"architecture/workflow_executor_design/#matrix-expansion-scaling","title":"Matrix Expansion Scaling","text":"<ul> <li>Time Complexity: O(n\u2081 \u00d7 n\u2082 \u00d7 ... \u00d7 n\u2096) where n\u1d62 is the size of each matrix dimension</li> <li>Space Complexity: O(jobs) linear in the number of generated job combinations</li> <li>Practical Limits: Reasonable for typical causal discovery experiments (&lt; 1000 jobs)</li> </ul>"},{"location":"architecture/workflow_executor_design/#memory-usage","title":"Memory Usage","text":"<ul> <li>Workflow parsing: Linear in YAML file size</li> <li>Matrix expansion: Linear in number of generated jobs</li> <li>Path construction: Constant per job</li> </ul>"},{"location":"architecture/workflow_executor_design/#future-optimisations","title":"Future Optimisations","text":"<ul> <li>Lazy matrix expansion for large matrices</li> <li>Streaming job processing for memory efficiency</li> <li>Job batching for DASK execution</li> </ul>"},{"location":"architecture/workflow_executor_design/#alignment-with-causaliq-standards","title":"Alignment with CausalIQ Standards","text":""},{"location":"architecture/workflow_executor_design/#development-guidelines-compliance","title":"Development Guidelines Compliance","text":"<ul> <li>\u2705 Small incremental changes: 99-line focused implementation</li> <li>\u2705 100% test coverage: Comprehensive unit and functional tests  </li> <li>\u2705 CI compliance: All formatting, linting, and type checking standards met</li> <li>\u2705 British English: Documentation and code comments</li> <li>\u2705 Type safety: Complete type annotations with mypy validation</li> </ul>"},{"location":"architecture/workflow_executor_design/#architecture-principles","title":"Architecture Principles","text":"<ul> <li>\u2705 GitHub Actions foundation: Consistent with CI/CD workflow patterns</li> <li>\u2705 Action-based components: Integration with existing action framework</li> <li>\u2705 Schema-first design: Leverage existing validation infrastructure</li> <li>\u2705 Incremental functionality: Foundation for future execution features</li> </ul> <p>This design provides a solid foundation for workflow execution while maintaining the incremental development approach and high quality standards established in the CausalIQ Workflow project.</p>"}]}