{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Documentation Structure","text":"<p>This directory contains focused documentation for causaliq-workflow development.</p>"},{"location":"#essential-documents","title":"Essential Documents","text":""},{"location":"#start-here","title":"\ud83d\udccb Start Here","text":"<ul> <li>Development Roadmap - Complete feature list, progress tracking, and three-month implementation plan</li> <li>Example Workflows - Series-based workflow examples and YAML configuration patterns</li> </ul>"},{"location":"#design","title":"\ud83d\udd27 Design","text":"<ul> <li>Technical Architecture - System design, Series concept, component architecture, and workflow-specific development patterns</li> </ul>"},{"location":"#development-support","title":"\ud83d\udcd6 Development Support","text":"<ul> <li>LLM Development Guide - Comprehensive development standards and LLM communication patterns for the CausalIQ ecosystem</li> </ul>"},{"location":"#quick-navigation","title":"Quick Navigation","text":""},{"location":"#for-design-work","title":"For Design Work","text":"<p>Start with: Technical Architecture \u2192 Development Roadmap</p>"},{"location":"#for-understanding-requirements","title":"For Understanding Requirements","text":"<p>Start with: Development Roadmap \u2192 Example Workflows</p>"},{"location":"#for-llm-collaboration","title":"For LLM Collaboration","text":"<p>Use: LLM Development Guide (comprehensive, ecosystem-wide standards)</p>"},{"location":"#document-purpose-summary","title":"Document Purpose Summary","text":"Document Purpose When to Use <code>roadmap.md</code> Feature specifications and progress tracking Feature development planning <code>example_workflows.md</code> YAML workflow examples Understanding series concept <code>technical_architecture.md</code> Package architecture and design patterns Building components <p>This streamlined structure focuses on actionable implementation guidance while maintaining single sources of truth.</p>"},{"location":"api/","title":"API Reference","text":"<p>The CausalIQ Workflow framework provides a comprehensive API for building, validating, and executing data processing workflows. The API is organized into several key modules:</p>"},{"location":"api/#core-components","title":"Core Components","text":""},{"location":"api/#action-framework","title":"Action Framework","text":"<p>Base classes and interfaces for creating reusable workflow actions that follow GitHub Actions patterns.</p> <ul> <li>Action - Abstract base class for all workflow actions</li> <li>ActionInput/ActionOutput - Type-safe input/output specifications  </li> <li>ActionExecutionError/ActionValidationError - Exception handling</li> </ul>"},{"location":"api/#action-registry","title":"Action Registry","text":"<p>Centralized discovery and execution system for workflow actions with plugin architecture support.</p> <ul> <li>ActionRegistry - Dynamic action discovery and management</li> <li>WorkflowContext - Complete workflow context for actions</li> <li>ActionRegistryError - Registry-specific exceptions</li> </ul>"},{"location":"api/#workflow-engine","title":"Workflow Engine","text":"<p>Powerful workflow parsing, validation, and execution engine with matrix expansion and templating.</p> <ul> <li>WorkflowExecutor - Main workflow processing engine</li> <li>WorkflowExecutionError - Workflow execution exceptions</li> <li>Template system - Variable substitution and validation</li> </ul>"},{"location":"api/#schema-validation","title":"Schema Validation","text":"<p>Robust workflow validation against JSON schemas with detailed error reporting.</p> <ul> <li>validate_workflow - Schema-based workflow validation</li> <li>load_schema/load_workflow_file - File loading utilities</li> <li>WorkflowValidationError - Validation-specific exceptions</li> </ul>"},{"location":"api/#status-system","title":"Status System","text":"<p>Comprehensive task execution status enumeration for workflow logging and monitoring.</p> <ul> <li>TaskStatus - Standardized status reporting for all task execution outcomes</li> <li>Status properties - Categorization helpers (success, error, execution, dry-run)</li> <li>Status definitions - Complete coverage of execution, comparison, and error statuses</li> </ul>"},{"location":"api/#logging-system","title":"Logging System","text":"<p>Centralized logging infrastructure with multiple output destinations for workflow execution monitoring.</p> <ul> <li>WorkflowLogger - Multi-destination logging with file/terminal output support</li> <li>LogLevel - Verbosity control (NONE, SUMMARY, ALL)</li> <li>Output configuration - Flexible logging destination management</li> </ul>"},{"location":"api/#cli-interface","title":"CLI Interface","text":"<p>Command-line interface for workflow execution and management.</p> <ul> <li>Command-line tools - Direct workflow execution</li> <li>Integration support - CI/CD pipeline integration</li> </ul>"},{"location":"api/#quick-start","title":"Quick Start","text":"<pre><code>from causaliq_workflow import WorkflowExecutor\n\n# Create executor and run workflow\nexecutor = WorkflowExecutor()\nworkflow = executor.parse_workflow(\"my_workflow.yml\")\nresults = executor.execute_workflow(workflow, mode=\"run\")\n</code></pre>"},{"location":"api/#next-steps","title":"Next Steps","text":"<ul> <li>Usage Examples - Comprehensive code examples and patterns</li> <li>Action Framework - Learn how to create custom actions</li> <li>CLI Interface - Command-line usage and CI/CD integration</li> </ul> <p>For detailed examples and usage patterns, see the Usage Examples page.</p>"},{"location":"example_workflows/","title":"CausalIQ Workflow - Example Workflows","text":""},{"location":"example_workflows/#design-philosophy-inspired-by-ci-workflows","title":"Design Philosophy: Inspired by CI-Workflows","text":"<p>The CausalIQ Workflows are inspired by the concepts within  Continuous Integration (CI) workflows. They provide a subset of the features  provided by CI workflows and much of the functionality is simplified to ease the job of specifying CausalIQ workflows.</p> <p>Key concepts in CausalIQ workflows are:</p> <ul> <li>Workflows are a series of one or more sequential steps.</li> <li>Steps can be either an action or they can run shell commands.</li> <li>actions execute typical causal inference and evaluation activities like structure    learning, graph evaluation, causal inference etc., and actions:<ul> <li>are implemented in other CausalIQ packages such as causaliq-discovery,        causaliq-analysis etc. as specified in the uses: keyword of the step;</li> <li>take parameters values for the action - e.g., the algorithm to use - are specified using the with: keyword;</li> <li>actions can be implemented intelligently to perform their work efficiently and in parallel.</li> </ul> </li> <li>a matrix concept allows the set of steps to be repeated over multiple   combinations of values, for example over a set of networks, sample sizes and   algorithms. This is particularly valuable for large scale comparative experiments.</li> <li> <p>workflows may be run from the command line interface (CLI) using commands like:</p> <p><code>shell cwork example_workflow.yaml --network=child,property</code>   - CLI arguments can be used to override workflow and matrix parameters so     that the same workflow can be easily reused;   - the run: command can be used to run a workflow so that \"workflows of     workflows\" are supported.</p> </li> </ul> <p>The CausalIQ Workflow CLI has a --mode argument which controls its overall behaviour as follows:</p> <ul> <li>--mode=run: this actually executes the workflow. Note that, in this case  the actions in the workflow are performed conservatively - if the outputs of  actions are already present on the file system the action is not repeated. This  therefore faciltates restarting the workflow if it has previously been interrupted.</li> <li>--mode=dry-run: this is the default behaviour and reports what would be done if --mode=run were used, but does not perform the actual actions. In doing so,   it has the valuable side-effect of checking that the workflow definitions are   all valid.</li> <li>--mode=compare: all actions in the workflow will be re-run regardless of whether the output is present on the filesystem or not. If the output is present on the filesystem, the newly-generated output will replace it, and any differences with the previous version reported. This mode is therefore a very powerful form of functional testing.  </li> </ul>"},{"location":"example_workflows/#example-workflows-in-causal-discovery","title":"Example Workflows in Causal Discovery","text":""},{"location":"example_workflows/#simple-one-off-structure-learning-examples","title":"Simple One-Off Structure Learning Examples","text":"<p>The first simple example runs the Tabu-Stable structure learning algorithm on 1000 synthetic rows from the Asia network. Input and output files are generally specified in terms of their location in CausalIQ Workflows, and the filenames are fixed according to what kind of entity they are: a graph, metadata or trace for example.</p> <p>This will create a graph.xml and metadata.json file in the specified output folder. The debug switch also means that a trace.csv will be produced.</p> <pre><code># tabu_asia.yaml\ndescription: \"Tabu-stable learning Asia from 1K data with full trace\"\n\nsteps:\n  - name: \"Learn Structure\"\n    uses: \"causaliq-discovery\"\n    with:\n      algorithm: \"tabu-stable\"\n      sample_size: \"1k\"\n      dataset: \"/data/asia\"\n      output: \"/results/tabu-asia\"\n      debug: True\n</code></pre> <p>The same result could be achieved using the causaliq-discovery CLI command directly, but using this simple workflow avoids having to specify the dataset, algorithm etc. on the command line by using the following command:</p> <pre><code>cwork tabu_asia.yaml  # cwork is a synonym for causaliq-workflow\n</code></pre> <p>This \"shorthand\" capability becomes especially useful when plotting charts which  typically require a lot of parameter values to specify axis titles, chart type and colours etc., which become unmanageable to specify on the command line.</p> <p>Another simple workflow might be to download resources from Zenodo. This would download all the resources associated with my PhD thesis and unzip in the specified output folder.</p> <pre><code># download_paper.yaml\ndescription: \"Download assets for PhD Thesis\"\n\nsteps:\n  - name: \"Download paper\"\n    uses: \"causaliq-papers\"\n    with:\n      operation: \"download\"\n      doi: \"10.5072/zenodo.338579\"\n      output: \"/papers/2025KitsonThesis\"\n</code></pre>"},{"location":"example_workflows/#parameterised-structure-learning-example","title":"Parameterised Structure Learning Example","text":"<p>We can make the simple learning example more general by adding workflow-level variables to parameterise the workflow. In this example, the sample_size parameter has a default value of 1000 if not specified on the command line, whereas the None value for network indicates that this must be specified on the command line.  Note how the network and sample_size parameters are used in the dataset and output action parameters so that the correct input file and output folder are used.</p> <pre><code># tabu_learning.yaml\ndescription: \"Parameterised Tabu-stable learning\"\nsample_size: \"1K\"\nnetwork: None\n\nsteps:\n  - name: \"Learn Structure\"\n    uses: \"causaliq-discovery\"\n    with:\n      algorithm: \"tabu-stable\"\n      max_time: 10\n      sample_size: {{sample_size}}\n      dataset: \"/data/{{network}}\"\n      output: \"/results/{{network}}/{{sample_size}}\"\n</code></pre> <p>We can now run Tabu-Stable structure learning for any network and sample size using commands like:</p> <pre><code>cwork tabu_learning.yaml --network=child --sample_size=500\n</code></pre>"},{"location":"example_workflows/#multiple-step-examples","title":"Multiple Step Examples","text":"<p>We could enhance this by adding a step before structure learning where an LLM provides an initial graph. In this example, the dataset is provided so that the LLM has access to the values, and there is also some additional domain context provided in a json file. This could generate a graph.xml in the output folder and probably also some metadata and/or history of the prompts and responses  made to the LLM. Note that, this LLM generated graph would then be available to be re-used in other experiments, and the prompts/response trail provides the means to exactly replicate this result.</p> <pre><code># tabu_llm_learning.yaml\ndescription: \"Tabu-stable learning Asia with LLM initialisation\"\nsample_size: \"1K\"\nnetwork: None\n\nsteps:\n  - name: \"LLM Graph Initialisation\"\n    uses: \"causaliq-knowledge\"\n    with:\n      operation: \"propose-graph\"\n      dataset: \"/data/{{network}}\"\n      context: \"/knowledge/context/{{network}}\"\n      output: \"/knowledge/llm_graphs/{{network}}\"\n\n  - name: \"Learn Structure\"\n    uses: \"causaliq-discovery\"\n    with:\n      initial_graph: \"/knowledge/llm_graphs/asia\"\n      # etc ....\n</code></pre> <p>Alternatively, or additionally, we can add some steps after structure learning to compute some graph metrics and visualise the way that the graph was learnt.</p> <p>The \"Evaluate graph\" step requires the bic-score which needs the dataset to be specified, and F1 needs the true graph to be supplied - the relevant causaliq-analysis action is responsible for checking that these kind of dependencies are satisified. The results folder is specified where the learnt graph and learning metadata is, and so these results are appended to the metadata.</p> <pre><code># learn_and_evaluate.yaml\n\n# workflow variables ... \n\nsteps:\n\n  # step(s) to learn graph ...\n\n  - name: \"Evaluate graph\"\n    uses: \"causaliq-analysis\"\n    with:\n      operation: \"evaluate-graph\"\n      dataset: \"/data/{{network}}\"\n      true_graph: \"/knowledge/true_graph/{{network}}\"\n      metrics: [\"bic-score\", \"f1\", \"scaled-shd\"]\n      basis: [\"cpdag\"]\n      graph: \"/results/{{network}}/{{sample_size}}\"\n</code></pre>"},{"location":"example_workflows/#matrix-strategy-workflow","title":"Matrix Strategy Workflow","text":"<p>In many cases we will wish to run comparative experiments with a different result for each combinations of algorithm, network, and sample size and randomisation. Note how we structure the output folder path so that we ensure the result for each individual experiment is placed in its own folder. We also use an \"id\" workflow variablesto keep these results separate from those of other workflows.</p> <p>Internally, the action - in this case causal-discovery - may be implemented intelligently to maximise efficiency. For example, in this case, it may read the maximum number of rows from the filesystem dataset just once, and then adjust the effective sample size and randomisation internally for each individual experiment.</p> <pre><code># matrix_experiment.yaml\nid: \"stability001\"\ndescription: \"Algorithm stability comparison\"\nrandomise: [\"variable_order\", \"variable_name\"]\n\nmatrix:\n  network: [\"asia\", \"cancer\"]\n  algorithm: [\"pc\", \"ges\"]\n  sample_size: [\"100\", \"1K\"]\n  seed: [1, 25]\n\nsteps:\n  - name: \"Structure Learning\"\n    uses: \"causaliq-discovery\"\n    with:\n      algorithm: \"{{algorithm}}\"\n      sample_size: \"{{sample_size}}\"\n      dataset: \"/data/{{network}}\"\n      randomise: {{randomise}}\n      seed: {{seed}}\n      output: \"/results/{{id}}/{{algorithm}}/{{network}}/{{sample_size}}/{{seed}}\"\n</code></pre>"},{"location":"example_workflows/#workflow-of-workflows","title":"Workflow of workflows","text":"<p>The ability of one workflow to call other parameterised workflows facilitates the creation and testing of complex CausalIQ workflows, for instance to run all the experiments, analysis and asset generation for a research paper or thesis. CausalIQ Papers makes use of this to provide reproducibility of CausalIQ published papers.</p> <p>The example below shows a simple top level workflow which might reproduce the experiments and analysis behind a thesis. The lower level workflows such as create_chapter_4.yaml might then call other workflows to perform the structure learning, result analysis and asset generation for Chapter 4.</p> <pre><code># reproduce_thesis.yaml\n\nid: \"Kitson2025thesis\"\n\nmatrix:\n  chapter: [4, 5, 6]\n\nsteps:\n  name: \"Create chapter {{chapter}}\"\n  run: \"cwork create_chapter_{{chapter}}.yaml --id={{id}}\n</code></pre>"},{"location":"example_workflows/#parallel-jobs","title":"Parallel Jobs","text":"<p>CI workflows provide a jobs: keyword which allows multiple sequences of steps to run in parallel. We are not planning to implement this at the moment, instead relying on actions to provde parallelism using DAK tasks. This keeps CausalIQ Workflow functionality simple and reflects the fact that structure learning steps involving many individual structure learning experiments can keep even very powerful machines busy.</p>"},{"location":"example_workflows/#template-variables","title":"Template Variables","text":""},{"location":"example_workflows/#flexible-path-templating-pattern","title":"Flexible Path Templating Pattern","text":"<p>Our implementation supports flexible path templating using matrix variables:</p> <pre><code># Template variables can be used in action parameters:\n# {{id}} - workflow identifier\n# {{network}} - current matrix value for network\n# {{algorithm}} - current matrix value for algorithm  \n# {{sample_size}} - current matrix value for sample size\n\n# Example expansion for graphs using matrix above:\n# Job 1: dataset=\"/experiments/data/asia.csv\", result=\"/experiments/results/algo-comparison-001/pc/asia/100\"\n# Job 2: dataset=\"/experiments/data/asia.csv\", result=\"/experiments/results/algo-comparison-001/pc/asia/1K\"\n# Job 3: dataset=\"/experiments/data/asia.csv\", result=\"/experiments/results/algo-comparison-001/ges/graph_asia_0.01.xml\"\n# ... (8 total combinations)\n</code></pre>"},{"location":"example_workflows/#template-variable-validation","title":"Template Variable Validation","text":"<p>The workflow executor automatically validates that all template variables (<code>{{variable}}</code>) used in action parameters are available from either: - Workflow properties: <code>id</code>, <code>description</code>  - Matrix variables: Variables defined in the <code>matrix</code> section</p> <p>Valid Template Usage:</p> <pre><code>id: \"my-experiment-001\"\nmatrix:\n  dataset: [\"asia\", \"cancer\"]\n  algorithm: [\"pc\", \"ges\"]\n\nsteps:\n  - uses: \"dummy-structure-learner\"\n    with:\n      # These are all valid - variables exist in workflow context\n      output: \"/results/{{id}}/{{dataset}}_{{algorithm}}\"\n      description: \"Running {{algorithm}} on {{dataset}}\"\n</code></pre> <p>Invalid Template Usage (Validation Error):</p> <pre><code>id: \"my-experiment-001\"\nmatrix:\n  dataset: [\"asia\", \"cancer\"]\n\nsteps:\n  - uses: \"dummy-structure-learner\"\n    with:\n      # This will cause a WorkflowExecutionError\n      output: \"/results/{{unknown_variable}}/{{missing_param}}\"\n</code></pre> <p>Error Message Example:</p> <pre><code>WorkflowExecutionError: Unknown template variables: unknown_variable, missing_param\nAvailable context: id, description, dataset\n</code></pre>"},{"location":"example_workflows/#implemented-features","title":"Implemented Features","text":""},{"location":"example_workflows/#action-framework","title":"\u2705 Action Framework","text":"<ul> <li>Action Registration: Actions defined as Python classes</li> <li>Type Safety: Input/output specifications with type hints</li> <li>Error Handling: Comprehensive ActionExecutionError and ActionValidationError</li> <li>GraphML Output: Standardized format for causal graphs</li> </ul>"},{"location":"example_workflows/#workflow-execution-engine","title":"\u2705 Workflow Execution Engine","text":"<ul> <li>YAML Parsing: Parse and validate GitHub Actions-style workflow files</li> <li>Matrix Expansion: Convert matrix variables into individual job configurations</li> <li>Flexible Path Templating: User-controlled path generation with {{}} template variables</li> <li>Template Variable Validation: Automatic validation of {{variable}} patterns against available context</li> <li>Error Propagation: Comprehensive error handling with WorkflowExecutionError</li> </ul>"},{"location":"example_workflows/#schema-validation","title":"\u2705 Schema Validation","text":"<ul> <li>GitHub Actions Syntax: Familiar workflow patterns</li> <li>Matrix Variables: Full support for parameterized experiments  </li> <li>Flexible Action Parameters: Template variables in action <code>with:</code> blocks</li> <li>Action Parameters: with blocks for action configuration</li> </ul>"},{"location":"example_workflows/#test-coverage","title":"\u2705 Test Coverage","text":"<ul> <li>Functional Tests: Real filesystem operations with tracked test data</li> <li>Unit Tests: Mocked dependencies for action logic testing</li> <li>Schema Tests: Comprehensive validation of all workflow features</li> <li>100% Coverage: Complete test coverage including edge cases</li> </ul>"},{"location":"example_workflows/#example-action-implementation","title":"Example Action Implementation","text":"<pre><code>class DummyStructureLearnerAction(Action):\n    \"\"\"Reference action implementation.\"\"\"\n\n    name = \"dummy-structure-learner\"\n    version = \"1.0.0\"\n\n    inputs = {\n        \"data_path\": ActionInput(\n            name=\"data_path\",\n            description=\"Path to input CSV dataset (auto-constructed)\",\n            required=True,\n            type_hint=\"str\",\n        ),\n        \"output_dir\": ActionInput(\n            name=\"output_dir\", \n            description=\"Directory for output files (auto-constructed)\",\n            required=True,\n            type_hint=\"str\",\n        ),\n        \"dataset\": ActionInput(\n            name=\"dataset\",\n            description=\"Dataset name from matrix\",\n            required=True,\n            type_hint=\"str\",\n        ),\n        \"algorithm\": ActionInput(\n            name=\"algorithm\",\n            description=\"Algorithm name from matrix\",\n            required=True,\n            type_hint=\"str\",\n        ),\n    }\n\n    def run(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Create GraphML output file.\"\"\"\n        # Implementation creates valid GraphML file\n        # Returns graph_path, node_count, edge_count\n</code></pre>"},{"location":"example_workflows/#current-implementation-workflowexecutor","title":"Current Implementation: WorkflowExecutor","text":""},{"location":"example_workflows/#workflowexecutor-implementation-phase-1-complete","title":"WorkflowExecutor Implementation (Phase 1 Complete)","text":"<pre><code>from causaliq_workflow import WorkflowExecutor\n\n# Parse workflow and expand matrix\nexecutor = WorkflowExecutor()\nworkflow = executor.parse_workflow(\"experiment.yml\")\n\n# Matrix expansion example\nmatrix = {\"algorithm\": [\"pc\", \"ges\"], \"dataset\": [\"asia\", \"cancer\"], \"alpha\": [0.01, 0.05]}\njobs = executor.expand_matrix(matrix)  # Returns 8 job configurations\n\n# Example workflow with flexible paths\nworkflow_example = {\n    \"id\": \"experiment-001\",\n    \"description\": \"Flexible causal discovery experiment\",\n    \"matrix\": matrix,\n    \"steps\": [{\n        \"name\": \"Structure Learning\",\n        \"uses\": \"dummy-structure-learner\", \n        \"with\": {\n            \"dataset\": \"/experiments/data/{{dataset}}.csv\",\n            \"result\": \"/experiments/results/{{id}}/{{algorithm}}/graph_{{dataset}}_{{alpha}}.xml\",\n            \"alpha\": \"{{alpha}}\"\n        }\n    }]\n}\n\n# Each job contains expanded matrix variables for template substitution\nfor job in jobs:\n    print(f\"Job: {job}\")\n    # Example: {'algorithm': 'pc', 'dataset': 'asia', 'alpha': 0.01}\n</code></pre> <p>Implemented Features: - \u2705 Parse and validate YAML workflow files - \u2705 Expand matrix variables into job configurations - \u2705 Support flexible path templating with {{}} variables - \u2705 Comprehensive error handling and 100% test coverage</p> <p>Next Phase: Action execution engine with template variable substitution</p> <p>ges_series:     algorithm: \"ges\"      package: \"causaliq-discovery\"     datasets: [\"alarm\", \"asia\"]  # Reference same datasets     sample_sizes: [100, 500, 1000]     randomizations: 10     hyperparameters:       score_type: [\"bic\", \"aic\"]</p>"},{"location":"example_workflows/#resource-configuration","title":"Resource configuration","text":"<p>resources:   max_parallel_jobs: 8   memory_limit_per_job: \"4GB\"   runtime_limit_per_job: \"30m\"   total_runtime_limit: \"4h\"</p>"},{"location":"example_workflows/#analysis-configuration","title":"Analysis configuration","text":"<p>analysis:   compare_series: [\"pc_series\", \"ges_series\"]   metrics: [\"shd\", \"precision\", \"recall\", \"f1\"]   statistical_significance: 0.05   output_format: [\"csv\", \"json\"]</p>"},{"location":"example_workflows/#logging-and-monitoring","title":"Logging and monitoring","text":"<p>monitoring:   progress_updates: \"1m\"   log_level: \"INFO\"   save_intermediate: true</p> <pre><code>\n## Use Case 2: External Package Integration\n\n### Scenario\nUse R bnlearn package alongside Python algorithms for comprehensive comparison.\n\n### Multi-Package Workflow\n```yaml\n# multi_package_comparison.yaml\nmetadata:\n  name: \"python_r_algorithm_comparison\"\n  description: \"Compare Python and R causal discovery implementations\"\n\nseries:\n  python_pc:\n    algorithm: \"pc\"\n    package: \"causaliq-discovery\"\n    datasets: [\"alarm\"]\n    sample_sizes: [500, 1000]\n    randomizations: 5\n    hyperparameters:\n      alpha: 0.05\n\n  r_bnlearn_pc:\n    algorithm: \"pc.stable\"\n    package: \"r_bnlearn\"\n    datasets: [\"alarm\"]  # Same datasets for comparison\n    sample_sizes: [500, 1000] \n    randomizations: 5\n    hyperparameters:\n      alpha: 0.05\n      test: \"cor\"\n\n# External package requirements\nexternal_packages:\n  r_bnlearn:\n    check_installation: true\n    required_version: \"&gt;=4.7\"\n    install_if_missing: false  # Fail if not available\n\nresources:\n  max_parallel_jobs: 4  # Fewer for external packages\n  memory_limit_per_job: \"8GB\"\n\nvalidation:\n  dry_run: true  # Preview before execution\n  check_dependencies: true\n</code></pre>"},{"location":"example_workflows/#use-case-3-configuration-inheritance","title":"Use Case 3: Configuration Inheritance","text":""},{"location":"example_workflows/#scenario","title":"Scenario","text":"<p>Create specific experiments based on common base configuration.</p>"},{"location":"example_workflows/#base-configuration","title":"Base Configuration","text":"<pre><code># base_causal_experiment.yaml\nmetadata:\n  name: \"base_causal_discovery_template\"\n  description: \"Base template for causal discovery experiments\"\n\ndefaults:\n  datasets:\n    - name: \"alarm\" \n      zenodo_id: \"alarm_networks_v1\"\n    - name: \"asia\"\n      zenodo_id: \"asia_networks_v1\"\n  sample_sizes: [100, 500, 1000]\n  randomizations: 10\n\nresources:\n  max_parallel_jobs: 6\n  memory_limit_per_job: \"4GB\"\n  runtime_limit_per_job: \"1h\"\n\nmonitoring:\n  log_level: \"INFO\"\n  progress_updates: \"30s\"\n</code></pre>"},{"location":"example_workflows/#inherited-specific-experiment","title":"Inherited Specific Experiment","text":"<pre><code># pc_alpha_study.yaml\ninherits: \"base_causal_experiment.yaml\"\n\nmetadata:\n  name: \"pc_alpha_sensitivity_study\"\n  description: \"Study effect of alpha parameter on PC algorithm\"\n\noverrides:\n  series:\n    pc_alpha_study:\n      algorithm: \"pc\"\n      package: \"causaliq-discovery\" \n      hyperparameters:\n        alpha: [0.001, 0.01, 0.05, 0.1, 0.2]\n\n  analysis:\n    metrics: [\"shd\", \"precision\", \"recall\"]\n    focus_parameter: \"alpha\"\n    statistical_tests: true\n</code></pre>"},{"location":"example_workflows/#use-case-4-llm-integration-for-model-averaging","title":"Use Case 4: LLM Integration for Model Averaging","text":""},{"location":"example_workflows/#scenario_1","title":"Scenario","text":"<p>Use LLM to guide intelligent model averaging across multiple algorithm results.</p>"},{"location":"example_workflows/#llm-enhanced-workflow","title":"LLM-Enhanced Workflow","text":"<pre><code># llm_model_averaging.yaml\nmetadata:\n  name: \"llm_guided_model_averaging\"\n  description: \"Use LLM for intelligent model averaging\"\n\n# First run multiple algorithms\nseries:\n  pc_results:\n    algorithm: \"pc\"\n    package: \"causaliq-discovery\"\n    datasets: [\"healthcare_study\"]\n    sample_sizes: [1000]\n    randomizations: 5\n    hyperparameters:\n      alpha: 0.05\n\n  ges_results:\n    algorithm: \"ges\"\n    package: \"causaliq-discovery\"\n    datasets: [\"healthcare_study\"]\n    sample_sizes: [1000]\n    randomizations: 5\n    hyperparameters:\n      score_type: \"bic\"\n\n# LLM integration for analysis\nllm_analysis:\n  stage: \"post_discovery\"\n  package: \"causaliq-knowledge\"\n  tasks:\n    - task: \"analyze_algorithm_outputs\"\n      inputs: [\"pc_results\", \"ges_results\"]\n      domain_knowledge: \"healthcare/cardiology\"\n\n    - task: \"suggest_averaging_weights\"\n      based_on: \"algorithm_reliability_analysis\"\n\n    - task: \"validate_combined_graph\"\n      domain_expertise: true\n\n# Model averaging using LLM suggestions\nmodel_averaging:\n  package: \"causaliq-analysis\"\n  method: \"llm_weighted_average\"\n  inputs: [\"pc_results\", \"ges_results\", \"llm_suggestions\"]\n</code></pre>"},{"location":"example_workflows/#use-case-5-dataset-download-and-randomization","title":"Use Case 5: Dataset Download and Randomization","text":""},{"location":"example_workflows/#scenario_2","title":"Scenario","text":"<p>Automated dataset download from Zenodo with systematic randomization for robustness testing.</p>"},{"location":"example_workflows/#dataset-management-workflow","title":"Dataset Management Workflow","text":"<pre><code># dataset_robustness_study.yaml\nmetadata:\n  name: \"dataset_robustness_analysis\"\n  description: \"Study algorithm robustness to data variations\"\n\n# Dataset configuration with automatic download\ndatasets:\n  primary_dataset:\n    name: \"benchmark_networks\"\n    source: \"zenodo\"\n    zenodo_id: \"benchmark_causal_v2\"\n    cache_locally: true\n\n# Randomization strategies  \nrandomization:\n  strategies:\n    - type: \"subsample\"\n      fractions: [0.7, 0.8, 0.9, 1.0]\n\n    - type: \"variable_reorder\"\n      random_seeds: [42, 123, 456]\n\n    - type: \"noise_injection\"\n      noise_levels: [0.0, 0.05, 0.1]\n      noise_type: \"gaussian\"\n\n# Series using randomized datasets\nseries:\n  robustness_test:\n    algorithm: \"pc\"\n    package: \"causaliq-discovery\"\n    apply_all_randomizations: true\n    hyperparameters:\n      alpha: 0.05\n\n# Analysis focuses on robustness metrics\nanalysis:\n  robustness_metrics:\n    - \"stability_across_subsamples\"\n    - \"invariance_to_ordering\"\n    - \"noise_resistance\"\n  generate_robustness_report: true\n</code></pre>"},{"location":"example_workflows/#cli-usage-examples","title":"CLI Usage Examples","text":""},{"location":"example_workflows/#basic-execution","title":"Basic Execution","text":"<pre><code># Execute a series-based workflow\ncausaliq-workflow run pc_ges_comparison.yaml\n\n# Dry-run to preview execution plan\ncausaliq-workflow validate pc_ges_comparison.yaml --dry-run\n\n# Monitor running workflow\ncausaliq-workflow status workflow-abc123\n\n# Pause running workflow\ncausaliq-workflow pause workflow-abc123\n\n# Resume paused workflow  \ncausaliq-workflow resume workflow-abc123\n</code></pre>"},{"location":"example_workflows/#advanced-options","title":"Advanced Options","text":"<pre><code># Override resource limits\ncausaliq-workflow run experiment.yaml --max-jobs 16 --memory-per-job 8GB\n\n# Run specific series only\ncausaliq-workflow run experiment.yaml --series pc_series\n\n# Export results in specific format\ncausaliq-workflow results export workflow-abc123 --format csv --output results/\n</code></pre>"},{"location":"example_workflows/#python-api-examples","title":"Python API Examples","text":""},{"location":"example_workflows/#basic-workflow-execution","title":"Basic Workflow Execution","text":"<pre><code>from causaliq_workflow import WorkflowManager, ConfigurationManager\n\n# Load and validate configuration\nconfig_manager = ConfigurationManager()\nworkflow_config = config_manager.load_workflow(\"pc_ges_comparison.yaml\")\n\n# Execute workflow\nworkflow_manager = WorkflowManager()\nresult = workflow_manager.execute_workflow(workflow_config)\n\n# Access results by series\npc_results = result.get_series_results(\"pc_series\")\nges_results = result.get_series_results(\"ges_series\")\n</code></pre>"},{"location":"example_workflows/#series-analysis","title":"Series Analysis","text":"<pre><code>from causaliq_workflow.analysis import SeriesComparison\n\n# Compare algorithm performance across series\ncomparison = SeriesComparison()\ncomparison.add_series(\"PC Algorithm\", pc_results)\ncomparison.add_series(\"GES Algorithm\", ges_results)\n\n# Generate comparison metrics\nmetrics = comparison.compare_algorithms([\"shd\", \"precision\", \"recall\"])\nstatistical_significance = comparison.statistical_test(alpha=0.05)\n\n# Export results\ncomparison.export_results(\"algorithm_comparison.csv\")\n</code></pre>"},{"location":"example_workflows/#configuration-inheritance","title":"Configuration Inheritance","text":"<pre><code>from causaliq_workflow.config import ConfigurationInheritance\n\n# Create experiment based on template\ninheritance = ConfigurationInheritance()\nbase_config = inheritance.load_base(\"base_causal_experiment.yaml\")\n\n# Apply specific overrides\nspecific_config = inheritance.create_derived(\n    base_config,\n    overrides={\n        \"series\": {\n            \"custom_study\": {\n                \"algorithm\": \"fci\",\n                \"hyperparameters\": {\"alpha\": 0.01}\n            }\n        }\n    }\n)\n</code></pre> <p>This focused approach emphasizes the series concept and immediate implementation needs while providing practical examples for the three-month development phase.</p>"},{"location":"example_workflows/#use-case-2-production-causal-inference-workflow","title":"Use Case 2: Production Causal Inference Workflow","text":""},{"location":"example_workflows/#scenario_3","title":"Scenario","text":"<p>A business wants to continuously analyze the causal impact of marketing campaigns on customer behavior using streaming data.</p>"},{"location":"example_workflows/#workflow-configuration","title":"Workflow Configuration","text":"<pre><code># production_inference_workflow.yaml\nmetadata:\n  name: \"marketing_causal_inference\"\n  version: \"2.1\"\n  description: \"Real-time causal inference for marketing effectiveness\"\n  environment: \"production\"\n\nparameters:\n  data_stream: \"kafka://marketing-events\"\n  batch_size: 10000\n  update_frequency: \"1h\"\n  lookback_window: \"30d\"\n\nresources:\n  dask_cluster: \"kubernetes://marketing-cluster\"\n  min_workers: 3\n  max_workers: 20\n  auto_scale: true\n  memory_per_worker: \"4GB\"\n\nmonitoring:\n  metrics_endpoint: \"prometheus://metrics\"\n  alert_threshold: 0.1\n  slack_notifications: true\n\nsteps:\n  - name: \"stream_ingestion\"\n    package: \"causaliq-data\"\n    method: \"ingest_stream\"\n    parameters:\n      source: ${parameters.data_stream}\n      batch_size: ${parameters.batch_size}\n      schema_validation: true\n    outputs: [\"raw_events\"]\n\n  - name: \"real_time_preprocessing\"\n    package: \"causaliq-data\"\n    method: \"preprocess_streaming\"\n    depends_on: [\"stream_ingestion\"]\n    parameters:\n      window_size: ${parameters.lookback_window}\n      features: [\"campaign_type\", \"customer_segment\", \"channel\", \"timestamp\"]\n      target: \"conversion\"\n    inputs: [\"raw_events\"]\n    outputs: [\"processed_batch\"]\n\n  - name: \"causal_graph_update\"\n    package: \"causaliq-discovery\"\n    method: \"incremental_learning\"\n    depends_on: [\"real_time_preprocessing\"]\n    parameters:\n      existing_graph: \"models/marketing_graph.pkl\"\n      update_threshold: 1000\n      stability_check: true\n    inputs: [\"processed_batch\"]\n    outputs: [\"updated_graph\", \"graph_changes\"]\n\n  - name: \"intervention_effects\"\n    package: \"causaliq-inference\"\n    method: \"estimate_ate\"\n    depends_on: [\"causal_graph_update\"]\n    parameters:\n      treatment_vars: [\"campaign_type\", \"channel\"]\n      outcome_var: \"conversion\"\n      adjustment_sets: \"auto\"\n    inputs: [\"updated_graph\", \"processed_batch\"]\n    outputs: [\"treatment_effects\", \"confidence_intervals\"]\n\n  - name: \"anomaly_detection\"\n    package: \"causaliq-monitoring\"\n    method: \"detect_effect_anomalies\"\n    depends_on: [\"intervention_effects\"]\n    parameters:\n      baseline_window: \"7d\"\n      anomaly_threshold: 2.0\n    inputs: [\"treatment_effects\"]\n    outputs: [\"anomalies\", \"alerts\"]\n\n  - name: \"automated_insights\"\n    package: \"causaliq-workflow\"\n    method: \"llm_generate_insights\"\n    depends_on: [\"intervention_effects\", \"anomaly_detection\"]\n    parameters:\n      context: \"marketing_optimization\"\n      include_recommendations: true\n      business_constraints: \"config/business_rules.yaml\"\n    inputs: [\"treatment_effects\", \"anomalies\", \"graph_changes\"]\n    outputs: [\"insights\", \"recommendations\"]\n\n  - name: \"dashboard_update\"\n    package: \"causaliq-visualization\"\n    method: \"update_dashboard\"\n    depends_on: [\"automated_insights\"]\n    parameters:\n      dashboard_id: \"marketing_causal_dashboard\"\n      auto_refresh: true\n    inputs: [\"treatment_effects\", \"insights\", \"recommendations\"]\n    outputs: [\"dashboard_url\"]\n\ntriggers:\n  schedule: \"0 * * * *\"  # Every hour\n  data_threshold: 5000   # Trigger if 5k new events\n\nfailure_handling:\n  retry_attempts: 3\n  fallback_to_previous: true\n  alert_on_failure: true\n</code></pre>"},{"location":"example_workflows/#use-case-3-interactive-research-exploration","title":"Use Case 3: Interactive Research Exploration","text":""},{"location":"example_workflows/#scenario_4","title":"Scenario","text":"<p>An interactive session where researchers explore causal relationships with LLM assistance and iterative refinement.</p>"},{"location":"example_workflows/#workflow-configuration_1","title":"Workflow Configuration","text":"<pre><code># interactive_exploration_workflow.yaml\nmetadata:\n  name: \"interactive_causal_exploration\"\n  version: \"1.0\"\n  description: \"LLM-assisted interactive causal discovery\"\n  mode: \"interactive\"\n\nparameters:\n  data_source: \"research/climate_data.csv\"\n  domain: \"climate_science\"\n  interaction_mode: \"jupyter\"\n\nresources:\n  dask_cluster: \"local\"\n  workers: 2\n  memory_per_worker: \"6GB\"\n\ninteraction:\n  llm_model: \"gpt-4\"\n  enable_suggestions: true\n  save_conversation: true\n\nsteps:\n  - name: \"initial_analysis\"\n    package: \"causaliq-data\"\n    method: \"exploratory_analysis\"\n    parameters:\n      generate_summary: true\n      correlation_analysis: true\n    outputs: [\"data_summary\", \"correlations\"]\n\n  - name: \"llm_initial_consultation\"\n    package: \"causaliq-workflow\"\n    method: \"llm_domain_consultation\"\n    depends_on: [\"initial_analysis\"]\n    parameters:\n      domain: ${parameters.domain}\n      task: \"What causal relationships should we investigate in this climate dataset?\"\n      context_data: true\n    inputs: [\"data_summary\", \"correlations\"]\n    outputs: [\"domain_insights\", \"suggested_relationships\"]\n    interactive: true\n\n  - name: \"guided_discovery\"\n    package: \"causaliq-discovery\"\n    method: \"guided_search\"\n    depends_on: [\"llm_initial_consultation\"]\n    parameters:\n      prior_knowledge: \"domain_insights\"\n      search_strategy: \"hypothesis_driven\"\n    inputs: [\"data_summary\", \"suggested_relationships\"]\n    outputs: [\"candidate_graphs\"]\n    interactive: true\n\n  - name: \"iterative_refinement\"\n    type: \"interactive_loop\"\n    depends_on: [\"guided_discovery\"]\n    max_iterations: 10\n    steps:\n      - name: \"graph_evaluation\"\n        package: \"causaliq-validation\"\n        method: \"evaluate_graph\"\n        inputs: [\"candidate_graphs\"]\n        outputs: [\"evaluation_metrics\"]\n\n      - name: \"llm_feedback\"\n        package: \"causaliq-workflow\"\n        method: \"llm_evaluate_graph\"\n        parameters:\n          domain: ${parameters.domain}\n          include_suggestions: true\n        inputs: [\"candidate_graphs\", \"evaluation_metrics\"]\n        outputs: [\"llm_feedback\", \"improvement_suggestions\"]\n        interactive: true\n\n      - name: \"user_decision\"\n        type: \"user_input\"\n        prompt: \"Based on the LLM feedback, would you like to: (1) Accept graph (2) Refine further (3) Try different approach?\"\n        outputs: [\"user_choice\"]\n\n      - name: \"conditional_refinement\"\n        type: \"conditional\"\n        condition: \"user_choice == 'refine'\"\n        package: \"causaliq-discovery\"\n        method: \"refine_graph\"\n        inputs: [\"candidate_graphs\", \"improvement_suggestions\"]\n        outputs: [\"refined_graphs\"]\n\n  - name: \"final_interpretation\"\n    package: \"causaliq-workflow\"\n    method: \"llm_comprehensive_interpretation\"\n    depends_on: [\"iterative_refinement\"]\n    parameters:\n      domain: ${parameters.domain}\n      include_limitations: true\n      suggest_experiments: true\n    inputs: [\"candidate_graphs\", \"evaluation_metrics\"]\n    outputs: [\"final_interpretation\", \"experiment_suggestions\"]\n\nnotebook_integration:\n  auto_generate_cells: true\n  include_visualizations: true\n  save_checkpoints: true\n</code></pre>"},{"location":"example_workflows/#example-cli-usage","title":"Example CLI Usage","text":"<pre><code># Execute a workflow\ncausaliq-workflow run workflow.yaml --config production.yaml\n\n# Validate workflow before execution\ncausaliq-workflow validate workflow.yaml\n\n# Interactive mode\ncausaliq-workflow interactive --domain healthcare --data patient_data.csv\n\n# Monitor running workflow\ncausaliq-workflow status workflow-123\n\n# Generate workflow template\ncausaliq-workflow template --type discovery --domain finance\n\n# List available packages and methods\ncausaliq-workflow list-methods --package causaliq-discovery\n</code></pre>"},{"location":"example_workflows/#integration-examples","title":"Integration Examples","text":""},{"location":"example_workflows/#python-api-usage","title":"Python API Usage","text":"<pre><code>from causaliq_workflow import WorkflowEngine, DaskClusterManager\n\n# Set up DASK cluster\ncluster_manager = DaskClusterManager()\nclient = cluster_manager.create_local_cluster(workers=4)\n\n# Initialize workflow engine\nengine = WorkflowEngine(client=client)\n\n# Load and execute workflow\nresult = engine.execute_workflow(\"workflow.yaml\")\n\n# Access results\ncausal_graph = result.get_output(\"ensemble_graph\")\ninterpretation = result.get_output(\"interpretation\")\n\n# Interactive LLM consultation\nllm_analyzer = engine.get_llm_analyzer()\ninsights = llm_analyzer.interpret_results(result.all_outputs, domain=\"healthcare\")\n</code></pre>"},{"location":"example_workflows/#jupyter-notebook-integration","title":"Jupyter Notebook Integration","text":"<pre><code>%%causaliq_workflow\nmetadata:\n  name: \"notebook_workflow\"\n\nsteps:\n  - name: \"discovery\"\n    package: \"causaliq-discovery\"\n    method: \"pc\"\n    parameters:\n      alpha: 0.05\n\n# Results automatically displayed in notebook\n</code></pre>"},{"location":"roadmap/","title":"CausalIQ Workflow - Development Roadmap","text":"<p>Last updated: November 20, 2025 Current milestone: CLI Implementation Complete</p>"},{"location":"roadmap/#current-status","title":"\ud83c\udfaf Current Status","text":"<p>\u2705 COMPLETED: Full CLI Implementation - Production-ready workflow execution system</p> <p>Latest commit: <code>36b16d8 feat: implement CLI with real-time workflow execution feedback</code></p> <p>Current Capabilities:</p> <ul> <li>Complete command-line interface: <code>causaliq-workflow run [--dry-run] &lt;workflow&gt;</code></li> <li>Real-time workflow execution with step-by-step feedback  </li> <li>Action registry with plugin architecture</li> <li>100% test coverage (471/471 lines) with full quality compliance</li> <li>Working end-to-end execution from YAML configuration</li> </ul> <p>Next Phase: Algorithm Integration Target: Connect real causal discovery algorithms (PC, GES, bnlearn)</p>"},{"location":"roadmap/#completed-implementation","title":"\u2705 Completed Implementation","text":"<p>See Git commit history for detailed implementation progress</p> <p>Key Commits:</p> <ul> <li><code>ce41487</code> - Action registry with auto-discovery plugin system</li> <li><code>302b70a</code> - WorkflowExecutor with YAML parsing and matrix expansion  </li> <li><code>a2c01da</code> - Action framework with dummy structure learner</li> <li><code>b9c9c81</code> - Schema validation using JSON Schema</li> <li><code>36b16d8</code> - CLI with real-time workflow execution feedback</li> </ul> <p>Current Architecture:</p> <ul> <li>\ud83d\udccb CLI: <code>causaliq-workflow run [--dry-run] &lt;workflow&gt;</code></li> <li>\ud83d\udd0c Action Registry: Auto-discovery plugin system</li> <li>\u2699\ufe0f WorkflowExecutor: YAML parsing, matrix expansion, step execution</li> <li>\ud83d\udcca Schema Validation: JSON Schema with error reporting</li> <li>\ud83e\udde9 Testing: 100% coverage, 201 tests passing</li> </ul>"},{"location":"roadmap/#upcoming-implementation","title":"\ud83d\udee3\ufe0f Upcoming Implementation","text":""},{"location":"roadmap/#milestone-1-enhanced-workflow-execution","title":"Milestone 1: Enhanced Workflow Execution","text":"<p>Target: Production-ready execution with intelligent status tracking and logging</p> <p>Commit 1.1: Basic Task Logging Infrastructure</p> <ul> <li>[ ] log_task() method - Implement formatted message output with status/runtime/files</li> <li>[ ] Message formatting - Standardized format: timestamp, action, status, description</li> <li>[ ] Comprehensive testing - All status types with various input/output scenarios</li> </ul> <p>Commit 1.2: Action Output File Interface</p> <ul> <li>[ ] get_output_files() method - Add to Action base class for file discovery</li> <li>[ ] Default implementation - Empty list for actions without specific outputs</li> <li>[ ] Test integration - Implement in test actions for validation</li> </ul> <p>Commit 1.3: FileManager Foundation</p> <ul> <li>[ ] FileManager class - File existence and comparison utilities</li> <li>[ ] Traditional file logic - Basic exists/missing detection for replace-semantics files</li> <li>[ ] Isolated testing - File operations without workflow integration</li> </ul> <p>Commit 1.4: Skip Logic Implementation</p> <ul> <li>[ ] should_skip_action() method - Determine if action can skip based on existing outputs</li> <li>[ ] Traditional files only - Skip logic for replace-semantics files (no append-semantics yet)</li> <li>[ ] Comprehensive scenarios - Test various file existence and modification patterns</li> </ul> <p>Commit 1.5: ActionExecutor Wrapper</p> <ul> <li>[ ] ActionExecutor class - Wrapper for action execution with status determination</li> <li>[ ] Status logic - EXECUTES vs SKIPS for traditional files in run mode</li> <li>[ ] Mock integration - Test execution wrapper without WorkflowExecutor changes</li> </ul> <p>Commit 1.6: Dry-Run Status Logic</p> <ul> <li>[ ] WOULD_EXECUTE status - Implement dry-run equivalent of EXECUTES</li> <li>[ ] WOULD_SKIP status - Implement dry-run equivalent of SKIPS  </li> <li>[ ] Mode differentiation - Proper status based on run vs dry-run mode</li> </ul> <p>Commit 1.7: WorkflowExecutor Integration</p> <ul> <li>[ ] Logger creation - WorkflowExecutor creates and configures WorkflowLogger</li> <li>[ ] ActionExecutor usage - Replace direct action calls with ActionExecutor wrapper</li> <li>[ ] Regression testing - Ensure all existing workflows continue to pass</li> </ul>"},{"location":"roadmap/#milestone-2-progress-monitoring-user-experience","title":"Milestone 2: Progress Monitoring &amp; User Experience","text":"<p>Target: Real-time progress tracking with resource monitoring and enhanced CLI</p> <p>Commit 2.1: Runtime Estimation Interface</p> <ul> <li>[ ] estimate_runtime() method - Add to Action base class for progress calculation</li> <li>[ ] Default estimation - 1-second default for actions without specific estimates</li> <li>[ ] Progress foundation - Basic estimation without user interface</li> </ul> <p>Commit 2.2: Progress Calculation Engine</p> <ul> <li>[ ] Progress calculation - Aggregate runtime estimates for workflow progress tracking</li> <li>[ ] Background tracking - Progress computation without user interface display</li> <li>[ ] Accuracy testing - Validate progress calculation with various workflow scenarios</li> </ul> <p>Commit 2.3: ProgressReporter Foundation</p> <ul> <li>[ ] ProgressReporter class - Click integration for progress bar display</li> <li>[ ] Basic structure - Progress bar initialization and configuration</li> <li>[ ] Static progress - Progress structure without real-time updates yet</li> </ul> <p>Commit 2.4: Live Progress Integration</p> <ul> <li>[ ] Real-time updates - Connect progress reporter to workflow execution</li> <li>[ ] Action completion - Update progress as actions complete</li> <li>[ ] Optional display - Toggle progress bars based on CLI parameters</li> </ul> <p>Commit 2.5: Status Aggregation &amp; Summary</p> <ul> <li>[ ] Status aggregation - Count tasks by status type (EXECUTES, SKIPS, etc.)</li> <li>[ ] Summary formatting - Clear report with counts, runtime, resource usage</li> <li>[ ] Report accuracy - Comprehensive testing for summary calculation</li> </ul> <p>Commit 2.6: Enhanced Error Reporting</p> <ul> <li>[ ] FAILED status formatting - User-friendly error messages with actionable suggestions</li> <li>[ ] INVALID_* status details - Clear parameter validation error reporting</li> <li>[ ] Error summary - Aggregate error information for debugging</li> </ul> <p>Commit 2.7: CLI Enhancement &amp; Testing</p> <ul> <li>[ ] Enhanced CLI options - Improve CLI based on real-world testing feedback</li> <li>[ ] Better error messages - Refine error handling discovered during external package testing</li> <li>[ ] Path resolution improvements - Handle edge cases found during real usage</li> <li>[ ] Complete CLI testing - All logging features with file output verification</li> <li>[ ] Performance validation - Logging overhead measurement and optimization</li> </ul>"},{"location":"roadmap/#milestone-3-advanced-features-production-readiness","title":"Milestone 3: Advanced Features &amp; Production Readiness","text":"<p>Target: Advanced workflow features with comparison modes and resource monitoring</p> <p>Commit 3.1: Append-Semantics File Support</p> <ul> <li>[ ] get_output_contribution_key() - Action method for append-semantics identification</li> <li>[ ] has_existing_contribution() - Check if action's section exists in append-semantics files</li> <li>[ ] FileManager enhancement - Handle metadata.json style files with action-specific sections</li> </ul> <p>Commit 3.2: File Comparison Foundation</p> <ul> <li>[ ] Comparison utilities - Basic file diff and comparison logic in FileManager</li> <li>[ ] Text file diffs - Generate meaningful comparisons for various file types</li> <li>[ ] Isolated testing - File comparison without execution integration</li> </ul> <p>Commit 3.3: Compare Mode Status Logic</p> <ul> <li>[ ] IDENTICAL status - Implement when re-execution produces same outputs</li> <li>[ ] DIFFERENT status - Implement when re-execution produces changed outputs</li> <li>[ ] Compare mode execution - New execution path for output comparison</li> </ul> <p>Commit 3.4: Resource Monitoring Infrastructure</p> <ul> <li>[ ] Memory monitoring - Track memory usage during action execution</li> <li>[ ] CPU monitoring - Track CPU utilization and report in log messages</li> <li>[ ] Resource reporting - Include resource usage in status messages</li> </ul> <p>Commit 3.5: Timeout Handling</p> <ul> <li>[ ] Timeout configuration - Per-action timeout settings and monitoring</li> <li>[ ] TIMED_OUT status - Graceful termination with timeout status reporting</li> <li>[ ] Cleanup logic - Proper resource cleanup when actions exceed timeout</li> </ul> <p>Commit 3.6: Advanced Progress Features</p> <ul> <li>[ ] Estimated completion - Real-time estimates based on action progress</li> <li>[ ] Resource display - Memory/CPU usage in progress indicators</li> <li>[ ] Smart updates - Adaptive progress update frequency based on action complexity</li> </ul> <p>Commit 3.7: Integration Testing &amp; Optimization</p> <ul> <li>[ ] End-to-end validation - Complete logging system integration testing</li> <li>[ ] Matrix workflows - Multi-action workflow testing with all status types</li> <li>[ ] 100% coverage - Maintain comprehensive test coverage for all logging features</li> <li>[ ] Compare mode testing - Complete integration testing for output comparison</li> <li>[ ] Resource monitoring validation - Accuracy testing for memory/CPU tracking</li> <li>[ ] Performance optimization - Final performance tuning for large workflows</li> <li>[ ] Documentation updates - Complete documentation for all advanced logging features</li> </ul>"},{"location":"roadmap/#milestone-4-algorithm-integration-foundation","title":"Milestone 4: Algorithm Integration Foundation","text":"<p>Target: Robust testing infrastructure with concrete action implementations</p> <p>Commit 4.1: Test Action Fixtures</p> <ul> <li>[ ] Concrete test actions - Real algorithm implementations in tests/functional/fixtures</li> <li>[ ] PC algorithm test action - Simple structure learning with actual causal discovery logic</li> <li>[ ] Multiple test algorithms - Different algorithms to test various scenarios (GES, constraint-based)</li> <li>[ ] Data processing - Handle real CSV data and generate actual GraphML outputs</li> </ul> <p>Commit 4.2: Algorithm Testing Infrastructure</p> <ul> <li>[ ] Output standardization - GraphML files with proper causal graph representation</li> <li>[ ] Parameter validation - Algorithm-specific parameter handling and validation</li> <li>[ ] Data fixtures - Test datasets for consistent algorithm validation</li> <li>[ ] Results validation - Verify GraphML output structure and content</li> </ul> <p>Commit 4.3: End-to-End Validation</p> <ul> <li>[ ] Complete workflow testing - CLI \u2192 ActionRegistry \u2192 Test actions \u2192 Results</li> <li>[ ] Matrix workflow validation - Multi-algorithm, multi-dataset test scenarios</li> <li>[ ] Performance benchmarking - Execution time and resource usage with real algorithms</li> <li>[ ] Documentation examples - Complete usage examples using test action fixtures</li> </ul>"},{"location":"roadmap/#milestone-success-metrics","title":"\ud83c\udfaf Milestone Success Metrics","text":"<p>Milestone 1 (Enhanced Workflow Execution):</p> <ul> <li>\u2705 Intelligent skip logic based on existing outputs  </li> <li>\u2705 Comprehensive status tracking (EXECUTES, SKIPS, WOULD_EXECUTE, WOULD_SKIP)</li> <li>\u2705 Production-ready file management and execution wrapper</li> <li>\u2705 Maintain 100% test coverage with enhanced functionality</li> </ul> <p>Milestone 2 (Progress Monitoring &amp; UX):</p> <ul> <li>\u2705 Real-time progress bars with estimated completion times</li> <li>\u2705 Resource usage monitoring (memory, CPU) during execution</li> <li>\u2705 Enhanced CLI with improved error reporting and path resolution</li> <li>\u2705 Comprehensive status summaries and user-friendly error messages</li> </ul> <p>Milestone 3 (Advanced Features &amp; Production):</p> <ul> <li>\u2705 Compare mode for output validation (IDENTICAL, DIFFERENT status)</li> <li>\u2705 Advanced file handling with append-semantics support</li> <li>\u2705 Resource monitoring with timeout handling and cleanup</li> <li>\u2705 Production-ready performance optimization and testing</li> </ul> <p>Milestone 4 (Algorithm Integration Foundation):</p> <ul> <li>\u2705 Concrete test actions with real algorithm implementations in test fixtures</li> <li>\u2705 Standardized GraphML output format for causal graphs</li> <li>\u2705 Complete end-to-end workflow validation with actual algorithm execution</li> <li>\u2705 Sustainable testing infrastructure for future algorithm integration</li> </ul>"},{"location":"roadmap/#possible-future-features","title":"\ud83d\ude80 Possible Future Features","text":"<p>External Algorithm Integration (After robust test infrastructure): - Multi-language workflows (R bnlearn, Java Tetrad, Python causal-learn) - External CausalIQ package integration (discovery, analysis) - Matrix-driven algorithm comparisons across datasets - Automatic dataset download and preprocessing</p> <p>Production Features: - \ud83d\udccb Workflow queuing - CI-style runner management - \ud83d\udcca Monitoring dashboard - Real-time execution tracking - \ud83d\uddfa Artifacts &amp; caching - Persistent storage, result reuse - \ud83d\udd12 Security &amp; isolation - Secrets management, containers - \ud83d\udcc8 Performance optimization** - Resource limits, scheduling</p> <p>Research Platform: - \ud83e\udd16 LLM integration - Model averaging, hypothesis generation - \ud83c\udf10 Web interface - Browser-based workflow designer - \ud83d\ude80 Cloud deployment - AWS/GCP/Azure runners - \ud83d\udc65 Collaboration - Multi-researcher workflows - \ud83d\udcda Publication workflows - Reproducible research outputs</p> <p>Advanced Capabilities: - Workflow marketplace - Sharing and discovering research workflow templates - Interactive notebooks - Jupyter integration with workflow execution - Multi-machine execution - Distributed workflows across compute clusters - AI-assisted optimization - Automated hyperparameter and workflow tuning - Integration ecosystem - Plugins for major research tools and platforms</p> <p>This roadmap leverages Git commit history for completed work, provides detailed milestone-based planning for upcoming functionality, and outlines future possibilities.</p>"},{"location":"technical_architecture/","title":"CausalIQ Workflow - Technical Architecture","text":""},{"location":"technical_architecture/#architectural-vision-configuration-free-research-reproducibility-platform","title":"Architectural Vision: Configuration-Free Research Reproducibility Platform","text":""},{"location":"technical_architecture/#core-architecture-principles","title":"Core Architecture Principles","text":"<p>CausalIQ Workflow is designed as a zero-configuration workflow orchestration engine that enables reproducible causal discovery research through:</p> <ol> <li>Auto-Discovery Action System: Actions are automatically discovered from installed Python packages - no configuration files required</li> <li>Sequential Steps with Matrix Expansion: Simple, predictable workflow execution with powerful parameterization</li> <li>Conservative Execution: Actions skip work if outputs already exist, enabling safe workflow restart and efficient re-runs</li> <li>Mode-Based Operation: <code>--mode=dry-run|run|compare</code> provides validation, execution, and functional testing capabilities</li> <li>Convention-Based Plugin Pattern: Actions follow simple naming conventions for automatic registration</li> <li>Implicit Parameter Passing: CLI parameters flow through workflows without formal definitions</li> <li>Action-Level Validation: Each action validates its own inputs (integrated with dry-run capability)</li> <li>Workflow Composition: Workflows can call other workflows via <code>cwork</code> commands, enabling complex research workflows</li> <li>Standardized Output: Fixed filenames by type (<code>graph.xml</code>, <code>metadata.json</code>, <code>trace.csv</code>) with hierarchical organization</li> </ol>"},{"location":"technical_architecture/#zero-configuration-auto-discovery","title":"Zero-Configuration Auto-Discovery","text":"<p>How Action Discovery Works: When a workflow runs, the system automatically finds and registers all available actions through Python's module introspection - no registry files, no configuration needed. Simply install a package that exports an <code>Action</code> class and it becomes available in workflows.</p>"},{"location":"technical_architecture/#research-reproducibility-pattern","title":"Research Reproducibility Pattern","text":"<p>Paper Reproduction = Workflow-of-Workflows where: - Top-level workflow defines paper reproduction strategy - Component workflows handle specific analyses (structure learning, visualization, etc.) - causaliq-papers processes workflow dependencies to generate targeted execution plans - causaliq-workflow executes the optimized workflow graph</p>"},{"location":"technical_architecture/#example-simplified-workflow-architecture","title":"Example: Simplified Workflow Architecture","text":"<pre><code># paper-reproduction.yml (top-level workflow)\nid: \"peters2023causal-reproduction\"\nmatrix:\n  network: [\"asia\", \"cancer\"]\n  algorithm: [\"pc\", \"ges\", \"fci\"]\n\nsteps:\n  - name: \"Structure Learning\"\n    uses: \"causaliq-discovery\"\n    with:\n      algorithm: \"{{algorithm}}\"\n      dataset: \"/data/{{network}}\"\n      output: \"/results/{{id}}/{{algorithm}}/{{network}}\"\n\n  - name: \"Analysis\"\n    uses: \"causaliq-analysis\"\n    with:\n      operation: \"evaluate-graph\"\n      graph: \"/results/{{id}}/{{algorithm}}/{{network}}\"\n\n  - name: \"Generate Figures\"\n    uses: \"causaliq-visualization\"\n    with:\n      input: \"/results/{{id}}/{{algorithm}}/{{network}}\"\n      output: \"/results/{{id}}/figures\"\n\n# Workflow composition via CLI calls\nsteps:\n  - name: \"Run Structure Learning\"\n    run: |\n      cwork structure-learning.yml \\\n        --network=\"{{network}}\" --algorithm=\"{{algorithm}}\"\n</code></pre>"},{"location":"technical_architecture/#conservative-execution-mode-control","title":"Conservative Execution &amp; Mode Control","text":"<pre><code># CLI execution modes\ncwork workflow.yml --mode=dry-run    # Default: validate and preview (no execution)\ncwork workflow.yml --mode=run        # Execute workflow (skip if outputs exist)\ncwork workflow.yml --mode=compare    # Re-execute and compare with existing outputs\n</code></pre>"},{"location":"technical_architecture/#action-intelligence-efficiency","title":"Action Intelligence &amp; Efficiency","text":"<pre><code># Actions support robust execution patterns with validation\naction.run(inputs, dry_run=True)    # (a) Validate and preview execution\naction.run(inputs, force=False)     # (b) Skip if output file already exists (conservative)\naction.compare(inputs)              # (c) Regenerate and compare with filesystem\n\n# Implicit parameter passing - no formal definitions needed\nclass StructureLearnerAction(Action):\n    def run(self, inputs, matrix_job, dry_run=False):\n        # Action handles its own validation\n        self.validate_inputs(inputs)\n\n        # Conservative execution: skip if outputs exist\n        if not inputs.force and self.outputs_exist(inputs):\n            return self.load_existing_outputs(inputs)\n\n        if dry_run:\n            return self.simulate_execution(inputs)\n        return self.learn_structure(inputs)\n</code></pre>"},{"location":"technical_architecture/#system-overview","title":"System Overview","text":"<p>The causaliq-workflow serves as the orchestration layer within the CausalIQ ecosystem, coordinating causal discovery experiments through GitHub Actions-inspired YAML workflows. This architecture models causal discovery experiments as familiar CI/CD workflows, providing unprecedented flexibility while leveraging proven workflow patterns.</p>"},{"location":"technical_architecture/#current-implementation-status","title":"Current Implementation Status","text":""},{"location":"technical_architecture/#implemented-components-v010","title":"\u2705 Implemented Components (v0.1.0)","text":"<ol> <li>Auto-Discovery Action Registry (<code>causaliq_workflow.registry</code>)</li> <li>Zero-configuration action discovery: Automatically finds all installed action packages</li> <li>Convention-based registration: Packages export 'CausalIQAction' class for automatic registration  </li> <li>Python module introspection: Uses <code>pkgutil.iter_modules()</code> for discovery without config files</li> <li>Runtime action lookup: Dynamic resolution of action names to classes during workflow execution</li> <li> <p>Namespace isolation: Each action package maintains its own namespace and dependencies</p> </li> <li> <p>Action Framework (<code>causaliq_workflow.action</code>)</p> </li> <li>Abstract base class for workflow actions</li> <li>Type-safe input/output specifications</li> <li>Comprehensive error handling with <code>ActionExecutionError</code> and <code>ActionValidationError</code></li> <li> <p>100% test coverage with unit and functional tests</p> </li> <li> <p>Schema Validation (<code>causaliq_workflow.schema</code>)</p> </li> <li>JSON Schema-based workflow validation</li> <li>Support for GitHub Actions-style syntax</li> <li>Matrix variables, with parameters, data_root/output_root validation</li> <li> <p>Comprehensive error reporting with schema path context</p> </li> <li> <p>Workflow Execution Engine (<code>causaliq_workflow.workflow</code>)</p> </li> <li><code>WorkflowExecutor</code> class for parsing YAML workflows</li> <li>Matrix expansion with cartesian product generation</li> <li>Path construction from matrix variables and workflow configuration</li> <li> <p>Integration with auto-discovery action registry</p> </li> <li> <p>Example Action Package (<code>tests/functional/fixtures/test_action</code>)</p> </li> <li>Reference implementation demonstrating auto-discovery pattern</li> <li>GraphML output format for causal graph representation</li> <li>Matrix variable support (dataset, algorithm parameters)</li> <li> <p>Real filesystem operations with proper path construction</p> </li> <li> <p>Workflow Schema (<code>causaliq_workflow.schemas.causaliq-workflow.json</code>)</p> </li> <li>GitHub Actions-inspired syntax with causal discovery extensions</li> <li>Matrix strategy support for parameterized experiments</li> <li>Path construction with <code>data_root</code>, <code>output_root</code>, and <code>id</code> fields</li> <li>Action parameters via <code>with</code> blocks</li> </ol>"},{"location":"technical_architecture/#auto-discovery-architecture-how-it-works","title":"Auto-Discovery Architecture: How It Works","text":""},{"location":"technical_architecture/#the-discovery-process-step-by-step","title":"The Discovery Process: Step-by-Step","text":""},{"location":"technical_architecture/#1-workflow-execution-begins","title":"1. Workflow Execution Begins","text":"<p>When you run <code>causaliq-workflow my-experiment.yml</code>, the system: - Creates a new <code>ActionRegistry</code> instance - Triggers the automatic discovery process - Scans all installed Python packages for actions</p>"},{"location":"technical_architecture/#2-package-scanning-phase","title":"2. Package Scanning Phase","text":"<p>The registry uses Python's module introspection to: - Iterate through all importable modules using <code>pkgutil.iter_modules()</code> - Attempt to import each module safely (catching import errors) - Look for modules that export an 'Action' class</p>"},{"location":"technical_architecture/#3-convention-based-registration","title":"3. Convention-Based Registration","text":"<p>For each discovered module, the system: - Checks if the module has a 'CausalIQAction' attribute - Verifies that it's a subclass of <code>causaliq_workflow.action.Action</code> - Registers the action using the module name as the action identifier - Builds a runtime lookup table: <code>{action_name: CausalIQAction_class}</code></p>"},{"location":"technical_architecture/#4-workflow-resolution","title":"4. Workflow Resolution","text":"<p>When a workflow step specifies <code>uses: \"my-custom-action\"</code>: - The registry looks up \"my-custom-action\" in the registered actions - Instantiates the corresponding CausalIQAction class - Passes workflow parameters to the action's <code>run()</code> method</p>"},{"location":"technical_architecture/#zero-configuration-plugin-pattern","title":"Zero-Configuration Plugin Pattern","text":""},{"location":"technical_architecture/#creating-a-new-action-package","title":"Creating a New Action Package","text":"<p>Developers create action packages by following a simple convention:</p> <p>Step 1: Package Structure</p> <pre><code>my-custom-action/\n\u251c\u2500\u2500 pyproject.toml           # Standard Python package config\n\u251c\u2500\u2500 my_custom_action/        # Package directory  \n\u2502   \u2514\u2500\u2500 __init__.py         # Must export 'Action' class\n\u2514\u2500\u2500 README.md\n</code></pre> <p>Step 2: Action Implementation</p> <pre><code># my_custom_action/__init__.py\nfrom causaliq_workflow.action import Action\n\nclass CausalIQAction(Action):  # Must be named 'CausalIQAction'\n    name = \"my-custom-action\"\n    description = \"Performs custom analysis\"\n\n    def run(self, inputs):\n        # Action logic here\n        return {\"result\": \"analysis complete\"}\n</code></pre> <p>Step 3: Installation &amp; Discovery</p> <pre><code>pip install my-custom-action    # Install the package\ncausaliq-workflow my-workflow.yml  # Action automatically discovered\n</code></pre>"},{"location":"technical_architecture/#why-this-works","title":"Why This Works","text":"<ul> <li>No configuration files: No registry.json, no plugin.xml, no setup scripts</li> <li>Standard Python packaging: Uses familiar pyproject.toml and pip install</li> <li>Immediate availability: Actions become available as soon as the package is installed</li> <li>Namespace safety: 'CausalIQAction' avoids conflicts with generic 'Action' classes in other packages</li> <li>Version management: Standard semantic versioning through package versions</li> </ul>"},{"location":"technical_architecture/#core-architectural-decisions","title":"Core Architectural Decisions","text":""},{"location":"technical_architecture/#github-actions-foundation","title":"GitHub Actions Foundation","text":"<p>The architecture is built on GitHub Actions workflow patterns, adapted for causal discovery:</p> <pre><code>name: \"Causal Discovery Experiment\"\nid: \"asia-comparison-001\"\ndata_root: \"/data\"\noutput_root: \"/results\"\n\nmatrix:\n  dataset: [\"asia\", \"cancer\"]  \n  algorithm: [\"pc\", \"ges\"]\n\nsteps:\n  - name: \"Structure Learning\"\n    uses: \"dummy-structure-learner\"\n    with:\n      alpha: 0.05\n      max_iter: 1000\n</code></pre>"},{"location":"technical_architecture/#action-based-components","title":"Action-Based Components","text":"<p>Actions are reusable workflow components with semantic versioning:</p> <pre><code>class Action(ABC):\n    \"\"\"Abstract base class for all workflow actions.\"\"\"\n\n    name: str                    # Action identifier\n    version: str                 # Semantic version\n    description: str             # Human description  \n    inputs: Dict[str, ActionInput]   # Type-safe inputs\n    outputs: Dict[str, str]      # Output descriptions\n\n    @abstractmethod\n    def run(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Execute the action with given inputs.\"\"\"\n</code></pre>"},{"location":"technical_architecture/#core-architecture-components","title":"Core Architecture Components","text":""},{"location":"technical_architecture/#1-workflow-execution-engine-causaliq_workflowworkflow","title":"1. Workflow Execution Engine (<code>causaliq_workflow.workflow</code>)","text":"<pre><code>class WorkflowExecutor:\n    \"\"\"Parse and execute GitHub Actions-style workflows with matrix expansion.\"\"\"\n\n    def parse_workflow(self, workflow_path: Union[str, Path]) -&gt; Dict[str, Any]:\n        \"\"\"Parse workflow YAML file with validation and template variable validation.\"\"\"\n\n    def expand_matrix(self, matrix: Dict[str, List[Any]]) -&gt; List[Dict[str, Any]]:\n        \"\"\"Expand matrix variables into individual job configurations.\"\"\"\n\n    def _extract_template_variables(self, text: Any) -&gt; Set[str]:\n        \"\"\"Extract {{variable}} patterns from strings using regex.\"\"\"\n\n    def _validate_template_variables(self, variables: Set[str], context: Set[str]) -&gt; None:\n        \"\"\"Validate template variables against available context.\"\"\"\n\n    def _collect_template_variables(self, obj: Any) -&gt; Set[str]:\n        \"\"\"Recursively collect template variables from workflow configuration.\"\"\"\n</code></pre> <p>Current Implementation Status: \u2705 Phase 1 Complete - Parse and validate YAML workflow files - Matrix expansion with cartesian product generation - Template variable validation with context checking - Path construction from matrix variables - 100% test coverage with comprehensive error handling</p> <p>Next Phase: Step execution, environment management, conditional execution</p>"},{"location":"technical_architecture/#2-package-level-algorithm-registry-causaliq_workflowalgorithms","title":"2. Package-Level Algorithm Registry (<code>causaliq_workflow.algorithms</code>)","text":"<pre><code>class AlgorithmRegistry:\n    \"\"\"Manage package-level algorithm plugins.\"\"\"\n\n    def discover_packages(self) -&gt; List[AlgorithmPackage]:\n        \"\"\"Auto-discover bnlearn, Tetrad, causal-learn packages.\"\"\"\n\n    def execute_algorithm(self, package: str, algorithm: str, \n                         data: Dataset, params: Dict) -&gt; Result:\n        \"\"\"Execute algorithm with cross-language bridge handling.\"\"\"\n\nclass BnlearnPackage:\n    \"\"\"R bnlearn package integration via rpy2.\"\"\"\n\nclass TetradPackage:\n    \"\"\"Java Tetrad package integration via py4j.\"\"\"\n\nclass CausalLearnPackage:  \n    \"\"\"Python causal-learn direct integration.\"\"\"\n</code></pre>"},{"location":"technical_architecture/#3-action-based-component-library-causaliq_workflowactions","title":"3. Action-Based Component Library (<code>causaliq_workflow.actions</code>)","text":"<pre><code>class ActionRegistry:\n    \"\"\"Manage reusable workflow actions.\"\"\"\n\n    def register_action(self, name: str, version: str, action: Action) -&gt; None:\n        \"\"\"Register versioned action: load-network@v1.\"\"\"\n\n    def execute_action(self, action_ref: str, inputs: Dict) -&gt; ActionResult:\n        \"\"\"Execute action with input validation and output handling.\"\"\"\n\nclass LoadNetworkAction(Action):\n    \"\"\"Action: load-network@v1 - Load causal network dataset.\"\"\"\n\nclass CausalDiscoveryAction(Action):\n    \"\"\"Action: causal-discovery@v1 - Run causal discovery algorithm.\"\"\"\n\nclass EvaluateGraphAction(Action):\n    \"\"\"Action: evaluate-graph@v1 - Evaluate learned graph against true graph.\"\"\"\n</code></pre>"},{"location":"technical_architecture/#ci-workflow-syntax-examples","title":"CI Workflow Syntax Examples","text":""},{"location":"technical_architecture/#basic-matrix-workflow","title":"Basic Matrix Workflow","text":"<pre><code>name: \"Algorithm Comparison\"\n\nstrategy:\n  matrix:\n    algorithm: [\"PC\", \"GES\", \"LINGAM\"]\n    network: [\"asia\", \"sachs\", \"alarm\"]\n    sample_size: [100, 500, 1000]\n\n  exclude:\n    - algorithm: \"LINGAM\"\n      network: \"alarm\"  # LINGAM doesn't work with discrete data\n\nsteps:\n  - name: \"Load Network Data\"\n    uses: \"load-network@v1\"\n    with:\n      network_name: \"${{ matrix.network }}\"\n      sample_size: \"${{ matrix.sample_size }}\"\n\n  - name: \"Run Causal Discovery\"\n    uses: \"causal-discovery@v1\"\n    with:\n      algorithm: \"${{ matrix.algorithm }}\"\n      package: \"auto-detect\"\n      data: \"${{ steps.load_network.outputs.dataset }}\"\n\n  - name: \"Evaluate Results\"\n    uses: \"evaluate-graph@v1\"\n    with:\n      learned_graph: \"${{ steps.causal_discovery.outputs.graph }}\"\n      true_graph: \"${{ steps.load_network.outputs.true_graph }}\"\n</code></pre>"},{"location":"technical_architecture/#conditional-execution","title":"Conditional Execution","text":"<pre><code>steps:\n  - name: \"Run PC Algorithm\"\n    uses: \"causal-discovery@v1\"\n    with:\n      algorithm: \"pc\"\n      package: \"bnlearn\"\n\n  - name: \"Run GES Algorithm\"\n    if: \"${{ matrix.sample_size &gt;= 500 }}\"  # Only for larger samples\n    uses: \"causal-discovery@v1\"\n    with:\n      algorithm: \"ges\"\n      package: \"causal-learn\"\n</code></pre>"},{"location":"technical_architecture/#dask-integration-architecture","title":"DASK Integration Architecture","text":""},{"location":"technical_architecture/#ci-workflow-dask-task-graph-conversion","title":"CI Workflow \u2192 DASK Task Graph Conversion","text":"<pre><code>class DaskTaskGraphBuilder:\n    \"\"\"Convert CI workflows into DASK task graphs.\"\"\"\n\n    def build_workflow_graph(self, workflow: WorkflowDefinition) -&gt; Dict:\n        \"\"\"Convert CI workflow steps into DASK computation graph.\"\"\"\n\n    def handle_matrix_strategy(self, strategy: MatrixStrategy) -&gt; List[TaskDefinition]:\n        \"\"\"Convert matrix strategy into parallel DASK tasks.\"\"\"\n\n    def manage_cross_language_bridges(self, action: Action) -&gt; TaskWrapper:\n        \"\"\"Manage R/Java bridges with proper lifecycle and cleanup.\"\"\"\n</code></pre>"},{"location":"technical_architecture/#resource-management","title":"Resource Management","text":"<p>CI workflow features map to DASK execution controls:</p> <ul> <li><code>max_parallel</code>: DASK worker pool size limitation</li> <li><code>timeout_minutes</code>: Per-task timeout enforcement</li> <li><code>runs_on</code>: DASK cluster specification (local/remote)</li> <li><code>fail_fast</code>: Task failure propagation strategy</li> </ul>"},{"location":"technical_architecture/#cross-language-integration","title":"Cross-Language Integration","text":""},{"location":"technical_architecture/#r-bnlearn-integration","title":"R bnlearn Integration","text":"<ul> <li>Bridge: rpy2 with automatic data conversion</li> <li>Lifecycle: Package-level R session management</li> <li>Error Handling: Graceful R exception translation</li> </ul>"},{"location":"technical_architecture/#java-tetrad-integration","title":"Java Tetrad Integration","text":"<ul> <li>Bridge: py4j with JVM lifecycle management</li> <li>Data Conversion: Pandas \u2194 Tetrad data structures</li> <li>Resource Management: Proper JVM cleanup</li> </ul>"},{"location":"technical_architecture/#python-causal-learn-integration","title":"Python causal-learn Integration","text":"<ul> <li>Direct Integration: Native Python execution</li> <li>Optimised Paths: No cross-language overhead</li> <li>Data Handling: Efficient NumPy/Pandas operations</li> </ul>"},{"location":"technical_architecture/#template-processing","title":"Template Processing","text":""},{"location":"technical_architecture/#github-actions-style-variables","title":"GitHub Actions-Style Variables","text":"<pre><code>env:\n  RANDOM_SEED: 42\n  DATA_DIR: \"${{ github.workspace }}/data\"\n\nsteps:\n  - name: \"Process ${{ matrix.network }} with ${{ matrix.algorithm }}\"\n    with:\n      output_path: \"${{ env.DATA_DIR }}/results/${{ matrix.network }}_${{ matrix.algorithm }}.json\"\n</code></pre>"},{"location":"technical_architecture/#jinja2-implementation","title":"Jinja2 Implementation","text":"<ul> <li>Syntax: <code>${{ variable.property }}</code> exactly matching GitHub Actions</li> <li>Context: Matrix variables, environment variables, step outputs</li> <li>Security: Safe template processing with variable validation</li> </ul>"},{"location":"technical_architecture/#integration-with-causaliq-ecosystem","title":"Integration with CausalIQ Ecosystem","text":""},{"location":"technical_architecture/#package-coordination","title":"Package Coordination","text":"<ul> <li>causaliq-discovery: Core algorithms integrated as package plugins</li> <li>causaliq-knowledge: Knowledge provision via action-based architecture</li> <li>causaliq-analysis: Statistical analysis actions and workflow post-processing</li> <li>causaliq-experiments: Configuration and result storage with CI workflow metadata</li> </ul>"},{"location":"technical_architecture/#development-standards","title":"Development Standards","text":"<ul> <li>GitHub Actions schema compliance: Official JSON schema for validation</li> <li>Action versioning: Semantic versioning for all reusable actions</li> <li>CausalIQ integration standards: Plugin architecture, result standardisation</li> <li>79-character line limit: All code adheres to CausalIQ formatting standards</li> <li>Type safety: Full MyPy type checking with strict configuration</li> </ul>"},{"location":"technical_architecture/#design-patterns","title":"Design Patterns","text":""},{"location":"technical_architecture/#yaml-first-configuration","title":"YAML-First Configuration","text":"<ul> <li>All workflow functionality expressible through YAML</li> <li>External workflow definitions without code changes</li> <li>Clear, actionable error messages for configuration issues</li> </ul>"},{"location":"technical_architecture/#package-level-plugin-architecture","title":"Package-Level Plugin Architecture","text":"<ul> <li>Dynamic discovery and registration of algorithm packages</li> <li>Cross-language bridge management at package level</li> <li>Preference resolution for algorithm conflicts</li> </ul>"},{"location":"technical_architecture/#action-based-composability","title":"Action-Based Composability","text":"<ul> <li>Reusable, versioned workflow components</li> <li>Standardised input/output interfaces</li> <li>Community potential for shared actions</li> </ul> <p>This architecture transforms causal discovery workflow definition from domain-specific patterns into familiar CI/CD workflows, dramatically reducing the learning curve while providing enterprise-grade features for research.</p>"},{"location":"api/actions/","title":"Action Framework","text":"<p>The action framework provides the foundational classes for building reusable workflow components that follow GitHub Actions patterns.</p>"},{"location":"api/actions/#core-classes","title":"Core Classes","text":""},{"location":"api/actions/#causaliq_workflowaction","title":"causaliq_workflow.action","text":""},{"location":"api/actions/#causaliq_workflow.action.Action","title":"Action","text":"<p>Base class for all workflow actions.</p> <p>Methods:</p> <ul> <li> <code>run</code>             \u2013              <p>Execute action with validated inputs, return outputs.</p> </li> <li> <code>validate_inputs</code>             \u2013              <p>Validate input values against input specifications.</p> </li> </ul>"},{"location":"api/actions/#causaliq_workflow.action.Action.run","title":"run  <code>abstractmethod</code>","text":"<pre><code>run(\n    inputs: Dict[str, Any],\n    mode: str = \"dry-run\",\n    context: Optional[WorkflowContext] = None,\n    logger: Optional[WorkflowLogger] = None,\n) -&gt; Dict[str, Any]\n</code></pre> <p>Execute action with validated inputs, return outputs.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Dict[str, Any]</code>           \u2013            <p>Dictionary of output values keyed by output name</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ActionExecutionError</code>             \u2013            <p>If action execution fails</p> </li> </ul>"},{"location":"api/actions/#causaliq_workflow.action.Action.run(inputs)","title":"<code>inputs</code>","text":"(<code>Dict[str, Any]</code>)           \u2013            <p>Dictionary of input values keyed by input name</p>"},{"location":"api/actions/#causaliq_workflow.action.Action.run(mode)","title":"<code>mode</code>","text":"(<code>str</code>, default:                   <code>'dry-run'</code> )           \u2013            <p>Execution mode ('dry-run', 'run', 'compare')</p>"},{"location":"api/actions/#causaliq_workflow.action.Action.run(context)","title":"<code>context</code>","text":"(<code>Optional[WorkflowContext]</code>, default:                   <code>None</code> )           \u2013            <p>Workflow context for optimization and intelligence</p>"},{"location":"api/actions/#causaliq_workflow.action.Action.run(logger)","title":"<code>logger</code>","text":"(<code>Optional[WorkflowLogger]</code>, default:                   <code>None</code> )           \u2013            <p>Optional logger for task execution reporting</p>"},{"location":"api/actions/#causaliq_workflow.action.Action.validate_inputs","title":"validate_inputs","text":"<pre><code>validate_inputs(inputs: Dict[str, Any]) -&gt; bool\n</code></pre> <p>Validate input values against input specifications.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if all inputs are valid</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ActionValidationError</code>             \u2013            <p>If validation fails</p> </li> </ul>"},{"location":"api/actions/#causaliq_workflow.action.Action.validate_inputs(inputs)","title":"<code>inputs</code>","text":"(<code>Dict[str, Any]</code>)           \u2013            <p>Dictionary of input values to validate</p>"},{"location":"api/actions/#causaliq_workflow.action.ActionInput","title":"ActionInput  <code>dataclass</code>","text":"<pre><code>ActionInput(\n    name: str,\n    description: str,\n    required: bool = False,\n    default: Any = None,\n    type_hint: str = \"Any\",\n)\n</code></pre> <p>Define action input specification.</p>"},{"location":"api/actions/#causaliq_workflow.action.ActionOutput","title":"ActionOutput  <code>dataclass</code>","text":"<pre><code>ActionOutput(name: str, description: str, value: Any)\n</code></pre> <p>Define action output specification.</p>"},{"location":"api/actions/#exception-handling","title":"Exception Handling","text":""},{"location":"api/actions/#causaliq_workflow.action.ActionExecutionError","title":"ActionExecutionError","text":"<p>Raised when action execution fails.</p>"},{"location":"api/actions/#causaliq_workflow.action.ActionValidationError","title":"ActionValidationError","text":"<p>Raised when action input validation fails.</p>"},{"location":"api/actions/#quick-example","title":"Quick Example","text":"<pre><code>from causaliq_workflow.action import Action, ActionExecutionError\nfrom typing import Any, Dict\n\nclass MyStructureLearnerAction(Action):\n    \"\"\"Custom structure learning action.\"\"\"\n\n    name = \"my-structure-learner\"\n    version = \"1.0.0\"\n    description = \"Custom causal structure learning implementation\"\n\n    def run(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Execute the structure learning algorithm.\"\"\"\n        try:\n            # Your implementation here\n            return {\n                \"graph_path\": \"/path/to/output.graphml\",\n                \"node_count\": 5,\n                \"edge_count\": 8,\n            }\n        except Exception as e:\n            raise ActionExecutionError(f\"Structure learning failed: {e}\") from e\n</code></pre>"},{"location":"api/actions/#design-patterns","title":"Design Patterns","text":""},{"location":"api/actions/#action-implementation-guidelines","title":"Action Implementation Guidelines","text":"<ol> <li>Inherit from Action base class - Provides standardized interface</li> <li>Define comprehensive inputs - Use ActionInput for type safety</li> <li>Document outputs clearly - Help users understand action results</li> <li>Handle errors gracefully - Use ActionExecutionError and ActionValidationError</li> <li>Follow semantic versioning - Enable workflow compatibility tracking</li> <li>Create GraphML output - Use standardized format for causal graphs</li> </ol>"},{"location":"api/actions/#testing-your-actions","title":"Testing Your Actions","text":"<pre><code>import pytest\nfrom pathlib import Path\nfrom causaliq_workflow.action import ActionExecutionError\n\ndef test_my_action_success():\n    \"\"\"Test successful action execution.\"\"\"\n    action = MyStructureLearnerAction()\n    inputs = {\n        \"data_path\": \"/path/to/test_data.csv\",\n        \"output_dir\": \"/path/to/output\",\n        \"alpha\": 0.05,\n    }\n\n    result = action.run(inputs)\n\n    assert \"graph_path\" in result\n    assert \"node_count\" in result\n    assert \"edge_count\" in result\n    assert Path(result[\"graph_path\"]).exists()\n\ndef test_my_action_missing_file():\n    \"\"\"Test action fails gracefully with missing input.\"\"\"\n    action = MyStructureLearnerAction()\n    inputs = {\n        \"data_path\": \"/nonexistent/file.csv\",\n        \"output_dir\": \"/path/to/output\",\n    }\n\n    with pytest.raises(ActionExecutionError):\n        action.run(inputs)\n</code></pre> <p>\u2190 Back to API Overview | Next: Action Registry \u2192</p>"},{"location":"api/cli/","title":"CLI Interface","text":"<p>The command-line interface provides direct workflow execution and management capabilities, with support for CI/CD pipeline integration.</p>"},{"location":"api/cli/#core-interface","title":"Core Interface","text":""},{"location":"api/cli/#causaliq_workflowcli","title":"causaliq_workflow.cli","text":""},{"location":"api/cli/#causaliq_workflow.cli","title":"cli","text":"<p>Command-line interface for causaliq-workflow.</p> <p>Functions:</p> <ul> <li> <code>cli</code>             \u2013              <p>Execute CausalIQ workflow files.</p> </li> <li> <code>main</code>             \u2013              <p>Entry point for the CLI.</p> </li> </ul>"},{"location":"api/cli/#causaliq_workflow.cli.cli","title":"cli","text":"<pre><code>cli(workflow_file: Path, mode: str, log_level: str) -&gt; None\n</code></pre> <p>Execute CausalIQ workflow files.</p> <p>WORKFLOW_FILE is the path to a YAML workflow file to execute.</p> <p>Examples:</p> <p>causaliq-workflow experiment.yml              # Validate and preview causaliq-workflow experiment.yml --mode=run  # Execute workflow causaliq-workflow experiment.yml --mode=dry-run --log-level=all              # Detailed preview</p>"},{"location":"api/cli/#causaliq_workflow.cli.main","title":"main","text":"<pre><code>main() -&gt; None\n</code></pre> <p>Entry point for the CLI.</p>"},{"location":"api/cli/#command-line-usage","title":"Command Line Usage","text":""},{"location":"api/cli/#basic-workflow-execution","title":"Basic Workflow Execution","text":"<pre><code># Execute a workflow file\npython -m causaliq_workflow experiments/my-workflow.yml\n\n# Execute with specific output directory\npython -m causaliq_workflow experiments/my-workflow.yml --output-dir /results\n\n# Dry-run mode (parse and validate without execution)\npython -m causaliq_workflow experiments/my-workflow.yml --dry-run\n\n# Verbose output for debugging\npython -m causaliq_workflow experiments/my-workflow.yml --verbose\n</code></pre>"},{"location":"api/cli/#matrix-expansion","title":"Matrix Expansion","text":"<pre><code># Show matrix expansion without execution\npython -m causaliq_workflow experiments/matrix-workflow.yml --expand-matrix\n\n# Execute specific matrix job\npython -m causaliq_workflow experiments/matrix-workflow.yml --matrix-job 0\n\n# Execute range of matrix jobs (useful for parallel execution)\npython -m causaliq_workflow experiments/matrix-workflow.yml --matrix-range 0-5\n</code></pre>"},{"location":"api/cli/#workflow-validation","title":"Workflow Validation","text":"<pre><code># Validate workflow syntax and schema\npython -m causaliq_workflow validate experiments/my-workflow.yml\n\n# Validate all workflows in directory\npython -m causaliq_workflow validate experiments/\n\n# Validate with custom schema\npython -m causaliq_workflow validate experiments/my-workflow.yml --schema custom-schema.json\n</code></pre>"},{"location":"api/cli/#cicd-integration","title":"CI/CD Integration","text":""},{"location":"api/cli/#github-actions-integration","title":"GitHub Actions Integration","text":"<pre><code>name: Execute Causal Discovery Workflow\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  validate:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v3\n    - uses: actions/setup-python@v4\n      with:\n        python-version: '3.9'\n\n    - name: Install dependencies\n      run: |\n        pip install causaliq-workflow\n\n    - name: Validate workflows\n      run: |\n        python -m causaliq_workflow validate workflows/\n\n  execute:\n    needs: validate\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        workflow: [\n          \"workflows/causal-discovery.yml\",\n          \"workflows/model-validation.yml\"\n        ]\n    steps:\n    - uses: actions/checkout@v3\n    - uses: actions/setup-python@v4\n      with:\n        python-version: '3.9'\n\n    - name: Install dependencies\n      run: |\n        pip install causaliq-workflow\n\n    - name: Execute workflow\n      run: |\n        python -m causaliq_workflow ${{ matrix.workflow }} --output-dir results/\n\n    - name: Upload results\n      uses: actions/upload-artifact@v3\n      with:\n        name: workflow-results\n        path: results/\n</code></pre>"},{"location":"api/cli/#jenkins-pipeline","title":"Jenkins Pipeline","text":"<pre><code>pipeline {\n    agent any\n\n    environment {\n        PYTHONPATH = \"${WORKSPACE}\"\n    }\n\n    stages {\n        stage('Setup') {\n            steps {\n                sh 'pip install causaliq-workflow'\n            }\n        }\n\n        stage('Validate') {\n            steps {\n                sh 'python -m causaliq_workflow validate workflows/'\n            }\n        }\n\n        stage('Execute Workflows') {\n            parallel {\n                stage('Causal Discovery') {\n                    steps {\n                        sh '''\n                            python -m causaliq_workflow workflows/causal-discovery.yml \\\\\n                                --output-dir results/causal-discovery/\n                        '''\n                    }\n                }\n                stage('Model Validation') {\n                    steps {\n                        sh '''\n                            python -m causaliq_workflow workflows/model-validation.yml \\\\\n                                --output-dir results/model-validation/\n                        '''\n                    }\n                }\n            }\n        }\n\n        stage('Archive Results') {\n            steps {\n                archiveArtifacts artifacts: 'results/**/*', fingerprint: true\n            }\n        }\n    }\n\n    post {\n        always {\n            cleanWs()\n        }\n    }\n}\n</code></pre>"},{"location":"api/cli/#advanced-usage-patterns","title":"Advanced Usage Patterns","text":""},{"location":"api/cli/#parameterized-execution","title":"Parameterized Execution","text":"<pre><code># Execute workflow with environment variable substitution\nexport DATASET_PATH=\"/data/experiments/asia.csv\"\nexport ALPHA_VALUE=\"0.05\"\npython -m causaliq_workflow experiments/parameterized-workflow.yml\n\n# Execute with explicit parameter overrides\npython -m causaliq_workflow experiments/workflow.yml \\\\\n    --set dataset_path=/data/custom.csv \\\\\n    --set alpha=0.01 \\\\\n    --set output_format=graphml\n</code></pre>"},{"location":"api/cli/#batch-processing","title":"Batch Processing","text":"<pre><code># Execute multiple workflows sequentially\nfor workflow in workflows/*.yml; do\n    echo \"Executing $workflow...\"\n    python -m causaliq_workflow \"$workflow\" --output-dir \"results/$(basename $workflow .yml)/\"\ndone\n\n# Parallel execution using GNU parallel\nfind workflows/ -name \"*.yml\" | parallel python -m causaliq_workflow {} --output-dir \"results/{/.}/\"\n</code></pre>"},{"location":"api/cli/#matrix-job-distribution","title":"Matrix Job Distribution","text":"<pre><code># Show total number of matrix jobs\npython -m causaliq_workflow experiments/large-matrix.yml --count-jobs\n\n# Execute jobs in parallel across multiple machines\n# Machine 1: jobs 0-9\npython -m causaliq_workflow experiments/large-matrix.yml --matrix-range 0-9\n\n# Machine 2: jobs 10-19  \npython -m causaliq_workflow experiments/large-matrix.yml --matrix-range 10-19\n\n# Machine 3: jobs 20-29\npython -m causaliq_workflow experiments/large-matrix.yml --matrix-range 20-29\n</code></pre>"},{"location":"api/cli/#error-handling-and-debugging","title":"Error Handling and Debugging","text":""},{"location":"api/cli/#verbose-output","title":"Verbose Output","text":"<pre><code># Enable debug logging\npython -m causaliq_workflow experiments/workflow.yml --verbose --log-level DEBUG\n\n# Output structured logs in JSON format\npython -m causaliq_workflow experiments/workflow.yml --log-format json\n\n# Save logs to file\npython -m causaliq_workflow experiments/workflow.yml --log-file execution.log\n</code></pre>"},{"location":"api/cli/#error-recovery","title":"Error Recovery","text":"<pre><code># Continue execution on action failures\npython -m causaliq_workflow experiments/workflow.yml --continue-on-error\n\n# Retry failed actions\npython -m causaliq_workflow experiments/workflow.yml --retry-count 3 --retry-delay 5\n\n# Skip specific steps\npython -m causaliq_workflow experiments/workflow.yml --skip-steps \"step1,step3\"\n</code></pre>"},{"location":"api/cli/#configuration","title":"Configuration","text":""},{"location":"api/cli/#configuration-file","title":"Configuration File","text":"<p>Create <code>~/.causaliq-workflow.toml</code>:</p> <pre><code>[execution]\ndefault_output_dir = \"/experiments/results\"\ncontinue_on_error = false\nretry_count = 1\nretry_delay = 2\n\n[logging]\nlevel = \"INFO\"\nformat = \"structured\"\nfile = \"~/.causaliq-workflow.log\"\n\n[registry]\npackages = [\n    \"causaliq_workflow.actions\",\n    \"my_custom_actions\"\n]\n\n[validation]\nstrict_mode = true\nschema_path = \"~/.causaliq-workflow-schema.json\"\n</code></pre>"},{"location":"api/cli/#environment-variables","title":"Environment Variables","text":"<pre><code># Configure via environment\nexport CAUSALIQ_OUTPUT_DIR=\"/experiments/results\"\nexport CAUSALIQ_LOG_LEVEL=\"DEBUG\"\nexport CAUSALIQ_RETRY_COUNT=\"3\"\nexport CAUSALIQ_CONTINUE_ON_ERROR=\"true\"\n\npython -m causaliq_workflow experiments/workflow.yml\n</code></pre> <p>\u2190 Previous: Schema Validation | Back to API Overview | Next: Examples \u2192</p>"},{"location":"api/examples/","title":"Usage Examples","text":"<p>Comprehensive examples demonstrating common patterns and advanced usage of the CausalIQ Workflow framework.</p>"},{"location":"api/examples/#basic-workflows","title":"Basic Workflows","text":""},{"location":"api/examples/#simple-action-execution","title":"Simple Action Execution","text":"<pre><code>from causaliq_workflow import WorkflowExecutor\n\n# Create and execute a basic workflow\nexecutor = WorkflowExecutor()\n\n# Define workflow programmatically\nsimple_workflow = {\n    \"id\": \"simple-experiment\",\n    \"description\": \"Basic structure learning experiment\",\n    \"steps\": [\n        {\n            \"name\": \"Learn Structure\", \n            \"uses\": \"structure-learner\",\n            \"with\": {\n                \"data_path\": \"/data/asia.csv\",\n                \"output_path\": \"/results/asia_structure.graphml\",\n                \"algorithm\": \"pc\",\n                \"alpha\": 0.05\n            }\n        }\n    ]\n}\n\n# Execute workflow\nresults = executor.execute_workflow(simple_workflow, mode=\"run\")\nprint(f\"Workflow completed: {results}\")\n</code></pre>"},{"location":"api/examples/#loading-from-yaml","title":"Loading from YAML","text":"<pre><code># experiments/basic-experiment.yml\nid: \"basic-experiment\"\ndescription: \"Structure learning with PC algorithm\"\ndata_root: \"/experiments/data\"\noutput_root: \"/experiments/results\"\n\nsteps:\n  - name: \"PC Structure Learning\"\n    uses: \"pc-learner\"\n    with:\n      data_path: \"{{data_root}}/asia.csv\"\n      output_path: \"{{output_root}}/{{id}}/structure.graphml\"\n      alpha: 0.05\n      max_depth: 3\n</code></pre> <pre><code># Execute YAML workflow\nworkflow = executor.parse_workflow(\"experiments/basic-experiment.yml\")\nresults = executor.execute_workflow(workflow, mode=\"run\")\n</code></pre>"},{"location":"api/examples/#matrix-workflows","title":"Matrix Workflows","text":""},{"location":"api/examples/#parameter-sweeps","title":"Parameter Sweeps","text":"<pre><code># experiments/parameter-sweep.yml\nid: \"parameter-sweep\"\ndescription: \"Multi-algorithm parameter sweep\"\ndata_root: \"/experiments/data\"\noutput_root: \"/experiments/results\"\n\nmatrix:\n  dataset: [\"asia\", \"cancer\", \"earthquake\"]\n  algorithm: [\"pc\", \"ges\", \"lingam\"]\n  alpha: [0.01, 0.05, 0.1]\n\nsteps:\n  - name: \"Structure Learning\"\n    uses: \"structure-learner\"\n    with:\n      data_path: \"{{data_root}}/{{dataset}}.csv\"\n      output_path: \"{{output_root}}/{{id}}/{{dataset}}_{{algorithm}}_{{alpha}}.graphml\"\n      algorithm: \"{{algorithm}}\"\n      alpha: \"{{alpha}}\"\n\n  - name: \"Validate Structure\"\n    uses: \"structure-validator\"\n    with:\n      structure_path: \"{{output_root}}/{{id}}/{{dataset}}_{{algorithm}}_{{alpha}}.graphml\"\n      metrics_path: \"{{output_root}}/{{id}}/{{dataset}}_{{algorithm}}_{{alpha}}_metrics.json\"\n</code></pre> <pre><code># Execute matrix workflow\nworkflow = executor.parse_workflow(\"experiments/parameter-sweep.yml\")\n\n# Show matrix expansion\nif \"matrix\" in workflow:\n    jobs = executor.expand_matrix(workflow[\"matrix\"])\n    print(f\"Generated {len(jobs)} parameter combinations\")\n\n    # Preview first few jobs\n    for i, job in enumerate(jobs[:3]):\n        print(f\"Job {i}: {job}\")\n\n# Execute all combinations\nresults = executor.execute_workflow(workflow, mode=\"run\")\n</code></pre>"},{"location":"api/examples/#conditional-matrix","title":"Conditional Matrix","text":"<pre><code># experiments/conditional-matrix.yml\nid: \"conditional-matrix\"\ndescription: \"Matrix with conditional parameters\"\n\nmatrix:\n  dataset: [\"small_data\", \"medium_data\", \"large_data\"]\n  algorithm: [\"pc\", \"ges\"]\n  include:\n    - dataset: \"small_data\"\n      algorithm: \"pc\"\n      alpha: 0.05\n      max_iter: 1000\n    - dataset: \"medium_data\" \n      algorithm: \"pc\"\n      alpha: 0.01\n      max_iter: 5000\n    - dataset: \"large_data\"\n      algorithm: \"ges\" \n      regularization: 0.1\n      max_iter: 10000\n\nsteps:\n  - name: \"Adaptive Structure Learning\"\n    uses: \"adaptive-learner\"\n    with:\n      data_path: \"/data/{{dataset}}.csv\"\n      algorithm: \"{{algorithm}}\"\n      alpha: \"{{alpha|default(0.05)}}\"\n      max_iter: \"{{max_iter|default(1000)}}\"\n      regularization: \"{{regularization|default(0.0)}}\"\n</code></pre>"},{"location":"api/examples/#multi-step-workflows","title":"Multi-Step Workflows","text":""},{"location":"api/examples/#data-processing-pipeline","title":"Data Processing Pipeline","text":"<pre><code># experiments/data-pipeline.yml\nid: \"data-processing-pipeline\"\ndescription: \"Complete data processing and analysis pipeline\"\n\nmatrix:\n  dataset: [\"healthcare\", \"finance\", \"biology\"]\n  preprocessing: [\"standard\", \"robust\"]\n\nsteps:\n  - name: \"Data Preprocessing\"\n    uses: \"data-preprocessor\"\n    with:\n      input_path: \"/raw_data/{{dataset}}.csv\"\n      output_path: \"/processed_data/{{dataset}}_{{preprocessing}}.csv\"\n      method: \"{{preprocessing}}\"\n      remove_outliers: true\n      normalize: true\n\n  - name: \"Feature Selection\"\n    uses: \"feature-selector\"\n    with:\n      input_path: \"/processed_data/{{dataset}}_{{preprocessing}}.csv\"\n      output_path: \"/features/{{dataset}}_{{preprocessing}}_selected.csv\"\n      method: \"mutual_info\"\n      max_features: 50\n\n  - name: \"Structure Learning\"\n    uses: \"structure-learner\"\n    with:\n      data_path: \"/features/{{dataset}}_{{preprocessing}}_selected.csv\"\n      output_path: \"/structures/{{dataset}}_{{preprocessing}}_structure.graphml\"\n      algorithm: \"pc\"\n\n  - name: \"Model Validation\"\n    uses: \"model-validator\"\n    with:\n      structure_path: \"/structures/{{dataset}}_{{preprocessing}}_structure.graphml\"\n      data_path: \"/features/{{dataset}}_{{preprocessing}}_selected.csv\"\n      output_path: \"/validation/{{dataset}}_{{preprocessing}}_results.json\"\n      k_fold: 5\n\n  - name: \"Generate Report\"\n    uses: \"report-generator\"\n    with:\n      structure_path: \"/structures/{{dataset}}_{{preprocessing}}_structure.graphml\"\n      validation_path: \"/validation/{{dataset}}_{{preprocessing}}_results.json\"\n      report_path: \"/reports/{{dataset}}_{{preprocessing}}_report.html\"\n</code></pre>"},{"location":"api/examples/#custom-actions","title":"Custom Actions","text":""},{"location":"api/examples/#creating-domain-specific-actions","title":"Creating Domain-Specific Actions","text":"<pre><code># custom_actions/causal_discovery.py\nfrom causaliq_workflow.action import Action, ActionExecutionError\nfrom typing import Any, Dict\nimport pandas as pd\nimport networkx as nx\n\nclass PCAlgorithmAction(Action):\n    \"\"\"PC algorithm for causal structure learning.\"\"\"\n\n    name = \"pc-algorithm\"\n    version = \"2.1.0\"\n    description = \"Peter-Clark algorithm for causal discovery\"\n\n    def run(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Execute PC algorithm.\"\"\"\n        try:\n            # Load data\n            data_path = inputs[\"data_path\"]\n            data = pd.read_csv(data_path)\n\n            # PC algorithm parameters\n            alpha = inputs.get(\"alpha\", 0.05)\n            max_depth = inputs.get(\"max_depth\", 3)\n\n            # Run PC algorithm (simplified example)\n            graph = self._run_pc_algorithm(data, alpha, max_depth)\n\n            # Save results\n            output_path = inputs[\"output_path\"]\n            nx.write_graphml(graph, output_path)\n\n            return {\n                \"structure_path\": output_path,\n                \"node_count\": graph.number_of_nodes(),\n                \"edge_count\": graph.number_of_edges(),\n                \"alpha_used\": alpha,\n                \"max_depth_used\": max_depth\n            }\n\n        except Exception as e:\n            raise ActionExecutionError(f\"PC algorithm failed: {e}\") from e\n\n    def _run_pc_algorithm(self, data, alpha, max_depth):\n        \"\"\"Simplified PC algorithm implementation.\"\"\"\n        # This would contain the actual PC algorithm logic\n        # For demonstration, create a simple graph\n        G = nx.DiGraph()\n        G.add_nodes_from(data.columns)\n        # Add some edges based on correlations (simplified)\n        return G\n\nclass NetworkAnalysisAction(Action):\n    \"\"\"Network analysis and metrics calculation.\"\"\"\n\n    name = \"network-analysis\" \n    version = \"1.3.0\"\n    description = \"Compute network topology metrics\"\n\n    def run(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Analyze network structure.\"\"\"\n        try:\n            # Load structure\n            structure_path = inputs[\"structure_path\"]\n            graph = nx.read_graphml(structure_path)\n\n            # Compute metrics\n            metrics = {\n                \"node_count\": graph.number_of_nodes(),\n                \"edge_count\": graph.number_of_edges(),\n                \"density\": nx.density(graph),\n                \"transitivity\": nx.transitivity(graph),\n                \"average_clustering\": nx.average_clustering(graph),\n            }\n\n            # Add centrality measures\n            centrality = nx.degree_centrality(graph)\n            metrics[\"max_centrality\"] = max(centrality.values())\n            metrics[\"avg_centrality\"] = sum(centrality.values()) / len(centrality)\n\n            # Save metrics\n            output_path = inputs[\"output_path\"]\n            import json\n            with open(output_path, \"w\") as f:\n                json.dump(metrics, f, indent=2)\n\n            return metrics\n\n        except Exception as e:\n            raise ActionExecutionError(f\"Network analysis failed: {e}\") from e\n</code></pre>"},{"location":"api/examples/#using-custom-actions","title":"Using Custom Actions","text":"<pre><code># experiments/custom-workflow.yml\nid: \"custom-causal-discovery\"\ndescription: \"Workflow using custom actions\"\n\nmatrix:\n  dataset: [\"asia\", \"cancer\"]\n  alpha: [0.01, 0.05]\n\nsteps:\n  - name: \"PC Structure Learning\"\n    uses: \"pc-algorithm\"\n    with:\n      data_path: \"/data/{{dataset}}.csv\"\n      output_path: \"/results/{{dataset}}_{{alpha}}_structure.graphml\"\n      alpha: \"{{alpha}}\"\n      max_depth: 3\n\n  - name: \"Network Analysis\"\n    uses: \"network-analysis\"\n    with:\n      structure_path: \"/results/{{dataset}}_{{alpha}}_structure.graphml\"\n      output_path: \"/results/{{dataset}}_{{alpha}}_metrics.json\"\n</code></pre>"},{"location":"api/examples/#error-handling-and-validation","title":"Error Handling and Validation","text":""},{"location":"api/examples/#robust-workflow-execution","title":"Robust Workflow Execution","text":"<pre><code>from causaliq_workflow import WorkflowExecutor, WorkflowExecutionError\nfrom causaliq_workflow.schema import WorkflowValidationError\nimport logging\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef execute_workflow_safely(workflow_path: str) -&gt; bool:\n    \"\"\"Execute workflow with comprehensive error handling.\"\"\"\n    executor = WorkflowExecutor()\n\n    try:\n        # Parse and validate\n        logger.info(f\"Loading workflow: {workflow_path}\")\n        workflow = executor.parse_workflow(workflow_path)\n        logger.info(f\"Workflow loaded: {workflow['id']}\")\n\n        # Execute with error handling\n        results = executor.execute_workflow(workflow, mode=\"run\")\n        logger.info(\"Workflow completed successfully\")\n\n        return True\n\n    except FileNotFoundError:\n        logger.error(f\"Workflow file not found: {workflow_path}\")\n        return False\n\n    except WorkflowValidationError as e:\n        logger.error(f\"Workflow validation failed: {e}\")\n        logger.error(f\"Schema path: {e.schema_path}\")\n        return False\n\n    except WorkflowExecutionError as e:\n        logger.error(f\"Workflow execution failed: {e}\")\n        # Check for specific error types\n        if \"Template\" in str(e):\n            logger.error(\"Template variable issue - check matrix and step parameters\")\n        elif \"Action\" in str(e):\n            logger.error(\"Action execution issue - check action inputs and availability\")\n        return False\n\n    except Exception as e:\n        logger.error(f\"Unexpected error: {e}\")\n        return False\n\n# Execute multiple workflows with error recovery\nworkflow_files = [\n    \"experiments/experiment-1.yml\",\n    \"experiments/experiment-2.yml\", \n    \"experiments/experiment-3.yml\"\n]\n\nsuccessful_workflows = []\nfailed_workflows = []\n\nfor workflow_file in workflow_files:\n    if execute_workflow_safely(workflow_file):\n        successful_workflows.append(workflow_file)\n    else:\n        failed_workflows.append(workflow_file)\n\nprint(f\"Successful: {len(successful_workflows)}\")\nprint(f\"Failed: {len(failed_workflows)}\")\n</code></pre>"},{"location":"api/examples/#integration-examples","title":"Integration Examples","text":""},{"location":"api/examples/#jupyter-notebook-integration","title":"Jupyter Notebook Integration","text":"<pre><code># notebook_integration.py\nfrom causaliq_workflow import WorkflowExecutor\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport json\n\nclass NotebookWorkflowRunner:\n    \"\"\"Workflow execution optimized for Jupyter notebooks.\"\"\"\n\n    def __init__(self):\n        self.executor = WorkflowExecutor()\n        self.results = {}\n\n    def run_experiment(self, workflow_path, display_results=True):\n        \"\"\"Run workflow and display results inline.\"\"\"\n        workflow = self.executor.parse_workflow(workflow_path)\n\n        print(f\"\ud83d\udd04 Executing: {workflow['id']}\")\n        print(f\"\ud83d\udccb Description: {workflow.get('description', 'No description')}\")\n\n        # Show matrix expansion if present\n        if \"matrix\" in workflow:\n            jobs = self.executor.expand_matrix(workflow[\"matrix\"])\n            print(f\"\ud83d\udd22 Matrix jobs: {len(jobs)}\")\n\n        # Execute\n        results = self.executor.execute_workflow(workflow, mode=\"run\")\n        self.results[workflow['id']] = results\n\n        if display_results:\n            self.display_results(workflow['id'])\n\n        return results\n\n    def display_results(self, workflow_id):\n        \"\"\"Display workflow results with visualizations.\"\"\"\n        results = self.results.get(workflow_id)\n        if not results:\n            print(\"No results available\")\n            return\n\n        # Display summary\n        print(\"\\\\n\ud83d\udcca Results Summary:\")\n        for step_result in results:\n            if isinstance(step_result, dict) and \"structure_path\" in step_result:\n                self.visualize_structure(step_result[\"structure_path\"])\n\n    def visualize_structure(self, structure_path):\n        \"\"\"Visualize learned causal structure.\"\"\"\n        try:\n            graph = nx.read_graphml(structure_path)\n\n            plt.figure(figsize=(10, 8))\n            pos = nx.spring_layout(graph)\n            nx.draw(graph, pos, with_labels=True, node_color='lightblue', \n                   node_size=1000, font_size=10, arrows=True)\n            plt.title(f\"Causal Structure ({graph.number_of_nodes()} nodes, {graph.number_of_edges()} edges)\")\n            plt.show()\n\n        except Exception as e:\n            print(f\"Visualization failed: {e}\")\n\n# Usage in Jupyter notebook\nrunner = NotebookWorkflowRunner()\n\n# Run experiments\nrunner.run_experiment(\"experiments/pc-analysis.yml\")\nrunner.run_experiment(\"experiments/ges-analysis.yml\")\n\n# Compare results\nprint(\"\\\\n\ud83d\udcc8 Experiment Comparison:\")\nfor workflow_id, results in runner.results.items():\n    print(f\"{workflow_id}: {len(results)} steps completed\")\n</code></pre>"},{"location":"api/examples/#docker-integration","title":"Docker Integration","text":"<pre><code># Dockerfile\nFROM python:3.9-slim\n\nWORKDIR /app\n\n# Install dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy workflow framework\nCOPY causaliq_workflow/ ./causaliq_workflow/\nCOPY experiments/ ./experiments/\nCOPY data/ ./data/\n\n# Create output directory\nRUN mkdir -p /app/results\n\n# Default command\nCMD [\"python\", \"-m\", \"causaliq_workflow\", \"experiments/default-experiment.yml\", \"--output-dir\", \"/app/results\"]\n</code></pre> <pre><code># Build and run\ndocker build -t causal-workflow .\n\n# Run specific experiment\ndocker run -v $(pwd)/results:/app/results causal-workflow \\\\\n    python -m causaliq_workflow experiments/large-scale-analysis.yml\n\n# Run with custom data\ndocker run -v $(pwd)/data:/app/custom_data -v $(pwd)/results:/app/results causal-workflow \\\\\n    python -m causaliq_workflow experiments/custom-data-experiment.yml\n</code></pre>"},{"location":"api/examples/#performance-optimization","title":"Performance Optimization","text":""},{"location":"api/examples/#parallel-matrix-execution","title":"Parallel Matrix Execution","text":"<pre><code>from causaliq_workflow import WorkflowExecutor\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\nimport multiprocessing as mp\n\ndef execute_matrix_job(workflow_data, job_index, job_params):\n    \"\"\"Execute single matrix job in separate process.\"\"\"\n    try:\n        executor = WorkflowExecutor()\n\n        # Create job-specific workflow\n        job_workflow = workflow_data.copy()\n        # Apply matrix parameters to workflow\n        # (implementation would substitute template variables)\n\n        results = executor.execute_workflow(job_workflow, mode=\"run\")\n        return job_index, results, None\n\n    except Exception as e:\n        return job_index, None, str(e)\n\ndef execute_workflow_parallel(workflow_path, max_workers=None):\n    \"\"\"Execute matrix workflow with parallel job execution.\"\"\"\n    executor = WorkflowExecutor()\n    workflow = executor.parse_workflow(workflow_path)\n\n    if \"matrix\" not in workflow:\n        # No matrix, execute normally\n        return executor.execute_workflow(workflow, mode=\"run\")\n\n    # Expand matrix\n    jobs = executor.expand_matrix(workflow[\"matrix\"])\n\n    if max_workers is None:\n        max_workers = min(len(jobs), mp.cpu_count())\n\n    print(f\"Executing {len(jobs)} matrix jobs with {max_workers} workers\")\n\n    results = {}\n    errors = {}\n\n    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n        # Submit all jobs\n        future_to_job = {\n            executor.submit(execute_matrix_job, workflow, i, job): i \n            for i, job in enumerate(jobs)\n        }\n\n        # Collect results\n        for future in as_completed(future_to_job):\n            job_index = future_to_job[future]\n            job_index_result, result, error = future.result()\n\n            if error:\n                errors[job_index] = error\n                print(f\"\u274c Job {job_index} failed: {error}\")\n            else:\n                results[job_index] = result\n                print(f\"\u2705 Job {job_index} completed\")\n\n    print(f\"\\\\nCompleted: {len(results)}/{len(jobs)} jobs\")\n    if errors:\n        print(f\"Errors: {len(errors)} jobs failed\")\n\n    return results, errors\n\n# Usage\nresults, errors = execute_workflow_parallel(\"experiments/large-matrix.yml\", max_workers=8)\n</code></pre> <p>\u2190 Previous: CLI Interface | Back to API Overview</p>"},{"location":"api/logging/","title":"Logging System API","text":"<p>The Logging System provides centralized logging infrastructure with multiple output destinations for workflow execution monitoring, debugging, and audit trails. It supports configurable verbosity levels and flexible output routing.</p>"},{"location":"api/logging/#workflowlogger-class","title":"WorkflowLogger Class","text":"<p>The <code>WorkflowLogger</code> class provides centralized logging with support for multiple output destinations including terminal output, file logging, and test-capturable output.</p>"},{"location":"api/logging/#causaliq_workflow.logger.WorkflowLogger","title":"WorkflowLogger","text":"<pre><code>WorkflowLogger(\n    terminal: bool = True,\n    log_file: Optional[Path] = None,\n    log_level: LogLevel = SUMMARY,\n)\n</code></pre> <p>Centralized logging with multiple output destinations.</p> <p>Provides standardized logging infrastructure supporting file output, terminal output, and test-capturable output for workflow execution monitoring and debugging.</p> <p>This is the core logging structure without task-specific logic. The log_task method and progress functionality will be added in subsequent commits.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>__enter__</code>             \u2013              <p>Context manager entry.</p> </li> <li> <code>__exit__</code>             \u2013              <p>Context manager exit with cleanup.</p> </li> <li> <code>close</code>             \u2013              <p>Close file streams and cleanup resources.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>has_output_destinations</code>               (<code>bool</code>)           \u2013            <p>Return True if any output destination is configured.</p> </li> <li> <code>is_file_logging</code>               (<code>bool</code>)           \u2013            <p>Return True if file logging is enabled.</p> </li> <li> <code>is_terminal_logging</code>               (<code>bool</code>)           \u2013            <p>Return True if terminal logging is enabled.</p> </li> </ul>"},{"location":"api/logging/#causaliq_workflow.logger.WorkflowLogger(terminal)","title":"<code>terminal</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Enable terminal output (default: True)</p>"},{"location":"api/logging/#causaliq_workflow.logger.WorkflowLogger(log_file)","title":"<code>log_file</code>","text":"(<code>Optional[Path]</code>, default:                   <code>None</code> )           \u2013            <p>Optional file path for log output</p>"},{"location":"api/logging/#causaliq_workflow.logger.WorkflowLogger(log_level)","title":"<code>log_level</code>","text":"(<code>LogLevel</code>, default:                   <code>SUMMARY</code> )           \u2013            <p>Logging verbosity level (default: SUMMARY)</p>"},{"location":"api/logging/#causaliq_workflow.logger.WorkflowLogger.has_output_destinations","title":"has_output_destinations  <code>property</code>","text":"<pre><code>has_output_destinations: bool\n</code></pre> <p>Return True if any output destination is configured.</p>"},{"location":"api/logging/#causaliq_workflow.logger.WorkflowLogger.is_file_logging","title":"is_file_logging  <code>property</code>","text":"<pre><code>is_file_logging: bool\n</code></pre> <p>Return True if file logging is enabled.</p>"},{"location":"api/logging/#causaliq_workflow.logger.WorkflowLogger.is_terminal_logging","title":"is_terminal_logging  <code>property</code>","text":"<pre><code>is_terminal_logging: bool\n</code></pre> <p>Return True if terminal logging is enabled.</p>"},{"location":"api/logging/#causaliq_workflow.logger.WorkflowLogger.__enter__","title":"__enter__","text":"<pre><code>__enter__() -&gt; WorkflowLogger\n</code></pre> <p>Context manager entry.</p>"},{"location":"api/logging/#causaliq_workflow.logger.WorkflowLogger.__exit__","title":"__exit__","text":"<pre><code>__exit__(exc_type: Optional[type], exc_val: Optional[Exception], exc_tb: Any) -&gt; None\n</code></pre> <p>Context manager exit with cleanup.</p>"},{"location":"api/logging/#causaliq_workflow.logger.WorkflowLogger.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close file streams and cleanup resources.</p>"},{"location":"api/logging/#key-features","title":"Key Features","text":""},{"location":"api/logging/#multi-destination-output","title":"Multi-Destination Output","text":"<ul> <li>Terminal logging - Real-time output to stdout for interactive monitoring</li> <li>File logging - Persistent log files with automatic directory creation</li> <li>Test capture - Structured output that can be captured and verified in tests</li> </ul>"},{"location":"api/logging/#resource-management","title":"Resource Management","text":"<ul> <li>Context manager support - Automatic cleanup with <code>with</code> statements</li> <li>File stream handling - Proper opening, writing, and closing of log files</li> <li>Error handling - Graceful handling of file system errors</li> </ul>"},{"location":"api/logging/#flexible-configuration","title":"Flexible Configuration","text":"<ul> <li>Verbosity control - Configure logging level at initialization</li> <li>Output selection - Enable/disable terminal and file output independently</li> <li>Append mode - Log files opened in append mode for multiple workflow runs</li> </ul>"},{"location":"api/logging/#loglevel-enum","title":"LogLevel Enum","text":"<p>The <code>LogLevel</code> enum defines verbosity levels for controlling the amount of logging output during workflow execution.</p>"},{"location":"api/logging/#causaliq_workflow.logger.LogLevel","title":"LogLevel","text":"<p>Logging verbosity levels for workflow execution.</p> <p>Attributes:</p> <ul> <li> <code>ALL</code>           \u2013            <p>Comprehensive logging - all task details and intermediate steps.</p> </li> <li> <code>NONE</code>           \u2013            <p>No logging output - silent execution.</p> </li> <li> <code>SUMMARY</code>           \u2013            <p>Summary-level logging - key status and final results only.</p> </li> </ul>"},{"location":"api/logging/#causaliq_workflow.logger.LogLevel.ALL","title":"ALL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ALL = 'all'\n</code></pre> <p>Comprehensive logging - all task details and intermediate steps.</p>"},{"location":"api/logging/#causaliq_workflow.logger.LogLevel.NONE","title":"NONE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>NONE = 'none'\n</code></pre> <p>No logging output - silent execution.</p>"},{"location":"api/logging/#causaliq_workflow.logger.LogLevel.SUMMARY","title":"SUMMARY  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SUMMARY = 'summary'\n</code></pre> <p>Summary-level logging - key status and final results only.</p>"},{"location":"api/logging/#verbosity-levels","title":"Verbosity Levels","text":""},{"location":"api/logging/#none","title":"NONE","text":"<p>Silent execution with no logging output. Useful for automated scripts where output needs to be minimal.</p>"},{"location":"api/logging/#summary","title":"SUMMARY","text":"<p>Summary-level logging showing key status information and final results only. Default level providing essential information without overwhelming detail.</p>"},{"location":"api/logging/#all","title":"ALL","text":"<p>Comprehensive logging including all task details and intermediate steps. Useful for debugging and detailed workflow analysis.</p>"},{"location":"api/logging/#usage-examples","title":"Usage Examples","text":""},{"location":"api/logging/#basic-logger-setup","title":"Basic Logger Setup","text":"<pre><code>from causaliq_workflow import WorkflowLogger, LogLevel\nfrom pathlib import Path\n\n# Terminal-only logging with default verbosity\nlogger = WorkflowLogger()\n\n# File-only logging with high verbosity\nlog_file = Path(\"workflow_execution.log\")\nlogger = WorkflowLogger(\n    terminal=False, \n    log_file=log_file, \n    log_level=LogLevel.ALL\n)\n\n# Both terminal and file logging\nlogger = WorkflowLogger(\n    terminal=True,\n    log_file=Path(\"logs/workflow.log\"),\n    log_level=LogLevel.SUMMARY\n)\n</code></pre>"},{"location":"api/logging/#context-manager-usage","title":"Context Manager Usage","text":"<pre><code>from pathlib import Path\n\n# Automatic resource cleanup\nwith WorkflowLogger(log_file=Path(\"execution.log\")) as logger:\n    # Logger will automatically close file streams\n    pass  # Log workflow execution here\n</code></pre>"},{"location":"api/logging/#configuration-examples","title":"Configuration Examples","text":"<pre><code># Silent execution for automated environments\nsilent_logger = WorkflowLogger(\n    terminal=False, \n    log_file=None, \n    log_level=LogLevel.NONE\n)\n\n# Development setup with verbose output\ndev_logger = WorkflowLogger(\n    terminal=True,\n    log_file=Path(\"debug.log\"),\n    log_level=LogLevel.ALL\n)\n\n# Production setup with summary logging\nprod_logger = WorkflowLogger(\n    terminal=True,\n    log_file=Path(\"/var/log/causaliq/workflow.log\"),\n    log_level=LogLevel.SUMMARY\n)\n</code></pre>"},{"location":"api/logging/#checking-logger-configuration","title":"Checking Logger Configuration","text":"<pre><code>logger = WorkflowLogger(\n    terminal=True, \n    log_file=Path(\"workflow.log\")\n)\n\n# Check configuration\nif logger.is_terminal_logging:\n    print(\"Terminal output enabled\")\n\nif logger.is_file_logging:\n    print(f\"File logging to: {logger.log_file}\")\n\nif logger.has_output_destinations:\n    print(\"Logger has at least one output destination\")\nelse:\n    print(\"Warning: No output destinations configured\")\n</code></pre>"},{"location":"api/logging/#integration-with-workflow-execution","title":"Integration with Workflow Execution","text":"<pre><code>from causaliq_workflow import WorkflowExecutor, WorkflowLogger, LogLevel\nfrom pathlib import Path\n\n# Configure logging for workflow execution\nlog_file = Path(\"experiments/workflow_run.log\")\n\nwith WorkflowLogger(\n    terminal=True, \n    log_file=log_file, \n    log_level=LogLevel.ALL\n) as logger:\n\n    # Future integration - logger will be passed to workflow executor\n    executor = WorkflowExecutor()\n    # workflow_results = executor.execute_with_logging(workflow, logger)\n</code></pre>"},{"location":"api/logging/#design-principles","title":"Design Principles","text":""},{"location":"api/logging/#separation-of-concerns","title":"Separation of Concerns","text":"<p>The logging system is designed as a separate module that can be integrated with various components without tight coupling.</p>"},{"location":"api/logging/#resource-safety","title":"Resource Safety","text":"<p>Proper resource management ensures file streams are always closed, even in error conditions, preventing resource leaks.</p>"},{"location":"api/logging/#test-friendly-design","title":"Test-Friendly Design","text":"<p>The logging infrastructure is designed to be easily mocked and tested, with clear interfaces for capturing output in automated tests.</p>"},{"location":"api/logging/#future-extensibility","title":"Future Extensibility","text":"<p>The core structure provides a foundation for adding task-specific logging functionality in subsequent development phases.</p>"},{"location":"api/logging/#integration-points","title":"Integration Points","text":""},{"location":"api/logging/#workflow-executor-integration","title":"Workflow Executor Integration","text":"<p>The WorkflowLogger is designed to integrate with the WorkflowExecutor for comprehensive workflow execution logging:</p> <pre><code># Future integration pattern\nexecutor = WorkflowExecutor(logger=logger)\nresults = executor.execute_workflow(workflow, mode=\"run\")\n</code></pre>"},{"location":"api/logging/#action-integration","title":"Action Integration","text":"<p>Actions will receive the logger as an optional parameter for task-level logging:</p> <pre><code># Future action integration\ndef run(self, inputs, mode=\"dry-run\", logger=None):\n    if logger:\n        # Log task execution details\n        pass\n    return results\n</code></pre>"},{"location":"api/logging/#cli-integration","title":"CLI Integration","text":"<p>The CLI will provide logging configuration options:</p> <pre><code># Future CLI integration\ncausaliq-workflow run workflow.yml --log-file=execution.log --log-level=all\n</code></pre>"},{"location":"api/logging/#error-handling","title":"Error Handling","text":""},{"location":"api/logging/#file-system-errors","title":"File System Errors","text":"<p>The logger handles common file system errors gracefully:</p> <ul> <li>Permission errors - Clear error messages for access denied scenarios</li> <li>Missing directories - Automatic creation of parent directories</li> <li>Disk space issues - Proper error reporting for write failures</li> </ul>"},{"location":"api/logging/#resource-cleanup","title":"Resource Cleanup","text":"<p>Context manager support ensures proper cleanup even when exceptions occur:</p> <pre><code>try:\n    with WorkflowLogger(log_file=Path(\"workflow.log\")) as logger:\n        # Workflow execution that might raise exceptions\n        pass\nexcept Exception as e:\n    # Logger file streams are automatically closed\n    print(f\"Workflow failed: {e}\")\n</code></pre> <p>Next: CLI Interface | Previous: Status System | Up: API Reference</p>"},{"location":"api/registry/","title":"Action Registry","text":"<p>The action registry provides centralized discovery, management, and execution of workflow actions with plugin architecture support.</p>"},{"location":"api/registry/#core-classes","title":"Core Classes","text":""},{"location":"api/registry/#causaliq_workflowregistry","title":"causaliq_workflow.registry","text":""},{"location":"api/registry/#causaliq_workflow.registry.ActionRegistry","title":"ActionRegistry","text":"<pre><code>ActionRegistry()\n</code></pre> <p>Registry for discovering and executing workflow actions dynamically.</p> <p>Uses import-time introspection to automatically discover actions when packages are imported. No configuration needed - just import the package and use 'uses: package-name' in workflows.</p> <p>Convention: Action packages should export an Action subclass named 'CausalIQAction' in their init.py file to avoid namespace collisions.</p> <p>Attributes:</p> <ul> <li> <code>_instance</code>               (<code>Optional[ActionRegistry]</code>)           \u2013            <p>Singleton instance of the ActionRegistry</p> </li> <li> <code>_actions</code>               (<code>Dict[str, Type[Action]]</code>)           \u2013            <p>Dictionary mapping action names to Action classes</p> </li> <li> <code>_discovery_errors</code>               (<code>List[str]</code>)           \u2013            <p>List of errors encountered during action discovery</p> </li> </ul> Initializes <p>_actions: Dictionary mapping action names to Action classes _discovery_errors: List to collect any discovery errors</p> <p>Methods:</p> <ul> <li> <code>get_available_actions</code>             \u2013              <p>Get dictionary of available action names to classes.</p> </li> <li> <code>get_action_class</code>             \u2013              <p>Get action class by name.</p> </li> <li> <code>has_action</code>             \u2013              <p>Check if action is available.</p> </li> <li> <code>execute_action</code>             \u2013              <p>Execute action with inputs and workflow context.</p> </li> <li> <code>list_actions_by_package</code>             \u2013              <p>Group actions by source package for documentation.</p> </li> </ul>"},{"location":"api/registry/#causaliq_workflow.registry.ActionRegistry.get_available_actions","title":"get_available_actions","text":"<pre><code>get_available_actions() -&gt; Dict[str, Type[Action]]\n</code></pre> <p>Get dictionary of available action names to classes.</p> <p>Returns:</p> <ul> <li> <code>Dict[str, Type[Action]]</code>           \u2013            <p>Dictionary mapping action names to Action classes</p> </li> </ul>"},{"location":"api/registry/#causaliq_workflow.registry.ActionRegistry.get_action_class","title":"get_action_class","text":"<pre><code>get_action_class(name: str) -&gt; Type[Action]\n</code></pre> <p>Get action class by name.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Type[Action]</code>           \u2013            <p>Action class</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ActionRegistryError</code>             \u2013            <p>If action not found</p> </li> </ul>"},{"location":"api/registry/#causaliq_workflow.registry.ActionRegistry.get_action_class(name)","title":"<code>name</code>","text":"(<code>str</code>)           \u2013            <p>Action name</p>"},{"location":"api/registry/#causaliq_workflow.registry.ActionRegistry.has_action","title":"has_action","text":"<pre><code>has_action(name: str) -&gt; bool\n</code></pre> <p>Check if action is available.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if action is available</p> </li> </ul>"},{"location":"api/registry/#causaliq_workflow.registry.ActionRegistry.has_action(name)","title":"<code>name</code>","text":"(<code>str</code>)           \u2013            <p>Action name to check</p>"},{"location":"api/registry/#causaliq_workflow.registry.ActionRegistry.execute_action","title":"execute_action","text":"<pre><code>execute_action(\n    name: str, inputs: Dict[str, Any], context: WorkflowContext\n) -&gt; Dict[str, Any]\n</code></pre> <p>Execute action with inputs and workflow context.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Dict[str, Any]</code>           \u2013            <p>Action outputs dictionary</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ActionRegistryError</code>             \u2013            <p>If action not found or execution fails</p> </li> </ul>"},{"location":"api/registry/#causaliq_workflow.registry.ActionRegistry.execute_action(name)","title":"<code>name</code>","text":"(<code>str</code>)           \u2013            <p>Action name</p>"},{"location":"api/registry/#causaliq_workflow.registry.ActionRegistry.execute_action(inputs)","title":"<code>inputs</code>","text":"(<code>Dict[str, Any]</code>)           \u2013            <p>Action input parameters</p>"},{"location":"api/registry/#causaliq_workflow.registry.ActionRegistry.execute_action(context)","title":"<code>context</code>","text":"(<code>WorkflowContext</code>)           \u2013            <p>Complete workflow context</p>"},{"location":"api/registry/#causaliq_workflow.registry.ActionRegistry.list_actions_by_package","title":"list_actions_by_package","text":"<pre><code>list_actions_by_package() -&gt; Dict[str, List[str]]\n</code></pre> <p>Group actions by source package for documentation.</p> <p>Returns:</p> <ul> <li> <code>Dict[str, List[str]]</code>           \u2013            <p>Dictionary mapping package names to action lists</p> </li> </ul>"},{"location":"api/registry/#causaliq_workflow.registry.WorkflowContext","title":"WorkflowContext  <code>dataclass</code>","text":"<pre><code>WorkflowContext(mode: str, matrix: Dict[str, List[Any]])\n</code></pre> <p>Workflow context for action execution optimization.</p> <p>Provides minimal context needed for actions to optimize across workflows. Actions receive specific data through inputs; context provides meta-information.</p> <p>Attributes:</p> <ul> <li> <code>mode</code>               (<code>str</code>)           \u2013            <p>Execution mode ('dry-run', 'run', 'compare')</p> </li> <li> <code>matrix</code>               (<code>Dict[str, List[Any]]</code>)           \u2013            <p>Complete matrix definition for cross-job optimization</p> </li> </ul>"},{"location":"api/registry/#exception-handling","title":"Exception Handling","text":""},{"location":"api/registry/#causaliq_workflow.registry.ActionRegistryError","title":"ActionRegistryError","text":"<p>Raised when action registry operations fail.</p> <p>This exception is raised when: - Requested action is not found in the registry - Action discovery fails during module scanning - Action validation fails - Other registry-related errors occur</p>"},{"location":"api/registry/#usage-examples","title":"Usage Examples","text":""},{"location":"api/registry/#basic-registry-operations","title":"Basic Registry Operations","text":"<pre><code>from causaliq_workflow.registry import ActionRegistry, ActionRegistryError\n\n# Create registry instance\nregistry = ActionRegistry()\n\n# List all available actions\nactions = registry.get_available_actions()\nfor action_name, action_class in actions.items():\n    print(f\"Action: {action_name} (v{action_class.version})\")\n\n# Check if specific action exists\nif registry.has_action(\"my-structure-learner\"):\n    action_class = registry.get_action_class(\"my-structure-learner\")\n    print(f\"Found action: {action_class.description}\")\n\n# Execute action directly through registry\ntry:\n    result = registry.execute_action(\n        \"my-structure-learner\",\n        {\"data_path\": \"/data/asia.csv\", \"alpha\": 0.05}\n    )\n    print(f\"Execution result: {result}\")\nexcept ActionRegistryError as e:\n    print(f\"Registry error: {e}\")\n</code></pre>"},{"location":"api/registry/#plugin-discovery","title":"Plugin Discovery","text":"<pre><code># List actions by package (useful for plugin systems)\nactions_by_package = registry.list_actions_by_package()\nfor package, actions in actions_by_package.items():\n    print(f\"Package: {package}\")\n    for action in actions:\n        print(f\"  - {action}\")\n\n# Discover actions from specific packages\nregistry = ActionRegistry(packages=[\"my_custom_actions\"])\ncustom_actions = registry.get_available_actions()\n</code></pre>"},{"location":"api/registry/#workflow-context","title":"Workflow Context","text":"<pre><code>from causaliq_workflow.registry import WorkflowContext\n\n# Create workflow context for action execution\ncontext = WorkflowContext(\n    mode=\"run\",\n    matrix={\"dataset\": [\"asia\", \"cancer\"], \"algorithm\": [\"pc\", \"ges\"]},\n)\n\n# Context provides execution metadata for action optimization\nprint(f\"Execution mode: {context.mode}\")\nprint(f\"Matrix definition: {context.matrix}\")\n\n# Actions can optimize across the full matrix space\nif len(context.matrix.get(\"dataset\", [])) &gt; 1:\n    print(\"Multi-dataset experiment - can pre-load data\")\n</code></pre>"},{"location":"api/registry/#architecture-notes","title":"Architecture Notes","text":"<p>The ActionRegistry uses Python's module discovery system to automatically find and register actions. Actions are discovered by:</p> <ol> <li>Package scanning - Searches specified packages for Action subclasses</li> <li>Automatic registration - Actions register themselves via class definition</li> <li>Name-based lookup - Actions identified by their <code>name</code> class attribute</li> <li>Version tracking - Support for action versioning and compatibility</li> </ol> <p>This design enables a flexible plugin architecture where actions can be distributed as separate packages and automatically discovered at runtime.</p> <p>\u2190 Previous: Actions | Back to API Overview | Next: Workflow Engine \u2192</p>"},{"location":"api/schema/","title":"Schema Validation","text":"<p>The schema validation system provides robust workflow validation against JSON schemas with detailed error reporting and custom schema support.</p>"},{"location":"api/schema/#core-functions","title":"Core Functions","text":""},{"location":"api/schema/#causaliq_workflowschema","title":"causaliq_workflow.schema","text":""},{"location":"api/schema/#causaliq_workflow.schema.validate_workflow","title":"validate_workflow","text":"<pre><code>validate_workflow(\n    workflow: Dict[str, Any], schema_path: Optional[Union[str, Path]] = None\n) -&gt; bool\n</code></pre> <p>Validate workflow against CausalIQ JSON Schema.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if workflow is valid</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>WorkflowValidationError</code>             \u2013            <p>If workflow validation fails</p> </li> </ul>"},{"location":"api/schema/#causaliq_workflow.schema.validate_workflow(workflow)","title":"<code>workflow</code>","text":"(<code>Dict[str, Any]</code>)           \u2013            <p>Workflow configuration dictionary</p>"},{"location":"api/schema/#causaliq_workflow.schema.validate_workflow(schema_path)","title":"<code>schema_path</code>","text":"(<code>Optional[Union[str, Path]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional path to custom schema file</p>"},{"location":"api/schema/#causaliq_workflow.schema.load_schema","title":"load_schema","text":"<pre><code>load_schema(schema_path: Optional[Union[str, Path]] = None) -&gt; Dict[str, Any]\n</code></pre> <p>Load CausalIQ workflow JSON Schema.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Dict[str, Any]</code>           \u2013            <p>Parsed JSON Schema dictionary</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>WorkflowValidationError</code>             \u2013            <p>If schema file cannot be loaded</p> </li> </ul>"},{"location":"api/schema/#causaliq_workflow.schema.load_schema(schema_path)","title":"<code>schema_path</code>","text":"(<code>Optional[Union[str, Path]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional path to custom schema file.         If None, loads default package schema.</p>"},{"location":"api/schema/#causaliq_workflow.schema.load_workflow_file","title":"load_workflow_file","text":"<pre><code>load_workflow_file(file_path: Union[str, Path]) -&gt; Dict[str, Any]\n</code></pre> <p>Load workflow from YAML file.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Dict[str, Any]</code>           \u2013            <p>Parsed workflow dictionary</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>WorkflowValidationError</code>             \u2013            <p>If file cannot be loaded</p> </li> </ul>"},{"location":"api/schema/#causaliq_workflow.schema.load_workflow_file(file_path)","title":"<code>file_path</code>","text":"(<code>Union[str, Path]</code>)           \u2013            <p>Path to workflow YAML file</p>"},{"location":"api/schema/#exception-handling","title":"Exception Handling","text":""},{"location":"api/schema/#causaliq_workflow.schema.WorkflowValidationError","title":"WorkflowValidationError","text":"<pre><code>WorkflowValidationError(message: str, schema_path: str = '')\n</code></pre> <p>Raised when workflow validation against JSON Schema fails.</p> <p>Parameters:</p>"},{"location":"api/schema/#causaliq_workflow.schema.WorkflowValidationError(message)","title":"<code>message</code>","text":"(<code>str</code>)           \u2013            <p>Validation error description</p>"},{"location":"api/schema/#causaliq_workflow.schema.WorkflowValidationError(schema_path)","title":"<code>schema_path</code>","text":"(<code>str</code>, default:                   <code>''</code> )           \u2013            <p>JSON Schema path where validation failed</p>"},{"location":"api/schema/#usage-examples","title":"Usage Examples","text":""},{"location":"api/schema/#basic-workflow-validation","title":"Basic Workflow Validation","text":"<pre><code>from causaliq_workflow.schema import validate_workflow, WorkflowValidationError\n\nworkflow_data = {\n    \"name\": \"My Experiment\",\n    \"id\": \"experiment-001\",\n    \"data_root\": \"/data\",\n    \"output_root\": \"/results\",\n    \"matrix\": {\n        \"dataset\": [\"asia\", \"cancer\"],\n        \"algorithm\": [\"pc\", \"ges\"],\n    },\n    \"steps\": [\n        {\n            \"name\": \"Structure Learning\",\n            \"uses\": \"my-custom-action\",\n            \"with\": {\n                \"alpha\": 0.05,\n            },\n        }\n    ],\n}\n\ntry:\n    result = validate_workflow(workflow_data)\n    print(\"Workflow validation passed!\")\n    print(f\"Validated workflow: {result['id']}\")\nexcept WorkflowValidationError as e:\n    print(f\"Validation failed: {e}\")\n    print(f\"Schema path: {e.schema_path}\")\n    if hasattr(e, 'validator'):\n        print(f\"Validation details: {e.validator}\")\n</code></pre>"},{"location":"api/schema/#loading-and-using-custom-schemas","title":"Loading and Using Custom Schemas","text":"<pre><code>from causaliq_workflow.schema import load_schema, validate_workflow\nfrom pathlib import Path\n\n# Load custom schema from file\nschema_path = Path(\"my-custom-schema.json\")\nschema = load_schema(schema_path)\n\n# Use custom schema with validation\ntry:\n    validate_workflow(workflow_data, schema)\n    print(\"Custom schema validation passed!\")\nexcept WorkflowValidationError as e:\n    print(f\"Custom validation failed: {e}\")\n</code></pre>"},{"location":"api/schema/#loading-workflow-files","title":"Loading Workflow Files","text":"<pre><code>from causaliq_workflow.schema import load_workflow_file\nfrom pathlib import Path\n\n# Load workflow from YAML or JSON file\nworkflow_path = Path(\"experiments/my-experiment.yml\")\nworkflow_data = load_workflow_file(workflow_path)\n\nprint(f\"Loaded workflow: {workflow_data['id']}\")\nprint(f\"Steps: {len(workflow_data.get('steps', []))}\")\n\n# File loading supports both YAML and JSON formats\njson_workflow = load_workflow_file(\"experiments/experiment.json\")\nyaml_workflow = load_workflow_file(\"experiments/experiment.yml\")\n</code></pre>"},{"location":"api/schema/#comprehensive-validation-pipeline","title":"Comprehensive Validation Pipeline","text":"<pre><code>from causaliq_workflow.schema import load_workflow_file, validate_workflow, WorkflowValidationError\nfrom pathlib import Path\n\ndef validate_workflow_file(file_path: Path) -&gt; dict:\n    \"\"\"Load and validate a workflow file with detailed error reporting.\"\"\"\n    try:\n        # Load workflow from file\n        workflow_data = load_workflow_file(file_path)\n        print(f\"\u2713 Loaded workflow from {file_path}\")\n\n        # Validate against schema\n        validated_workflow = validate_workflow(workflow_data)\n        print(f\"\u2713 Schema validation passed for workflow '{validated_workflow['id']}'\")\n\n        return validated_workflow\n\n    except FileNotFoundError:\n        print(f\"\u2717 Workflow file not found: {file_path}\")\n        raise\n    except WorkflowValidationError as e:\n        print(f\"\u2717 Schema validation failed:\")\n        print(f\"  Error: {e}\")\n        if hasattr(e, 'schema_path'):\n            print(f\"  Schema path: {e.schema_path}\")\n        raise\n    except Exception as e:\n        print(f\"\u2717 Unexpected error loading workflow: {e}\")\n        raise\n\n# Usage\nworkflow_files = [\n    Path(\"experiments/causal-discovery.yml\"),\n    Path(\"experiments/model-validation.json\"),\n    Path(\"experiments/parameter-sweep.yml\")\n]\n\nfor workflow_file in workflow_files:\n    try:\n        workflow = validate_workflow_file(workflow_file)\n        print(f\"Ready to execute: {workflow['id']}\\\\n\")\n    except Exception as e:\n        print(f\"Skipping invalid workflow: {workflow_file}\\\\n\")\n</code></pre>"},{"location":"api/schema/#schema-structure","title":"Schema Structure","text":"<p>The default workflow schema validates:</p>"},{"location":"api/schema/#required-fields","title":"Required Fields","text":"<ul> <li><code>id</code>: Unique workflow identifier</li> <li><code>steps</code>: Array of workflow steps</li> </ul>"},{"location":"api/schema/#optional-fields","title":"Optional Fields","text":"<ul> <li><code>name</code>: Human-readable workflow name</li> <li><code>description</code>: Workflow description</li> <li><code>matrix</code>: Parameter matrix for expansion</li> <li><code>data_root</code>: Base path for data files</li> <li><code>output_root</code>: Base path for output files</li> </ul>"},{"location":"api/schema/#step-schema","title":"Step Schema","text":"<p>Each step must include: - <code>uses</code>: Action identifier - <code>name</code> (optional): Human-readable step name - <code>with</code> (optional): Action parameters</p>"},{"location":"api/schema/#matrix-schema","title":"Matrix Schema","text":"<p>Matrix definitions support: - Simple arrays: <code>{\"param\": [\"value1\", \"value2\"]}</code> - Nested structures: Complex parameter combinations - Type validation: Ensures consistent parameter types</p>"},{"location":"api/schema/#error-reporting","title":"Error Reporting","text":"<p>WorkflowValidationError provides detailed information:</p> <pre><code>try:\n    validate_workflow(invalid_workflow)\nexcept WorkflowValidationError as e:\n    print(f\"Validation error: {e}\")\n    print(f\"Schema path: {e.schema_path}\")\n\n    # Access underlying jsonschema validation details\n    if hasattr(e, 'validator'):\n        print(f\"Failed constraint: {e.validator}\")\n        print(f\"Schema context: {e.schema_path}\")\n</code></pre>"},{"location":"api/schema/#custom-schema-development","title":"Custom Schema Development","text":"<pre><code># Example custom schema with additional constraints\ncustom_schema = {\n    \"type\": \"object\",\n    \"required\": [\"id\", \"version\", \"steps\"],\n    \"properties\": {\n        \"id\": {\"type\": \"string\", \"pattern\": \"^[a-z0-9-]+$\"},\n        \"version\": {\"type\": \"string\", \"pattern\": \"^\\\\d+\\\\.\\\\d+\\\\.\\\\d+$\"},\n        \"description\": {\"type\": \"string\", \"maxLength\": 200},\n        \"matrix\": {\n            \"type\": \"object\",\n            \"patternProperties\": {\n                \"^[a-z_]+$\": {\n                    \"type\": \"array\",\n                    \"minItems\": 1,\n                    \"items\": {\"type\": [\"string\", \"number\"]}\n                }\n            }\n        },\n        \"steps\": {\n            \"type\": \"array\",\n            \"minItems\": 1,\n            \"items\": {\n                \"type\": \"object\",\n                \"required\": [\"uses\"],\n                \"properties\": {\n                    \"name\": {\"type\": \"string\"},\n                    \"uses\": {\"type\": \"string\"},\n                    \"with\": {\"type\": \"object\"}\n                }\n            }\n        }\n    }\n}\n\n# Save and use custom schema\nimport json\nwith open(\"custom-workflow-schema.json\", \"w\") as f:\n    json.dump(custom_schema, f, indent=2)\n\ncustom_schema_obj = load_schema(\"custom-workflow-schema.json\")\nvalidate_workflow(workflow_data, custom_schema_obj)\n</code></pre> <p>\u2190 Previous: Workflow Engine | Back to API Overview | Next: CLI Interface \u2192</p>"},{"location":"api/status/","title":"Status System API","text":"<p>The Status System provides standardized task execution status reporting for workflow logging, monitoring, and debugging. It supports all execution modes (run, dry-run, compare) with comprehensive error categorization.</p>"},{"location":"api/status/#taskstatus-enum","title":"TaskStatus Enum","text":"<p>The <code>TaskStatus</code> enum defines all possible task execution outcomes with standardized string values and helpful categorization properties.</p>"},{"location":"api/status/#causaliq_workflow.status.TaskStatus","title":"TaskStatus","text":"<p>Enumeration of all possible task execution statuses.</p> <p>Provides standardized status reporting for workflow task execution, supporting different execution modes and comprehensive error categorization.</p> Status Categories <ul> <li>Execution: EXECUTES, WOULD_EXECUTE, SKIPS, WOULD_SKIP</li> <li>Comparison: IDENTICAL, DIFFERENT</li> <li>Errors: INVALID_USES, INVALID_PARAMETER, FAILED, TIMED_OUT</li> </ul> <p>Attributes:</p> <ul> <li> <code>DIFFERENT</code>           \u2013            <p>Task re-executed, outputs differ from previous run.</p> </li> <li> <code>EXECUTES</code>           \u2013            <p>Task executed successfully, output files created/updated.</p> </li> <li> <code>FAILED</code>           \u2013            <p>Task execution threw unexpected exception.</p> </li> <li> <code>IDENTICAL</code>           \u2013            <p>Task re-executed, outputs identical to previous run.</p> </li> <li> <code>INVALID_PARAMETER</code>           \u2013            <p>Parameters in with: block are invalid for action.</p> </li> <li> <code>INVALID_USES</code>           \u2013            <p>Action package specified in uses: not found.</p> </li> <li> <code>SKIPS</code>           \u2013            <p>Task skipped because output files exist and are current.</p> </li> <li> <code>TIMED_OUT</code>           \u2013            <p>Task exceeded configured timeout.</p> </li> <li> <code>WOULD_EXECUTE</code>           \u2013            <p>Task would execute successfully if run (dry-run mode).</p> </li> <li> <code>WOULD_SKIP</code>           \u2013            <p>Task would be skipped because output files exist (dry-run mode).</p> </li> <li> <code>is_dry_run</code>               (<code>bool</code>)           \u2013            <p>Return True if status is for dry-run mode.</p> </li> <li> <code>is_error</code>               (<code>bool</code>)           \u2013            <p>Return True if status indicates an error condition.</p> </li> <li> <code>is_execution</code>               (<code>bool</code>)           \u2013            <p>Return True if status indicates actual execution occurred.</p> </li> <li> <code>is_success</code>               (<code>bool</code>)           \u2013            <p>Return True if status indicates successful execution.</p> </li> </ul>"},{"location":"api/status/#causaliq_workflow.status.TaskStatus.DIFFERENT","title":"DIFFERENT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DIFFERENT = 'DIFFERENT'\n</code></pre> <p>Task re-executed, outputs differ from previous run.</p>"},{"location":"api/status/#causaliq_workflow.status.TaskStatus.EXECUTES","title":"EXECUTES  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>EXECUTES = 'EXECUTES'\n</code></pre> <p>Task executed successfully, output files created/updated.</p>"},{"location":"api/status/#causaliq_workflow.status.TaskStatus.FAILED","title":"FAILED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FAILED = 'FAILED'\n</code></pre> <p>Task execution threw unexpected exception.</p>"},{"location":"api/status/#causaliq_workflow.status.TaskStatus.IDENTICAL","title":"IDENTICAL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IDENTICAL = 'IDENTICAL'\n</code></pre> <p>Task re-executed, outputs identical to previous run.</p>"},{"location":"api/status/#causaliq_workflow.status.TaskStatus.INVALID_PARAMETER","title":"INVALID_PARAMETER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>INVALID_PARAMETER = 'INVALID_PARAMETER'\n</code></pre> <p>Parameters in with: block are invalid for action.</p>"},{"location":"api/status/#causaliq_workflow.status.TaskStatus.INVALID_USES","title":"INVALID_USES  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>INVALID_USES = 'INVALID_USES'\n</code></pre> <p>Action package specified in uses: not found.</p>"},{"location":"api/status/#causaliq_workflow.status.TaskStatus.SKIPS","title":"SKIPS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SKIPS = 'SKIPS'\n</code></pre> <p>Task skipped because output files exist and are current.</p>"},{"location":"api/status/#causaliq_workflow.status.TaskStatus.TIMED_OUT","title":"TIMED_OUT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TIMED_OUT = 'TIMED_OUT'\n</code></pre> <p>Task exceeded configured timeout.</p>"},{"location":"api/status/#causaliq_workflow.status.TaskStatus.WOULD_EXECUTE","title":"WOULD_EXECUTE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>WOULD_EXECUTE = 'WOULD_EXECUTE'\n</code></pre> <p>Task would execute successfully if run (dry-run mode).</p>"},{"location":"api/status/#causaliq_workflow.status.TaskStatus.WOULD_SKIP","title":"WOULD_SKIP  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>WOULD_SKIP = 'WOULD_SKIP'\n</code></pre> <p>Task would be skipped because output files exist (dry-run mode).</p>"},{"location":"api/status/#causaliq_workflow.status.TaskStatus.is_dry_run","title":"is_dry_run  <code>property</code>","text":"<pre><code>is_dry_run: bool\n</code></pre> <p>Return True if status is for dry-run mode.</p>"},{"location":"api/status/#causaliq_workflow.status.TaskStatus.is_error","title":"is_error  <code>property</code>","text":"<pre><code>is_error: bool\n</code></pre> <p>Return True if status indicates an error condition.</p>"},{"location":"api/status/#causaliq_workflow.status.TaskStatus.is_execution","title":"is_execution  <code>property</code>","text":"<pre><code>is_execution: bool\n</code></pre> <p>Return True if status indicates actual execution occurred.</p>"},{"location":"api/status/#causaliq_workflow.status.TaskStatus.is_success","title":"is_success  <code>property</code>","text":"<pre><code>is_success: bool\n</code></pre> <p>Return True if status indicates successful execution.</p>"},{"location":"api/status/#status-categories","title":"Status Categories","text":""},{"location":"api/status/#core-execution-statuses","title":"Core Execution Statuses","text":"<ul> <li>EXECUTES - Task executed successfully, output files created/updated</li> <li>WOULD_EXECUTE - Task would execute successfully if run (dry-run mode)</li> <li>SKIPS - Task skipped because output files exist and are current</li> <li>WOULD_SKIP - Task would be skipped because output files exist (dry-run mode)</li> </ul>"},{"location":"api/status/#compare-mode-statuses","title":"Compare Mode Statuses","text":"<ul> <li>IDENTICAL - Task re-executed, outputs identical to previous run</li> <li>DIFFERENT - Task re-executed, outputs differ from previous run</li> </ul>"},{"location":"api/status/#error-statuses","title":"Error Statuses","text":"<ul> <li>INVALID_USES - Action package specified in <code>uses:</code> not found</li> <li>INVALID_PARAMETER - Parameters in <code>with:</code> block are invalid for action</li> <li>FAILED - Task execution threw unexpected exception</li> <li>TIMED_OUT - Task exceeded configured timeout</li> </ul>"},{"location":"api/status/#properties","title":"Properties","text":""},{"location":"api/status/#is_success","title":"is_success","text":"<p>Returns <code>True</code> if status indicates successful execution: - <code>EXECUTES</code>, <code>WOULD_EXECUTE</code>, <code>SKIPS</code>, <code>WOULD_SKIP</code>, <code>IDENTICAL</code>, <code>DIFFERENT</code></p>"},{"location":"api/status/#is_error","title":"is_error","text":"<p>Returns <code>True</code> if status indicates an error condition: - <code>INVALID_USES</code>, <code>INVALID_PARAMETER</code>, <code>FAILED</code>, <code>TIMED_OUT</code></p>"},{"location":"api/status/#is_execution","title":"is_execution","text":"<p>Returns <code>True</code> if status indicates actual execution occurred: - <code>EXECUTES</code>, <code>IDENTICAL</code>, <code>DIFFERENT</code></p>"},{"location":"api/status/#is_dry_run","title":"is_dry_run","text":"<p>Returns <code>True</code> if status is for dry-run mode: - <code>WOULD_EXECUTE</code>, <code>WOULD_SKIP</code></p>"},{"location":"api/status/#usage-examples","title":"Usage Examples","text":""},{"location":"api/status/#basic-status-checking","title":"Basic Status Checking","text":"<pre><code>from causaliq_workflow import TaskStatus\n\n# Check if a task completed successfully\nif status == TaskStatus.EXECUTES:\n    print(\"Task executed successfully\")\n\n# Categorize status\nif status.is_success:\n    print(\"Task completed successfully\")\nelif status.is_error:\n    print(f\"Task failed with error: {status.value}\")\n</code></pre>"},{"location":"api/status/#status-based-control-flow","title":"Status-Based Control Flow","text":"<pre><code># Handle different execution outcomes\nmatch status:\n    case TaskStatus.EXECUTES | TaskStatus.IDENTICAL:\n        log_successful_execution(task_name, runtime)\n    case TaskStatus.SKIPS | TaskStatus.WOULD_SKIP:\n        log_skipped_task(task_name, reason=\"outputs exist\")\n    case TaskStatus.DIFFERENT:\n        log_output_changes(task_name, diff_summary)\n    case TaskStatus.FAILED:\n        log_task_failure(task_name, exception_info)\n    case _:\n        log_other_status(task_name, status)\n</code></pre>"},{"location":"api/status/#filtering-and-aggregation","title":"Filtering and Aggregation","text":"<pre><code># Count successful vs failed tasks\nsuccessful_tasks = [s for s in task_statuses if s.is_success]\nfailed_tasks = [s for s in task_statuses if s.is_error]\n\nprint(f\"Success rate: {len(successful_tasks)/len(task_statuses):.1%}\")\n\n# Find tasks that actually executed\nexecuted_tasks = [s for s in task_statuses if s.is_execution]\nprint(f\"Executed {len(executed_tasks)} tasks\")\n</code></pre>"},{"location":"api/status/#dry-run-vs-run-mode","title":"Dry-Run vs Run Mode","text":"<pre><code>def analyze_workflow_plan(statuses):\n    \"\"\"Analyze what a workflow would do in dry-run mode.\"\"\"\n    would_execute = [s for s in statuses if s == TaskStatus.WOULD_EXECUTE]\n    would_skip = [s for s in statuses if s == TaskStatus.WOULD_SKIP]\n\n    print(f\"Would execute: {len(would_execute)} tasks\")\n    print(f\"Would skip: {len(would_skip)} tasks\")\n\n    return len(would_execute) &gt; 0  # True if work needed\n</code></pre>"},{"location":"api/status/#integration-points","title":"Integration Points","text":""},{"location":"api/status/#workflow-logger-integration","title":"Workflow Logger Integration","text":"<p>The TaskStatus enum is designed for integration with the upcoming WorkflowLogger system:</p> <pre><code># Future logging integration example\nlogger.log_task(\n    action_name=\"causal-discovery\",\n    status=TaskStatus.EXECUTES,\n    message=\"learn graph\",\n    runtime=2.3,\n    outputs=\"/results/graph.xml\"\n)\n</code></pre>"},{"location":"api/status/#action-development","title":"Action Development","text":"<p>Actions will report their execution status using TaskStatus values:</p> <pre><code># Future action integration example  \ndef run(self, inputs, mode=\"dry-run\", logger=None):\n    if mode == \"dry-run\":\n        if self._outputs_exist(inputs):\n            return {\"status\": TaskStatus.WOULD_SKIP}\n        else:\n            return {\"status\": TaskStatus.WOULD_EXECUTE}\n\n    # Actual execution logic...\n    return {\"status\": TaskStatus.EXECUTES, \"outputs\": results}\n</code></pre>"},{"location":"api/status/#design-principles","title":"Design Principles","text":""},{"location":"api/status/#status-completeness","title":"Status Completeness","text":"<p>The enum covers all possible task execution outcomes across different modes, ensuring comprehensive status reporting without gaps.</p>"},{"location":"api/status/#categorization-properties","title":"Categorization Properties","text":"<p>Helper properties (<code>is_success</code>, <code>is_error</code>, etc.) enable easy filtering and aggregation without hardcoding status lists.</p>"},{"location":"api/status/#string-values","title":"String Values","text":"<p>All enum values use string representations matching their names, ensuring clear, readable log output.</p>"},{"location":"api/status/#mode-awareness","title":"Mode Awareness","text":"<p>Distinct statuses for dry-run vs run modes enable accurate workflow planning and execution reporting.</p> <p>Next: CLI Interface | Previous: Schema Validation | Up: API Reference</p>"},{"location":"api/workflow/","title":"Workflow Engine","text":"<p>The workflow execution engine provides powerful workflow parsing, validation, and execution with matrix expansion and template variable support.</p>"},{"location":"api/workflow/#core-classes","title":"Core Classes","text":""},{"location":"api/workflow/#causaliq_workflowworkflow","title":"causaliq_workflow.workflow","text":""},{"location":"api/workflow/#causaliq_workflow.workflow.WorkflowExecutor","title":"WorkflowExecutor","text":"<pre><code>WorkflowExecutor()\n</code></pre> <p>Parse and execute GitHub Actions-style workflows with matrix expansion.</p> <p>This class handles the parsing of YAML workflow files and expansion of matrix strategies into individual experiment jobs. It provides the foundation for executing multi-step causal discovery workflows with parameterised experiments using flexible action parameter templating.</p> <p>Methods:</p> <ul> <li> <code>parse_workflow</code>             \u2013              <p>Parse workflow YAML file with validation.</p> </li> <li> <code>expand_matrix</code>             \u2013              <p>Expand matrix variables into individual job configurations.</p> </li> <li> <code>execute_workflow</code>             \u2013              <p>Execute complete workflow with matrix expansion.</p> </li> </ul>"},{"location":"api/workflow/#causaliq_workflow.workflow.WorkflowExecutor.parse_workflow","title":"parse_workflow","text":"<pre><code>parse_workflow(\n    workflow_path: Union[str, Path], mode: str = \"dry-run\"\n) -&gt; Dict[str, Any]\n</code></pre> <p>Parse workflow YAML file with validation.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Dict[str, Any]</code>           \u2013            <p>Parsed and validated workflow dictionary</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>WorkflowExecutionError</code>             \u2013            <p>If workflow parsing or validation fails</p> </li> </ul>"},{"location":"api/workflow/#causaliq_workflow.workflow.WorkflowExecutor.parse_workflow(workflow_path)","title":"<code>workflow_path</code>","text":"(<code>Union[str, Path]</code>)           \u2013            <p>Path to workflow YAML file</p>"},{"location":"api/workflow/#causaliq_workflow.workflow.WorkflowExecutor.parse_workflow(mode)","title":"<code>mode</code>","text":"(<code>str</code>, default:                   <code>'dry-run'</code> )           \u2013            <p>Execution mode for action validation</p>"},{"location":"api/workflow/#causaliq_workflow.workflow.WorkflowExecutor.expand_matrix","title":"expand_matrix","text":"<pre><code>expand_matrix(matrix: Dict[str, List[Any]]) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Expand matrix variables into individual job configurations.</p> <p>Generates all combinations from matrix variables using cartesian product. Each combination becomes a separate job configuration.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>List[Dict[str, Any]]</code>           \u2013            <p>List of job configurations with matrix variables expanded</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>WorkflowExecutionError</code>             \u2013            <p>If matrix expansion fails</p> </li> </ul>"},{"location":"api/workflow/#causaliq_workflow.workflow.WorkflowExecutor.expand_matrix(matrix)","title":"<code>matrix</code>","text":"(<code>Dict[str, List[Any]]</code>)           \u2013            <p>Dictionary mapping variable names to lists of values</p>"},{"location":"api/workflow/#causaliq_workflow.workflow.WorkflowExecutor.execute_workflow","title":"execute_workflow","text":"<pre><code>execute_workflow(\n    workflow: Dict[str, Any],\n    mode: str = \"dry-run\",\n    cli_params: Optional[Dict[str, Any]] = None,\n    step_logger: Optional[Callable[[str, str, str], None]] = None,\n) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Execute complete workflow with matrix expansion.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>List[Dict[str, Any]]</code>           \u2013            <p>List of job results from matrix expansion</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>WorkflowExecutionError</code>             \u2013            <p>If workflow execution fails</p> </li> </ul>"},{"location":"api/workflow/#causaliq_workflow.workflow.WorkflowExecutor.execute_workflow(workflow)","title":"<code>workflow</code>","text":"(<code>Dict[str, Any]</code>)           \u2013            <p>Parsed workflow dictionary</p>"},{"location":"api/workflow/#causaliq_workflow.workflow.WorkflowExecutor.execute_workflow(mode)","title":"<code>mode</code>","text":"(<code>str</code>, default:                   <code>'dry-run'</code> )           \u2013            <p>Execution mode ('dry-run', 'run', 'compare')</p>"},{"location":"api/workflow/#causaliq_workflow.workflow.WorkflowExecutor.execute_workflow(cli_params)","title":"<code>cli_params</code>","text":"(<code>Optional[Dict[str, Any]]</code>, default:                   <code>None</code> )           \u2013            <p>Additional parameters from CLI</p>"},{"location":"api/workflow/#causaliq_workflow.workflow.WorkflowExecutor.execute_workflow(step_logger)","title":"<code>step_logger</code>","text":"(<code>Optional[Callable[[str, str, str], None]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional function to log step execution</p>"},{"location":"api/workflow/#exception-handling","title":"Exception Handling","text":""},{"location":"api/workflow/#causaliq_workflow.workflow.WorkflowExecutionError","title":"WorkflowExecutionError","text":"<p>Raised when workflow execution fails.</p>"},{"location":"api/workflow/#usage-examples","title":"Usage Examples","text":""},{"location":"api/workflow/#basic-workflow-execution","title":"Basic Workflow Execution","text":"<pre><code>from causaliq_workflow import WorkflowExecutor, WorkflowExecutionError\nfrom causaliq_workflow.registry import ActionRegistry\n\n# Create executor instance\nexecutor = WorkflowExecutor()\n\ntry:\n    # Parse and validate workflow (includes template variable validation)\n    workflow = executor.parse_workflow(\"experiment.yml\")\n    print(f\"Workflow ID: {workflow['id']}\")\n    print(f\"Description: {workflow['description']}\")\n\n    # Execute the complete workflow\n    results = executor.execute_workflow(workflow, mode=\"run\")\n    print(f\"Workflow completed successfully\")\n\nexcept WorkflowExecutionError as e:\n    print(f\"Workflow execution failed: {e}\")\n</code></pre>"},{"location":"api/workflow/#matrix-expansion","title":"Matrix Expansion","text":"<pre><code>from causaliq_workflow import WorkflowExecutor\n\nexecutor = WorkflowExecutor()\n\n# Define matrix for parameter sweeps\nmatrix = {\n    \"algorithm\": [\"pc\", \"ges\", \"lingam\"],\n    \"dataset\": [\"asia\", \"cancer\"],\n    \"alpha\": [0.01, 0.05]\n}\n\n# Expand into individual parameter combinations\njobs = executor.expand_matrix(matrix)\nprint(f\"Generated {len(jobs)} jobs from matrix\")  # Results in 12 jobs (3 \u00d7 2 \u00d7 2)\n\nfor i, job in enumerate(jobs):\n    print(f\"Job {i}: Algorithm={job['algorithm']}, Dataset={job['dataset']}, Alpha={job['alpha']}\")\n</code></pre>"},{"location":"api/workflow/#template-variable-system","title":"Template Variable System","text":"<pre><code># The WorkflowExecutor automatically validates template variables during parsing\n# Template variables ({{variable}}) are checked against available context\n\n# Example: Valid template usage\nvalid_workflow = {\n    \"id\": \"test-001\",\n    \"description\": \"Template validation example\", \n    \"matrix\": {\"dataset\": [\"asia\"], \"algorithm\": [\"pc\"]},\n    \"steps\": [{\n        \"uses\": \"my-custom-action\",\n        \"with\": {\n            \"output\": \"/results/{{id}}/{{dataset}}_{{algorithm}}.xml\",\n            \"description\": \"Processing {{dataset}} with {{algorithm}}\"\n        }\n    }]\n}\n\ntry:\n    workflow = executor.parse_workflow_dict(valid_workflow)\n    print(\"Template validation passed!\")\nexcept WorkflowExecutionError as e:\n    if \"Unknown template variables\" in str(e):\n        print(f\"Template validation failed: {e}\")\n        # Example: \"Unknown template variables: missing_var. Available context: id, dataset, algorithm\"\n    else:\n        print(f\"Workflow execution failed: {e}\")\n</code></pre>"},{"location":"api/workflow/#advanced-workflow-features","title":"Advanced Workflow Features","text":"<pre><code># Example workflow YAML showing flexible action parameters\nworkflow_yaml = \\\"\\\"\\\"\nid: \"experiment-001\"\ndescription: \"Flexible causal discovery experiment\"\nmatrix:\n  dataset: [\"asia\", \"cancer\"]\n  algorithm: [\"pc\", \"ges\"]\n  alpha: [0.01, 0.05]\n\nsteps:\n  - name: \"Structure Learning\"\n    uses: \"my-structure-learner\"\n    with:\n      data_path: \"/experiments/data/{{dataset}}.csv\"\n      output_dir: \"/experiments/results/{{id}}/{{algorithm}}/\"\n      alpha: \"{{alpha}}\"\n      max_iter: 1000\n\n  - name: \"Validation\"\n    uses: \"validate-graph\"\n    with:\n      graph_path: \"/experiments/results/{{id}}/{{algorithm}}/graph.graphml\"\n      metrics_output: \"/experiments/results/{{id}}/{{algorithm}}/metrics.json\"\n\\\"\\\"\\\"\n\n# Save and parse the workflow\nwith open(\"experiment.yml\", \"w\") as f:\n    f.write(workflow_yaml)\n\nworkflow = executor.parse_workflow(\"experiment.yml\")\n\n# Matrix expansion creates jobs with substituted variables\nif \"matrix\" in workflow:\n    jobs = executor.expand_matrix(workflow[\"matrix\"])\n    # Creates 8 jobs (2\u00d72\u00d72) with customizable file paths\n\n    for job in jobs:\n        print(f\"Job: {job}\")\n        # Example output: {'dataset': 'asia', 'algorithm': 'pc', 'alpha': 0.01}\n</code></pre>"},{"location":"api/workflow/#template-variable-context","title":"Template Variable Context","text":"<p>Template variables can reference:</p> <ol> <li>Workflow properties: <code>{{id}}</code>, <code>{{description}}</code>, <code>{{name}}</code></li> <li>Matrix variables: Any variables defined in the <code>matrix</code> section</li> <li>Step context: Variables available during step execution</li> <li>File paths: Dynamic path generation using workflow context</li> </ol>"},{"location":"api/workflow/#available-template-variables","title":"Available Template Variables","text":"Context Variables Example Workflow <code>id</code>, <code>description</code>, <code>name</code> <code>{{id}}</code> Matrix User-defined matrix vars <code>{{dataset}}</code>, <code>{{algorithm}}</code> Paths <code>data_root</code>, <code>output_root</code> <code>{{output_root}}/results</code>"},{"location":"api/workflow/#error-handling","title":"Error Handling","text":"<p>The WorkflowExecutor provides detailed error reporting for:</p> <ul> <li>Parse errors: Invalid YAML/JSON syntax</li> <li>Schema validation: Workflow structure validation</li> <li>Template errors: Unknown or invalid template variables</li> <li>Action errors: Action execution failures</li> <li>Matrix errors: Invalid matrix definitions</li> </ul> <p>\u2190 Previous: Registry | Back to API Overview | Next: Schema Validation \u2192</p>"},{"location":"design/action_architecture_design/","title":"Action Auto-Discovery Architecture","text":""},{"location":"design/action_architecture_design/#overview","title":"Overview","text":"<p>The action architecture provides reusable, automatically-discoverable workflow components following GitHub Actions patterns. Actions are zero-configuration plugins that become available immediately upon installation, with no registry files or manual setup required.</p>"},{"location":"design/action_architecture_design/#auto-discovery-action-framework","title":"Auto-Discovery Action Framework","text":""},{"location":"design/action_architecture_design/#how-actions-are-found-and-used","title":"How Actions Are Found and Used","text":""},{"location":"design/action_architecture_design/#the-discovery-lifecycle","title":"The Discovery Lifecycle","text":"<ol> <li>Installation Phase: Developer installs action package (<code>pip install my-action</code>)</li> <li>Discovery Phase: Framework scans Python environment for action packages  </li> <li>Registration Phase: Actions are automatically registered by module name</li> <li>Execution Phase: Workflows reference actions by name (<code>uses: \"my-action\"</code>)</li> </ol>"},{"location":"design/action_architecture_design/#convention-based-action-definition","title":"Convention-Based Action Definition","text":"<p>Actions follow a simple naming convention for automatic discovery:</p> <pre><code># my_action/__init__.py - Must export class named 'CausalIQAction'\nfrom causaliq_workflow.action import Action\n\nclass CausalIQAction(Action):  # Must be named 'CausalIQAction'\n    name = \"my-action\"\n    version = \"1.0.0\"\n    description = \"Performs custom analysis\"\n\n    def run(self, inputs):\n        # Implementation here\n        return {\"status\": \"complete\"}\n</code></pre>"},{"location":"design/action_architecture_design/#base-action-interface","title":"Base Action Interface","text":"<pre><code>from abc import ABC, abstractmethod\nfrom typing import Dict, Any, Optional\nfrom dataclasses import dataclass\nimport semantic_version\n\n@dataclass\nclass ActionInput:\n    \"\"\"Define action input specification.\"\"\"\n    name: str\n    description: str\n    required: bool = False\n    default: Any = None\n    type_hint: str = \"Any\"\n\n@dataclass\nclass ActionOutput:\n    \"\"\"Define action output specification.\"\"\"\n    name: str\n    description: str\n    value: Any\n\nclass Action(ABC):\n    \"\"\"Base class for all workflow actions.\"\"\"\n\n    # Action metadata\n    name: str = \"\"\n    version: str = \"1.0.0\"\n    description: str = \"\"\n    author: str = \"\"\n\n    # Input/output specifications\n    inputs: Dict[str, ActionInput] = {}\n    outputs: Dict[str, str] = {}  # name -&gt; description mapping\n\n    @abstractmethod\n    def run(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Execute action with validated inputs, return outputs.\"\"\"\n        pass\n\n    def validate_inputs(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Validate and process input values.\"\"\"\n        validated = {}\n\n        for input_name, input_spec in self.inputs.items():\n            if input_spec.required and input_name not in inputs:\n                raise ValueError(f\"Required input '{input_name}' missing for action {self.name}\")\n\n            value = inputs.get(input_name, input_spec.default)\n            validated[input_name] = value\n\n        return validated\n\n    def format_outputs(self, raw_outputs: Dict[str, Any]) -&gt; Dict[str, ActionOutput]:\n        \"\"\"Format raw outputs with metadata.\"\"\"\n        formatted = {}\n\n        for name, value in raw_outputs.items():\n            description = self.outputs.get(name, f\"Output from {self.name}\")\n            formatted[name] = ActionOutput(\n                name=name,\n                description=description,\n                value=value\n            )\n\n        return formatted\n</code></pre>"},{"location":"design/action_architecture_design/#auto-discovery-action-registry","title":"Auto-Discovery Action Registry","text":"<p>The registry automatically discovers and manages actions without configuration:</p> <pre><code>import pkgutil\nimport importlib\nfrom typing import Dict, Type, Any\n\nclass ActionRegistry:\n    \"\"\"Automatically discover and manage workflow actions.\"\"\"\n\n    def __init__(self):\n        self._actions: Dict[str, Type[Action]] = {}\n        self._discover_actions()  # Automatic discovery on initialization\n\n    def _discover_actions(self):\n        \"\"\"Scan Python environment for action packages.\"\"\"\n\n        # Iterate through all importable modules\n        for finder, module_name, ispkg in pkgutil.iter_modules():\n            try:\n                # Attempt to import the module\n                module = importlib.import_module(module_name)\n\n                # Check if module exports an 'Action' class\n                if hasattr(module, 'Action'):\n                    action_class = getattr(module, 'Action')\n\n                    # Verify it's a proper Action subclass\n                    if (isinstance(action_class, type) and \n                        issubclass(action_class, Action) and \n                        action_class != Action):\n\n                        # Register using module name as action identifier\n                        self._actions[module_name] = action_class\n\n            except ImportError:\n                # Skip modules that can't be imported\n                continue\n\n    def get_available_actions(self) -&gt; Dict[str, Type[Action]]:\n        \"\"\"Return copy of available actions.\"\"\"\n        return self._actions.copy()\n\n    def get_action_class(self, action_name: str) -&gt; Type[Action]:\n        \"\"\"Get action class by name.\"\"\"\n        if action_name not in self._actions:\n            raise ActionRegistryError(f\"Action '{action_name}' not found. Available actions: {list(self._actions.keys())}\")\n        return self._actions[action_name]\n</code></pre>"},{"location":"design/action_architecture_design/#how-discovery-works-step-by-step","title":"How Discovery Works Step-by-Step","text":"<ol> <li>Registry Initialization: When <code>ActionRegistry()</code> is created, discovery starts automatically</li> <li>Module Scanning: Uses <code>pkgutil.iter_modules()</code> to iterate through all Python modules</li> <li>Safe Import: Attempts to import each module, skipping those that fail</li> <li>Convention Check: Looks for a class named 'Action' in each module</li> <li>Validation: Ensures the Action class inherits from the base Action class</li> <li>Registration: Maps module name to Action class for workflow lookup</li> </ol>"},{"location":"design/action_architecture_design/#action-package-development-workflow","title":"Action Package Development Workflow","text":"<p>Step 1: Create Standard Python Package</p> <pre><code>mkdir my_custom_action\ncd my_custom_action\n</code></pre> <p>Step 2: Define Package Structure</p> <pre><code>my_custom_action/\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 my_custom_action/\n\u2502   \u2514\u2500\u2500 __init__.py  # Must export 'Action' class\n\u2514\u2500\u2500 README.md\n</code></pre> <p>Step 3: Implement Action Convention</p> <pre><code># my_custom_action/__init__.py\nfrom causaliq_workflow.action import Action\n\nclass CausalIQAction(Action):  # Must be named 'CausalIQAction'\n    name = \"my-custom-action\"\n    version = \"1.0.0\" \n    description = \"Custom analysis action\"\n\n    def run(self, inputs):\n        # Action implementation\n        result = self.perform_analysis(inputs['data'])\n        return {\"analysis_result\": result}\n</code></pre> <p>Step 4: Install and Use Immediately</p> <pre><code>pip install my_custom_action\ncausaliq-workflow my-experiment.yml  # Action automatically available\n</code></pre> <pre><code>\n## Auto-Discovery Action Examples\n\n### Example 1: Simple Analysis Action\n\n**Package: causaliq_analysis**\n\n```python\n# causaliq_analysis/__init__.py\nfrom causaliq_workflow.action import Action\nimport pandas as pd\nimport networkx as nx\n\nclass CausalIQAction(Action):  # Auto-discovered by this name\n    name = \"causaliq-analysis\"\n    version = \"1.0.0\"\n    description = \"Basic causal graph analysis\"\n\n    def run(self, inputs):\n        \"\"\"Analyze causal graph structure.\"\"\"\n        graph_path = inputs['graph_path']\n        graph = nx.read_graphml(graph_path)\n\n        analysis = {\n            \"nodes\": len(graph.nodes),\n            \"edges\": len(graph.edges), \n            \"density\": nx.density(graph),\n            \"is_dag\": nx.is_directed_acyclic_graph(graph)\n        }\n\n        return {\"analysis\": analysis}\n</code></pre> <p>Usage in Workflow:</p> <pre><code>steps:\n  - name: \"Analyze Graph\"\n    uses: \"causaliq_analysis\"  # Automatically discovered\n    with:\n      graph_path: \"/results/learned_graph.xml\"\n</code></pre>"},{"location":"design/action_architecture_design/#example-2-data-loading-action","title":"Example 2: Data Loading Action","text":"<p>Package: causaliq_data</p> <pre><code># causaliq_data/__init__.py  \nfrom causaliq_workflow.action import Action\nimport pandas as pd\nfrom pathlib import Path\n\nclass CausalIQAction(Action):\n    name = \"causaliq-data\"\n    version = \"2.1.0\"\n    description = \"Load and preprocess causal datasets\"\n\n    def run(self, inputs):\n        \"\"\"Load dataset with optional preprocessing.\"\"\"\n        dataset_name = inputs['dataset']\n        sample_size = inputs.get('sample_size')\n\n        # Load from standard datasets\n        if dataset_name == \"asia\":\n            data = self._load_asia_network()\n        elif dataset_name == \"cancer\":\n            data = self._load_cancer_network()\n        else:\n            # Load from file path\n            data = pd.read_csv(dataset_name)\n\n        # Apply sampling if requested\n        if sample_size and sample_size &lt; len(data):\n            data = data.sample(n=sample_size, random_state=42)\n\n        output_path = inputs['output_path']\n        data.to_csv(f\"{output_path}/data.csv\", index=False)\n\n        return {\n            \"data_path\": f\"{output_path}/data.csv\",\n            \"rows\": len(data),\n            \"columns\": len(data.columns)\n        }\n</code></pre>"},{"location":"design/action_architecture_design/#example-3-algorithm-bridge-action","title":"Example 3: Algorithm Bridge Action","text":"<p>Package: causaliq_pc_algorithm</p> <pre><code># causaliq_pc_algorithm/__init__.py\nfrom causaliq_workflow.action import Action\nimport pandas as pd\nimport networkx as nx\n\nclass CausalIQAction(Action):\n    name = \"causaliq-pc-algorithm\" \n    version = \"1.5.2\"\n    description = \"PC algorithm for causal structure learning\"\n\n    def run(self, inputs):\n        \"\"\"Execute PC algorithm.\"\"\"\n        data_path = inputs['data_path']\n        alpha = inputs.get('alpha', 0.05)\n        output_path = inputs['output_path']\n\n        # Load data\n        data = pd.read_csv(data_path)\n\n        # Run PC algorithm (implementation details omitted)\n        graph = self._execute_pc_algorithm(data, alpha)\n\n        # Save results\n        nx.write_graphml(graph, f\"{output_path}/graph.xml\")\n\n        # Generate metadata\n        metadata = {\n            \"algorithm\": \"pc\",\n            \"alpha\": alpha,\n            \"nodes\": len(graph.nodes),\n            \"edges\": len(graph.edges)\n        }\n\n        with open(f\"{output_path}/metadata.json\", 'w') as f:\n            json.dump(metadata, f, indent=2)\n\n        return {\n            \"graph_path\": f\"{output_path}/graph.xml\",\n            \"metadata_path\": f\"{output_path}/metadata.json\",\n            \"edge_count\": len(graph.edges)\n        }\n            columns = dataset.columns.tolist()\n            np.random.shuffle(columns)\n            randomised = dataset[columns]\n            transformation_log.append(f\"Shuffled column order: {' -&gt; '.join(columns)}\")\n\n        elif strategy == \"subsample\":\n            subsample_size = min(len(dataset) // 2, 1000)\n            randomised = dataset.sample(n=subsample_size).reset_index(drop=True)\n            transformation_log.append(f\"Subsampled to {subsample_size} rows\")\n\n        elif strategy == \"bootstrap\":\n            randomised = dataset.sample(n=len(dataset), replace=True).reset_index(drop=True)\n            transformation_log.append(\"Bootstrap resampling applied\")\n\n        else:\n            raise ValueError(f\"Unknown randomisation strategy: {strategy}\")\n\n        return {\n            \"randomised_dataset\": randomised,\n            \"transformation_log\": transformation_log\n        }\n</code></pre>"},{"location":"design/action_architecture_design/#algorithm-execution-actions","title":"Algorithm Execution Actions","text":"<pre><code>import networkx as nx\n\nclass CausalDiscoveryAction(Action):\n    \"\"\"Execute causal discovery algorithm from various packages.\"\"\"\n\n    name = \"causal-discovery\"\n    version = \"1.0.0\"\n    description = \"Run causal discovery algorithm with automatic package detection\"\n\n    inputs = {\n        \"algorithm\": ActionInput(\"algorithm\", \"Algorithm name (pc, ges, lingam, etc.)\", required=True),\n        \"package\": ActionInput(\"package\", \"Algorithm package (bnlearn, tetrad, causal-learn, auto)\", default=\"auto\"),\n        \"data\": ActionInput(\"data\", \"Input dataset\", required=True),\n        \"parameters\": ActionInput(\"parameters\", \"Algorithm-specific parameters\", default={})\n    }\n\n    outputs = {\n        \"learned_graph\": \"Learned causal graph as NetworkX DiGraph\",\n        \"algorithm_info\": \"Information about algorithm execution\",\n        \"performance_metrics\": \"Execution time, memory usage, convergence info\"\n    }\n\n    def run(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Execute causal discovery algorithm.\"\"\"\n        algorithm = inputs[\"algorithm\"].lower()\n        package = inputs[\"package\"]\n        data = inputs[\"data\"]\n        parameters = inputs[\"parameters\"]\n\n        # Auto-detect package if needed\n        if package == \"auto\":\n            package = self._detect_best_package(algorithm)\n\n        # Execute algorithm\n        start_time = time.time()\n\n        if package == \"bnlearn\":\n            learned_graph, algo_info = self._execute_bnlearn(algorithm, data, parameters)\n        elif package == \"tetrad\":\n            learned_graph, algo_info = self._execute_tetrad(algorithm, data, parameters)\n        elif package == \"causal-learn\":\n            learned_graph, algo_info = self._execute_causal_learn(algorithm, data, parameters)\n        else:\n            raise ValueError(f\"Unsupported package: {package}\")\n\n        execution_time = time.time() - start_time\n\n        performance_metrics = {\n            \"execution_time_seconds\": execution_time,\n            \"algorithm\": algorithm,\n            \"package\": package,\n            \"num_variables\": len(data.columns),\n            \"num_samples\": len(data),\n            \"num_edges\": learned_graph.number_of_edges()\n        }\n\n        return {\n            \"learned_graph\": learned_graph,\n            \"algorithm_info\": algo_info,\n            \"performance_metrics\": performance_metrics\n        }\n\n    def _detect_best_package(self, algorithm: str) -&gt; str:\n        \"\"\"Detect best available package for algorithm.\"\"\"\n        algorithm_packages = {\n            \"pc\": [\"bnlearn\", \"causal-learn\", \"tetrad\"],\n            \"ges\": [\"causal-learn\", \"tetrad\"], \n            \"lingam\": [\"causal-learn\"],\n            \"iamb\": [\"bnlearn\"],\n            \"gs\": [\"bnlearn\"]\n        }\n\n        preferred_packages = algorithm_packages.get(algorithm, [\"causal-learn\"])\n\n        # Check availability and return first available\n        for package in preferred_packages:\n            if self._is_package_available(package):\n                return package\n\n        raise RuntimeError(f\"No available package found for algorithm: {algorithm}\")\n\n    def _execute_bnlearn(self, algorithm: str, data: pd.DataFrame, \n                        parameters: Dict) -&gt; tuple:\n        \"\"\"Execute algorithm using R bnlearn.\"\"\"\n        try:\n            import rpy2.robjects as ro\n            from rpy2.robjects import pandas2ri\n        }\n</code></pre>"},{"location":"design/action_architecture_design/#benefits-of-auto-discovery-architecture","title":"Benefits of Auto-Discovery Architecture","text":""},{"location":"design/action_architecture_design/#for-action-developers","title":"For Action Developers","text":""},{"location":"design/action_architecture_design/#zero-configuration-setup","title":"Zero Configuration Setup","text":"<ul> <li>No registry management: No need to maintain configuration files or plugin registries</li> <li>Standard Python patterns: Use familiar <code>pyproject.toml</code>, <code>pip install</code>, and package structure</li> <li>Immediate availability: Actions become available as soon as the package is installed</li> <li>Simple convention: Just export a class named 'Action' from the package</li> </ul>"},{"location":"design/action_architecture_design/#development-workflow","title":"Development Workflow","text":"<ol> <li>Create package: Standard Python package with <code>pyproject.toml</code></li> <li>Implement action: Export 'Action' class following the interface</li> <li>Test locally: <code>pip install -e .</code> for development testing</li> <li>Publish: Standard PyPI publishing or GitHub releases</li> <li>Use immediately: Actions available in all workflows without restart</li> </ol>"},{"location":"design/action_architecture_design/#for-workflow-authors","title":"For Workflow Authors","text":""},{"location":"design/action_architecture_design/#seamless-integration","title":"Seamless Integration","text":"<ul> <li>Familiar syntax: Uses standard GitHub Actions-style <code>uses: \"action-name\"</code></li> <li>No configuration: No need to declare or configure actions before use</li> <li>Version management: Standard semantic versioning through package versions</li> <li>Dependency handling: Python's pip handles all dependencies automatically</li> </ul>"},{"location":"design/action_architecture_design/#ecosystem-growth","title":"Ecosystem Growth","text":"<ul> <li>Organic discovery: New actions become available automatically</li> <li>Community contributions: Easy for community to create and share actions</li> <li>Quality assurance: Actions are regular Python packages with standard testing</li> <li>Documentation: Standard Python documentation tools apply</li> </ul>"},{"location":"design/action_architecture_design/#for-the-framework","title":"For the Framework","text":""},{"location":"design/action_architecture_design/#architectural-benefits","title":"Architectural Benefits","text":"<ul> <li>Reduced complexity: No registry files, configuration, or plugin management code</li> <li>Robustness: Discovery failures don't break the system (graceful degradation)  </li> <li>Performance: Lazy loading and one-time discovery minimize overhead</li> <li>Maintainability: Less framework code means easier maintenance</li> </ul>"},{"location":"design/action_architecture_design/#ecosystem-integration","title":"Ecosystem Integration","text":"<ul> <li>Standard distribution: Uses PyPI and standard Python packaging</li> <li>Cross-platform: Works wherever Python works</li> <li>Version compatibility: Standard semantic versioning for compatibility management</li> <li>Testing integration: Actions can be tested with standard Python testing tools</li> </ul>"},{"location":"design/action_architecture_design/#auto-discovery-implementation-patterns","title":"Auto-Discovery Implementation Patterns","text":""},{"location":"design/action_architecture_design/#cross-language-bridges","title":"Cross-Language Bridges","text":"<p>Actions can bridge to R, Java, and other languages:</p> <pre><code># causaliq_bnlearn/__init__.py\nfrom causaliq_workflow.action import Action\nimport rpy2.robjects as ro\n\nclass CausalIQAction(Action):\n    name = \"causaliq-bnlearn\"\n\n    def __init__(self):\n        # Initialize R environment once\n        ro.r('library(bnlearn)')\n\n    def run(self, inputs):\n        # Bridge to R bnlearn package\n        algorithm = inputs['algorithm']  # 'pc', 'gs', 'iamb', etc.\n        ro.globalenv['data'] = inputs['data']\n        ro.r(f'result &lt;- {algorithm}(data)')\n        return {\"graph\": self._convert_to_networkx(ro.r('result'))}\n</code></pre>"},{"location":"design/action_architecture_design/#algorithm-collections","title":"Algorithm Collections","text":"<p>Single packages can provide multiple related algorithms:</p> <pre><code># causaliq_constraint_based/__init__.py\nclass CausalIQAction(Action):\n    name = \"causaliq-constraint-based\"\n\n    def run(self, inputs):\n        algorithm = inputs['algorithm']\n\n        if algorithm == 'pc':\n            return self._run_pc(inputs)\n        elif algorithm == 'fci':\n            return self._run_fci(inputs)\n        elif algorithm == 'cfci':\n            return self._run_cfci(inputs)\n        else:\n            raise ValueError(f\"Unknown algorithm: {algorithm}\")\n</code></pre> <p>This auto-discovery architecture creates a vibrant, extensible ecosystem where actions can be developed, shared, and used with minimal friction while maintaining the robustness and reliability needed for scientific workflows.</p>"},{"location":"design/action_auto_discovery_design/","title":"Action Auto-Discovery Design","text":""},{"location":"design/action_auto_discovery_design/#overview","title":"Overview","text":"<p>The auto-discovery system implements a convention-over-configuration approach where actions are automatically found and registered without any configuration files. This design eliminates the need for manual registry management while providing a seamless plugin ecosystem for causal discovery workflows.</p>"},{"location":"design/action_auto_discovery_design/#how-auto-discovery-works-the-complete-journey","title":"How Auto-Discovery Works: The Complete Journey","text":""},{"location":"design/action_auto_discovery_design/#phase-1-system-initialization","title":"Phase 1: System Initialization","text":""},{"location":"design/action_auto_discovery_design/#step-1-workflow-engine-starts","title":"Step 1: Workflow Engine Starts","text":"<p>When a user runs <code>causaliq-workflow experiment.yml</code>:</p> <ol> <li>WorkflowExecutor initialization: Creates a new <code>WorkflowExecutor</code> instance</li> <li>ActionRegistry creation: Instantiates <code>ActionRegistry()</code> which triggers discovery</li> <li>Discovery activation: The registry immediately begins scanning for actions</li> </ol>"},{"location":"design/action_auto_discovery_design/#step-2-python-environment-scanning","title":"Step 2: Python Environment Scanning","text":"<p>The system methodically searches the Python environment:</p> <ol> <li>Module enumeration: Uses <code>pkgutil.iter_modules()</code> to list all importable modules</li> <li>Safe import attempts: Tries to import each module with comprehensive error handling</li> <li>Module filtering: Focuses only on modules that successfully import</li> </ol>"},{"location":"design/action_auto_discovery_design/#phase-2-action-detection-and-registration","title":"Phase 2: Action Detection and Registration","text":""},{"location":"design/action_auto_discovery_design/#step-3-convention-based-detection","title":"Step 3: Convention-Based Detection","text":"<p>For each successfully imported module, the system applies the discovery convention:</p> <ol> <li>Attribute inspection: Checks if the module has an attribute named 'CausalIQAction'</li> <li>Type validation: Verifies that the 'CausalIQAction' attribute is a class</li> <li>Inheritance verification: Confirms it inherits from <code>causaliq_workflow.action.Action</code></li> <li>Action validation: Ensures the class implements required abstract methods</li> </ol>"},{"location":"design/action_auto_discovery_design/#step-4-automatic-registration","title":"Step 4: Automatic Registration","text":"<p>When an action is detected:</p> <ol> <li>Name extraction: Uses the module name as the action identifier</li> <li>Registry storage: Stores mapping: <code>{module_name: CausalIQAction_class}</code></li> <li>Metadata collection: Gathers action name, version, description from the class</li> <li>Availability confirmation: Marks the action as ready for workflow use</li> </ol>"},{"location":"design/action_auto_discovery_design/#phase-3-workflow-execution-integration","title":"Phase 3: Workflow Execution Integration","text":""},{"location":"design/action_auto_discovery_design/#step-5-yaml-parsing-and-action-resolution","title":"Step 5: YAML Parsing and Action Resolution","text":"<p>When processing a workflow file:</p> <pre><code>steps:\n  - name: \"Custom Analysis\"\n    uses: \"my_custom_action\"  # This triggers action lookup\n    with:\n      parameter: \"value\"\n</code></pre> <p>The system follows this resolution path:</p> <ol> <li>Action name extraction: Parses <code>uses: \"my_custom_action\"</code></li> <li>Registry lookup: Searches registered actions for \"my_custom_action\"</li> <li>Class retrieval: Gets the corresponding CausalIQAction class</li> <li>Instance creation: Instantiates the action with <code>CausalIQAction()</code></li> </ol>"},{"location":"design/action_auto_discovery_design/#step-6-dynamic-action-execution","title":"Step 6: Dynamic Action Execution","text":"<p>During step execution:</p> <ol> <li>Parameter preparation: Collects parameters from the <code>with</code> block</li> <li>Input validation: Action validates its own inputs using schemas</li> <li>Execution: Calls <code>action.run(inputs)</code> with validated parameters</li> <li>Output handling: Processes and stores action outputs</li> </ol>"},{"location":"design/action_auto_discovery_design/#the-plugin-developer-experience","title":"The Plugin Developer Experience","text":""},{"location":"design/action_auto_discovery_design/#creating-an-action-package-step-by-step","title":"Creating an Action Package: Step-by-Step","text":""},{"location":"design/action_auto_discovery_design/#step-1-standard-python-package-setup","title":"Step 1: Standard Python Package Setup","text":"<p>Developers start with familiar Python packaging:</p> <pre><code>mkdir causaliq-pc-algorithm\ncd causaliq-pc-algorithm\n</code></pre> <p>File: pyproject.toml</p> <pre><code>[project]\nname = \"causaliq-pc-algorithm\"\nversion = \"1.0.0\"\ndescription = \"PC algorithm implementation for CausalIQ workflows\"\ndependencies = [\n    \"causaliq-workflow&gt;=0.1.0\",\n    \"networkx&gt;=2.8.0\",\n    \"pandas&gt;=1.5.0\"\n]\n</code></pre>"},{"location":"design/action_auto_discovery_design/#step-2-implement-the-action-convention","title":"Step 2: Implement the Action Convention","text":"<p>File: causaliq_pc_algorithm/init.py</p> <pre><code>from causaliq_workflow.action import Action\nimport pandas as pd\nimport networkx as nx\n\n# Must be named 'CausalIQAction' for auto-discovery\nclass CausalIQAction(Action):\n    name = \"causaliq-pc-algorithm\"\n    version = \"1.0.0\" \n    description = \"PC algorithm for causal structure learning\"\n\n    def run(self, inputs):\n        # Read data\n        data = pd.read_csv(inputs['dataset'])\n\n        # Run PC algorithm\n        alpha = inputs.get('alpha', 0.05)\n        graph = self.pc_algorithm(data, alpha)\n\n        # Save results\n        output_path = inputs['output_path']\n        nx.write_graphml(graph, f\"{output_path}/graph.xml\")\n\n        return {\n            \"graph_path\": f\"{output_path}/graph.xml\",\n            \"nodes\": len(graph.nodes),\n            \"edges\": len(graph.edges)\n        }\n\n    def pc_algorithm(self, data, alpha):\n        # PC algorithm implementation\n        pass\n</code></pre>"},{"location":"design/action_auto_discovery_design/#step-3-installation-and-immediate-availability","title":"Step 3: Installation and Immediate Availability","text":"<pre><code># Install the package\npip install causaliq-pc-algorithm\n\n# Action is immediately available in workflows\ncausaliq-workflow my-experiment.yml\n</code></pre> <p>File: my-experiment.yml</p> <pre><code>name: \"PC Algorithm Test\"\nsteps:\n  - name: \"Learn Structure\"\n    uses: \"causaliq-pc-algorithm\"  # Automatically discovered\n    with:\n      dataset: \"/data/asia.csv\"\n      alpha: 0.01\n      output_path: \"/results/pc\"\n</code></pre>"},{"location":"design/action_auto_discovery_design/#discovery-process-implementation-details","title":"Discovery Process Implementation Details","text":""},{"location":"design/action_auto_discovery_design/#module-introspection-strategy","title":"Module Introspection Strategy","text":"<p>The system uses Python's built-in introspection capabilities:</p> <pre><code>import pkgutil\nimport importlib\nfrom typing import Dict, Type\n\nclass ActionRegistry:\n    def __init__(self):\n        self.actions: Dict[str, Type[Action]] = {}\n        self._discover_actions()\n\n    def _discover_actions(self):\n        \"\"\"Automatically discover and register all available actions.\"\"\"\n\n        # Scan all importable modules\n        for finder, module_name, ispkg in pkgutil.iter_modules():\n            try:\n                # Safely attempt to import module\n                module = importlib.import_module(module_name)\n\n                # Check for CausalIQAction class using naming convention\n                if hasattr(module, 'CausalIQAction'):\n                    action_class = getattr(module, 'CausalIQAction')\n\n                    # Verify it's actually an Action subclass\n                    if (isinstance(action_class, type) and \n                        issubclass(action_class, Action) and \n                        action_class != Action):\n\n                        # Register using module name\n                        self.actions[module_name] = action_class\n\n            except ImportError:\n                # Skip modules that can't be imported\n                continue\n</code></pre>"},{"location":"design/action_auto_discovery_design/#error-handling-and-resilience","title":"Error Handling and Resilience","text":"<p>The discovery process is designed to be robust:</p> <ol> <li>Import error tolerance: Failed module imports are silently skipped</li> <li>Type safety: Strict validation of action classes</li> <li>Namespace isolation: Each action operates in its own namespace</li> <li>Graceful degradation: System continues working even if some actions fail to load</li> </ol>"},{"location":"design/action_auto_discovery_design/#performance-considerations","title":"Performance Considerations","text":"<p>The auto-discovery system is optimized for efficiency:</p> <ol> <li>Lazy loading: Actions are only instantiated when needed</li> <li>One-time discovery: Module scanning happens once during registry creation</li> <li>Minimal overhead: Discovery adds negligible startup time</li> <li>Cached results: Action registry is reused across workflow steps</li> </ol>"},{"location":"design/action_auto_discovery_design/#ecosystem-integration-patterns","title":"Ecosystem Integration Patterns","text":""},{"location":"design/action_auto_discovery_design/#distribution-strategy","title":"Distribution Strategy","text":"<p>Action packages follow standard Python distribution:</p> <ol> <li>PyPI publishing: <code>pip install causaliq-tetrad-bridge</code></li> <li>GitHub releases: Direct installation from repositories</li> <li>Local development: <code>pip install -e .</code> for development packages</li> <li>Version management: Standard semantic versioning</li> </ol>"},{"location":"design/action_auto_discovery_design/#dependency-management","title":"Dependency Management","text":"<p>Actions declare their dependencies naturally:</p> <pre><code># Action package dependencies\ndependencies = [\n    \"causaliq-workflow&gt;=0.1.0\",    # Framework dependency\n    \"rpy2&gt;=3.5.0\",                 # R interface (if needed)\n    \"jpype1&gt;=1.4.0\",               # Java interface (if needed)\n    \"scikit-learn&gt;=1.2.0\"          # Algorithm dependencies\n]\n</code></pre>"},{"location":"design/action_auto_discovery_design/#cross-language-bridge-pattern","title":"Cross-Language Bridge Pattern","text":"<p>For R and Java integrations:</p> <pre><code>class RBridgeAction(Action):\n    def __init__(self):\n        # Initialize R environment\n        self.r_session = self._setup_r_environment()\n\n    def run(self, inputs):\n        # Execute R code through rpy2\n        result = self.r_session.run_algorithm(inputs)\n        return self._convert_r_output(result)\n</code></pre>"},{"location":"design/action_auto_discovery_design/#benefits-of-the-auto-discovery-approach","title":"Benefits of the Auto-Discovery Approach","text":""},{"location":"design/action_auto_discovery_design/#for-users","title":"For Users","text":"<ol> <li>Zero configuration: Install and use immediately</li> <li>No registry management: No config files to maintain</li> <li>Standard workflow: Familiar pip install \u2192 use pattern</li> <li>Automatic updates: New action versions available immediately</li> </ol>"},{"location":"design/action_auto_discovery_design/#for-developers","title":"For Developers","text":"<ol> <li>Standard Python packaging: No custom build systems</li> <li>Simple convention: Just export an 'Action' class</li> <li>Full Python ecosystem: Use any Python dependencies</li> <li>Independent development: No coordination with core framework needed</li> </ol>"},{"location":"design/action_auto_discovery_design/#for-the-ecosystem","title":"For the Ecosystem","text":"<ol> <li>Organic growth: Easy to create and share actions</li> <li>Version management: Standard semantic versioning</li> <li>Quality control: Actions are regular Python packages with testing</li> <li>Documentation: Standard Python documentation tools apply</li> </ol>"},{"location":"design/logging_architecture_design/","title":"Logging &amp; Task Status Architecture Design","text":"<p>Design Document Version: 1.0 Date: 2025-11-18 Status: Draft - Implementation Planned</p>"},{"location":"design/logging_architecture_design/#overview","title":"Overview","text":"<p>CausalIQ Workflow requires a comprehensive logging and task status system to provide visibility into workflow execution, enable debugging, and support different execution modes (dry-run, run, compare). This document outlines the architecture for task-level logging with standardized status reporting.</p>"},{"location":"design/logging_architecture_design/#requirements","title":"Requirements","text":""},{"location":"design/logging_architecture_design/#functional-requirements","title":"Functional Requirements","text":""},{"location":"design/logging_architecture_design/#fr1-multi-destination-logging","title":"FR1: Multi-Destination Logging","text":"<ul> <li>Requirement: Log messages to file and/or terminal simultaneously</li> <li>Rationale: Terminal output for interactive use, file output for audit trails and debugging</li> <li>Implementation: Configurable output destinations via CLI parameters</li> </ul>"},{"location":"design/logging_architecture_design/#fr2-test-capturable-output","title":"FR2: Test-Capturable Output","text":"<ul> <li>Requirement: Output must be capturable and verifiable in automated tests</li> <li>Rationale: Essential for testing workflow execution and validating status reporting</li> <li>Implementation: Structured logging interface that tests can intercept</li> </ul>"},{"location":"design/logging_architecture_design/#fr3-task-level-granularity","title":"FR3: Task-Level Granularity","text":"<ul> <li>Requirement: Messages produced at individual task level (each action execution)</li> <li>Rationale: Actions may execute multiple internal tasks (e.g., learning hundreds of graphs)</li> <li>Implementation: Actions control their own message granularity and frequency</li> </ul>"},{"location":"design/logging_architecture_design/#fr4-standardized-message-format","title":"FR4: Standardized Message Format","text":"<ul> <li>Requirement: Consistent, parseable message format across all actions</li> <li>Format: <code>YYYY-MM-DD HH:MM:SS [action-name] STATUS task description (details)</code></li> <li>Example: <code>2025-06-23 12:03:23 [causal-discovery] EXECUTES learn graph in 2.3s</code></li> <li>Rationale: Enables monitoring, parsing, and integration with external tools</li> </ul>"},{"location":"design/logging_architecture_design/#fr5-comprehensive-status-system","title":"FR5: Comprehensive Status System","text":"<ul> <li>Requirement: Task execution status covers all workflow execution modes</li> <li>Statuses: See Task Status Definitions</li> <li>Rationale: Provides complete visibility into workflow execution state</li> </ul>"},{"location":"design/logging_architecture_design/#fr6-summary-reporting","title":"FR6: Summary Reporting","text":"<ul> <li>Requirement: Aggregate status counts and resource consumption</li> <li>Includes: Task counts per status, total runtime, estimated resource usage</li> <li>Rationale: High-level workflow execution overview for monitoring</li> </ul>"},{"location":"design/logging_architecture_design/#fr7-progress-indication","title":"FR7: Progress Indication","text":"<ul> <li>Requirement: Real-time progress feedback during execution</li> <li>Implementation: Click-based progress bars with estimated completion</li> <li>Rationale: User experience during long-running workflows</li> </ul>"},{"location":"design/logging_architecture_design/#non-functional-requirements","title":"Non-Functional Requirements","text":""},{"location":"design/logging_architecture_design/#nfr1-performance","title":"NFR1: Performance","text":"<ul> <li>Requirement: Logging overhead &lt; 5% of total execution time</li> <li>Implementation: Efficient logging with minimal I/O blocking</li> </ul>"},{"location":"design/logging_architecture_design/#nfr2-flexibility","title":"NFR2: Flexibility","text":"<ul> <li>Requirement: Actions have complete control over message frequency/content</li> <li>Implementation: Optional logger interface, actions decide granularity</li> </ul>"},{"location":"design/logging_architecture_design/#nfr3-backward-compatibility","title":"NFR3: Backward Compatibility","text":"<ul> <li>Requirement: Existing actions work without modification</li> <li>Implementation: Logger parameter is optional, graceful degradation</li> </ul>"},{"location":"design/logging_architecture_design/#task-status-definitions","title":"Task Status Definitions","text":""},{"location":"design/logging_architecture_design/#core-execution-statuses","title":"Core Execution Statuses","text":""},{"location":"design/logging_architecture_design/#executes","title":"EXECUTES","text":"<ul> <li>Mode: run, compare</li> <li>Condition: Task executed successfully, output files created/updated</li> <li>Message: Includes actual runtime, input/output file sizes</li> <li>Example: <code>EXECUTES learn graph in 2.3s \u2192 inputs: /data/asia.csv (1.2MB) \u2192 outputs: /results/graph.xml (0.8MB)</code></li> </ul>"},{"location":"design/logging_architecture_design/#would_execute","title":"WOULD_EXECUTE","text":"<ul> <li>Mode: dry-run</li> <li>Condition: Task would execute successfully if run, output files absent</li> <li>Message: Includes estimated runtime from action</li> <li>Example: <code>WOULD_EXECUTE learn graph in ~2.1s \u2192 inputs: /data/asia.csv (1.2MB) \u2192 outputs: /results/graph.xml (estimated)</code></li> </ul>"},{"location":"design/logging_architecture_design/#skips","title":"SKIPS","text":"<ul> <li>Mode: run</li> <li>Condition: Task skipped because output files exist and are current</li> <li>Message: Includes existing file information</li> <li>Example: <code>SKIPS learn graph \u2192 outputs: /results/graph.xml (exists, 0.8MB)</code></li> <li>Special Case: For append-semantics files (e.g., metadata.json), only skips if action's expected contribution already exists</li> </ul>"},{"location":"design/logging_architecture_design/#would_skip","title":"WOULD_SKIP","text":"<ul> <li>Mode: dry-run  </li> <li>Condition: Task would be skipped because output files exist</li> <li>Message: Includes existing file information</li> <li>Example: <code>WOULD_SKIP learn graph \u2192 outputs: /results/graph.xml (exists, 0.8MB)</code></li> <li>Special Case: For append-semantics files, evaluates whether action's contribution would be added</li> </ul>"},{"location":"design/logging_architecture_design/#compare-mode-statuses","title":"Compare Mode Statuses","text":""},{"location":"design/logging_architecture_design/#identical","title":"IDENTICAL","text":"<ul> <li>Mode: compare</li> <li>Condition: Task re-executed, outputs identical to previous run</li> <li>Message: Includes comparison details</li> <li>Example: <code>IDENTICAL learn graph in 2.4s \u2192 outputs: /results/graph.xml (unchanged)</code></li> <li>Special Case: For append-semantics files, compares only the action's specific contribution</li> </ul>"},{"location":"design/logging_architecture_design/#different","title":"DIFFERENT","text":"<ul> <li>Mode: compare</li> <li>Condition: Task re-executed, outputs differ from previous run</li> <li>Message: Includes difference summary</li> <li>Example: <code>DIFFERENT learn graph in 2.3s \u2192 outputs: /results/graph.xml (5 edge changes)</code></li> <li>Special Case: For append-semantics files, reports differences in the action's contribution</li> </ul>"},{"location":"design/logging_architecture_design/#error-statuses","title":"Error Statuses","text":""},{"location":"design/logging_architecture_design/#invalid_uses","title":"INVALID_USES","text":"<ul> <li>Mode: All</li> <li>Condition: Action package specified in <code>uses:</code> not found</li> <li>Message: Available actions listed</li> <li>Example: <code>INVALID_USES unknown-action \u2192 Available: [causal-discovery, visualization]</code></li> </ul>"},{"location":"design/logging_architecture_design/#invalid_parameter","title":"INVALID_PARAMETER","text":"<ul> <li>Mode: All (detected in dry-run)</li> <li>Condition: Parameters in <code>with:</code> block are invalid for action</li> <li>Message: Specific parameter validation errors</li> <li>Example: <code>INVALID_PARAMETER learn graph \u2192 missing required: data_path, invalid: alpha=-0.1</code></li> </ul>"},{"location":"design/logging_architecture_design/#failed","title":"FAILED","text":"<ul> <li>Mode: run, compare</li> <li>Condition: Task execution threw unexpected exception</li> <li>Message: Exception details with traceback reference</li> <li>Example: <code>FAILED learn graph after 1.2s \u2192 FileNotFoundError: /data/missing.csv (see log line 1234)</code></li> </ul>"},{"location":"design/logging_architecture_design/#timed_out","title":"TIMED_OUT","text":"<ul> <li>Mode: run, compare  </li> <li>Condition: Task exceeded configured timeout</li> <li>Message: Timeout duration and partial results</li> <li>Example: <code>TIMED_OUT learn graph after 300s \u2192 timeout: 300s, partial outputs may exist</code></li> </ul>"},{"location":"design/logging_architecture_design/#architecture-design","title":"Architecture Design","text":""},{"location":"design/logging_architecture_design/#component-overview","title":"Component Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  WorkflowExecutor \u2502\u2500\u2500\u2500\u2192\u2502  ActionExecutor  \u2502\u2500\u2500\u2500\u2192\u2502     Action      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                       \u2502                       \u2502\n         \u25bc                       \u25bc                       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  WorkflowLogger  \u2502\u2190\u2500\u2500\u2500\u2502   StatusTracker  \u2502    \u2502   FileManager   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                       \u2502                       \n         \u25bc                       \u25bc                       \n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \n\u2502ProgressReporter  \u2502    \u2502  Summary Report  \u2502              \n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \n</code></pre>"},{"location":"design/logging_architecture_design/#core-classes","title":"Core Classes","text":""},{"location":"design/logging_architecture_design/#workflowlogger","title":"WorkflowLogger","text":"<pre><code>class WorkflowLogger:\n    \"\"\"Centralized logging with multiple output destinations.\"\"\"\n\n    def __init__(self, \n                 terminal: bool = True,\n                 log_file: Optional[Path] = None,\n                 log_level: LogLevel = LogLevel.SUMMARY):\n\n    def log_task(self, \n                action_name: str, \n                status: TaskStatus,\n                message: str,\n                runtime: Optional[float] = None,\n                inputs: Optional[Dict[str, Any]] = None,\n                outputs: Optional[str] = None,\n                job_info: Optional[str] = None) -&gt; None:\n\n    def start_progress(self, total_jobs: int, estimated_runtime: float) -&gt; None:\n    def update_progress(self, completed: int, current_job: str) -&gt; None:\n    def finish_progress(self) -&gt; None:\n</code></pre>"},{"location":"design/logging_architecture_design/#taskstatus","title":"TaskStatus","text":"<pre><code>class TaskStatus(Enum):\n    \"\"\"Enumeration of all possible task execution statuses.\"\"\"\n    EXECUTES = \"EXECUTES\"\n    WOULD_EXECUTE = \"WOULD_EXECUTE\"\n    SKIPS = \"SKIPS\"\n    WOULD_SKIP = \"WOULD_SKIP\"\n    IDENTICAL = \"IDENTICAL\"\n    DIFFERENT = \"DIFFERENT\"\n    INVALID_USES = \"INVALID_USES\"\n    INVALID_PARAMETER = \"INVALID_PARAMETER\"\n    FAILED = \"FAILED\"\n    TIMED_OUT = \"TIMED_OUT\"\n</code></pre>"},{"location":"design/logging_architecture_design/#actionexecutor","title":"ActionExecutor","text":"<pre><code>class ActionExecutor:\n    \"\"\"Handles action execution with logging and status determination.\"\"\"\n\n    def __init__(self, logger: WorkflowLogger, file_manager: FileManager):\n\n    def execute_action(self, \n                      action_name: str,\n                      action_class: Type[Action], \n                      inputs: Dict[str, Any],\n                      mode: str,\n                      context: WorkflowContext) -&gt; Dict[str, Any]:\n        \"\"\"Execute action with comprehensive status logging.\"\"\"\n\n    def should_skip(self, action_class: Type[Action], inputs: Dict[str, Any]) -&gt; bool:\n        \"\"\"Determine if action should be skipped based on existing outputs.\"\"\"\n\n    def estimate_runtime(self, action_class: Type[Action], inputs: Dict[str, Any]) -&gt; float:\n        \"\"\"Get runtime estimate from action for progress calculation.\"\"\"\n</code></pre>"},{"location":"design/logging_architecture_design/#modified-action-interface","title":"Modified Action Interface","text":"<pre><code>class Action(ABC):\n    \"\"\"Base class with optional logging support.\"\"\"\n\n    @abstractmethod\n    def run(self, \n           inputs: Dict[str, Any], \n           mode: str = \"dry-run\",\n           context: Optional[WorkflowContext] = None,\n           logger: Optional[WorkflowLogger] = None) -&gt; Dict[str, Any]:\n        \"\"\"Execute with optional logging capability.\"\"\"\n\n    def estimate_runtime(self, inputs: Dict[str, Any]) -&gt; float:\n        \"\"\"Provide runtime estimate for progress calculation.\"\"\"\n        return 1.0  # Default: 1 second estimate\n\n    def get_output_files(self, inputs: Dict[str, Any]) -&gt; List[str]:\n        \"\"\"Return list of output files this action will create.\"\"\"\n        return []  # Default: no specific outputs\n\n    def get_output_contribution_key(self, inputs: Dict[str, Any]) -&gt; Optional[str]:\n        \"\"\"Return key identifying this action's contribution in append-semantics files.\n\n        Used for files like metadata.json where multiple actions append content\n        rather than replacing the entire file. Return None for traditional files.\n        \"\"\"\n        return None  # Default: traditional replace-semantics\n\n    def has_existing_contribution(self, file_path: str, inputs: Dict[str, Any]) -&gt; bool:\n        \"\"\"Check if this action's contribution already exists in append-semantics file.\n\n        Only called when get_output_contribution_key() returns non-None.\n        Used to determine if action can skip execution.\n        \"\"\"\n        return False  # Default: not applicable\n</code></pre>"},{"location":"design/logging_architecture_design/#implementation-phases","title":"Implementation Phases","text":""},{"location":"design/logging_architecture_design/#phase-1-core-logging-infrastructure","title":"Phase 1: Core Logging Infrastructure","text":"<p>Target: Basic logging working with actions</p> <p>Components: - <code>TaskStatus</code> enum with all status definitions - <code>WorkflowLogger</code> class with file/terminal output - Modified <code>Action.run()</code> signature with optional logger parameter - Basic status logging from within actions</p> <p>Benefits: - Actions can immediately start logging task-level messages - Foundation for all subsequent logging features - No breaking changes (logger parameter optional)</p> <p>Example Usage:</p> <pre><code># Inside action.run()\nif logger:\n    logger.log_task(\n        action_name=self.name,\n        status=TaskStatus.EXECUTES,\n        message=\"learn graph\",\n        runtime=2.3,\n        inputs={\"dataset\": \"/data/asia.csv\"},\n        outputs=\"/results/graph.xml\"\n    )\n</code></pre>"},{"location":"design/logging_architecture_design/#phase-2-smart-execution-logic","title":"Phase 2: Smart Execution Logic","text":"<p>Target: Add output detection and skip logic</p> <p>Components: - <code>FileManager</code> class for output file detection - <code>ActionExecutor</code> class with skip logic - Runtime estimation interface - SKIP/WOULD_SKIP status implementation</p> <p>Benefits: - Intelligent workflow execution avoiding duplicate work - Progress estimation foundation - Proper dry-run vs run mode differentiation</p>"},{"location":"design/logging_architecture_design/#phase-3-cli-integration","title":"Phase 3: CLI Integration","text":"<p>Target: Complete user interface with logging</p> <p>Components: - CLI parameters: <code>--log-file</code>, <code>--log-level</code> - <code>ProgressReporter</code> with Click integration - Summary reports with status counts - User-friendly error display</p> <p>Benefits: - Real-world testing of logging design - Complete user experience - Feedback for design refinement</p>"},{"location":"design/logging_architecture_design/#phase-4-advanced-features","title":"Phase 4: Advanced Features","text":"<p>Target: Compare mode and monitoring</p> <p>Components: - File comparison for IDENTICAL/DIFFERENT - Resource monitoring (memory, CPU) - Timeout handling with TIMED_OUT - Enhanced progress displays</p>"},{"location":"design/logging_architecture_design/#design-decisions","title":"Design Decisions","text":""},{"location":"design/logging_architecture_design/#append-semantics-output-files","title":"Append-Semantics Output Files","text":""},{"location":"design/logging_architecture_design/#the-metadata-challenge","title":"The Metadata Challenge","text":"<p>Structure learning experiments often produce a <code>metadata.json</code> file that has append semantics rather than replace semantics:</p> <ul> <li>Initial Creation: Structure learning action creates metadata with algorithm details, runtime, parameters</li> <li>Subsequent Additions: Analysis actions add scores, metrics, validation results to the same file</li> <li>Incremental Growth: File grows over workflow execution rather than being replaced</li> </ul>"},{"location":"design/logging_architecture_design/#impact-on-task-status-determination","title":"Impact on Task Status Determination","text":"<p>Traditional Files (e.g., <code>graph.xml</code>): - File exists \u2192 task can skip - File missing \u2192 task must execute - Compare mode: entire file comparison</p> <p>Append-Semantics Files (e.g., <code>metadata.json</code>): - File exists \u2192 check if this action's contribution already present - Action's section missing \u2192 must execute (even if file exists) - Compare mode: compare only this action's contribution</p>"},{"location":"design/logging_architecture_design/#implementation-strategy","title":"Implementation Strategy","text":"<p>Action Interface Extension:</p> <pre><code>class Action(ABC):\n    def get_output_contribution_key(self, inputs: Dict[str, Any]) -&gt; Optional[str]:\n        \"\"\"Return key identifying this action's contribution in append-semantics files.\"\"\"\n        return None  # Most actions don't use append semantics\n\n    def has_existing_contribution(self, file_path: str, inputs: Dict[str, Any]) -&gt; bool:\n        \"\"\"Check if this action's contribution already exists in append-semantics file.\"\"\"\n        return False  # Default: not applicable\n</code></pre> <p>FileManager Enhancement:</p> <pre><code>class FileManager:\n    def should_skip_action(self, action: Action, inputs: Dict[str, Any]) -&gt; bool:\n        output_files = action.get_output_files(inputs)\n        for file_path in output_files:\n            if not os.path.exists(file_path):\n                return False  # Missing file, must execute\n\n            # Check append-semantics files\n            contrib_key = action.get_output_contribution_key(inputs)\n            if contrib_key and not action.has_existing_contribution(file_path, inputs):\n                return False  # Action's contribution missing\n\n        return True  # All outputs present (traditional or append-semantics)\n</code></pre> <p>Comparison Logic:</p> <pre><code>class FileManager:\n    def compare_outputs(self, action: Action, inputs: Dict[str, Any], \n                       old_run_dir: str, new_run_dir: str) -&gt; ComparisonResult:\n        \"\"\"Compare outputs, handling append-semantics files appropriately.\"\"\"\n\n        output_files = action.get_output_files(inputs)\n        for file_path in output_files:\n            contrib_key = action.get_output_contribution_key(inputs)\n            if contrib_key:\n                # Compare only this action's contribution\n                return self._compare_contribution(old_run_dir, new_run_dir, \n                                                file_path, contrib_key)\n            else:\n                # Traditional full-file comparison\n                return self._compare_full_file(old_run_dir, new_run_dir, file_path)\n</code></pre>"},{"location":"design/logging_architecture_design/#example-structure-learning-analysis-pipeline","title":"Example: Structure Learning + Analysis Pipeline","text":"<p>Workflow:</p> <pre><code>jobs:\n  structure_learning:\n    uses: causal-discovery\n    with: \n      algorithm: pc\n      data: data/asia.csv\n\n  analysis:\n    uses: graph-analysis  \n    with:\n      graph: ${{ jobs.structure_learning.outputs.graph }}\n</code></pre> <p>Execution Sequence:</p> <ol> <li>Structure Learning (first run):</li> <li>Creates: <code>/results/graph.xml</code>, <code>/results/metadata.json</code></li> <li><code>metadata.json</code>: <code>{\"structure_learning\": {\"algorithm\": \"pc\", \"runtime\": 2.3, ...}}</code></li> <li> <p>Status: <code>EXECUTES</code></p> </li> <li> <p>Analysis (first run):</p> </li> <li>Reads: <code>/results/graph.xml</code></li> <li>Appends to: <code>/results/metadata.json</code> </li> <li><code>metadata.json</code>: <code>{\"structure_learning\": {...}, \"analysis\": {\"scores\": {...}, \"metrics\": {...}}}</code></li> <li> <p>Status: <code>EXECUTES</code></p> </li> <li> <p>Structure Learning (second run):</p> </li> <li><code>metadata.json</code> exists but missing <code>structure_learning</code> section (analysis cleared it)</li> <li> <p>Status: <code>EXECUTES</code> (recreates structure learning contribution)</p> </li> <li> <p>Analysis (second run):</p> </li> <li><code>metadata.json</code> exists with <code>structure_learning</code> but missing <code>analysis</code> section</li> <li> <p>Status: <code>EXECUTES</code> (adds analysis contribution)</p> </li> <li> <p>Structure Learning (third run):</p> </li> <li><code>metadata.json</code> exists with <code>structure_learning</code> section present</li> <li>Status: <code>SKIPS</code> (contribution already exists)</li> </ol>"},{"location":"design/logging_architecture_design/#design-benefits","title":"Design Benefits","text":"<ul> <li>Precise Skip Logic: Actions only skip when their specific contribution exists</li> <li>Accurate Comparison: Compare mode focuses on action's actual output changes</li> <li>Incremental Execution: Workflows can build up metadata files progressively</li> <li>Clear Status Reporting: Users understand exactly what each action contributed</li> </ul>"},{"location":"design/logging_architecture_design/#why-task-level-logging","title":"Why Task-Level Logging?","text":"<ul> <li>Actions control granularity: Each action knows its internal complexity</li> <li>Flexible reporting: Single chart vs. hundreds of graphs handled appropriately  </li> <li>Real-time feedback: Users see progress as work happens</li> <li>Debugging capability: Specific task failures easily identified</li> </ul>"},{"location":"design/logging_architecture_design/#why-status-based-execution","title":"Why Status-Based Execution?","text":"<ul> <li>Mode-aware behavior: Same action behaves correctly in dry-run vs run</li> <li>Smart skipping: Avoid duplicate work, enable incremental execution</li> <li>Compare mode support: Foundation for regression testing workflows</li> <li>Clear error categorization: Different error types handled appropriately</li> </ul>"},{"location":"design/logging_architecture_design/#why-optional-logger-parameter","title":"Why Optional Logger Parameter?","text":"<ul> <li>Backward compatibility: Existing actions continue working unchanged</li> <li>Gradual adoption: Actions can add logging over time</li> <li>Testing simplicity: Tests can inject mock loggers easily</li> <li>Performance: No overhead for actions that don't use logging</li> </ul>"},{"location":"design/logging_architecture_design/#integration-points","title":"Integration Points","text":""},{"location":"design/logging_architecture_design/#workflowexecutor-integration","title":"WorkflowExecutor Integration","text":"<ul> <li>WorkflowExecutor creates WorkflowLogger based on CLI parameters</li> <li>Passes logger to ActionExecutor for all action executions</li> <li>Coordinates progress reporting across matrix jobs</li> <li>Handles summary report generation</li> </ul>"},{"location":"design/logging_architecture_design/#actionregistry-integration","title":"ActionRegistry Integration","text":"<ul> <li>Validates actions support logging interface during discovery</li> <li>Provides action metadata for runtime estimation</li> <li>Enables error status reporting for INVALID_USES</li> </ul>"},{"location":"design/logging_architecture_design/#exception-handling-integration","title":"Exception Handling Integration","text":"<ul> <li>All action exceptions caught by ActionExecutor</li> <li>Converted to appropriate FAILED status messages</li> <li>Stack traces logged with reference line numbers</li> <li>Graceful workflow continuation after failures</li> </ul>"},{"location":"design/logging_architecture_design/#testing-strategy","title":"Testing Strategy","text":""},{"location":"design/logging_architecture_design/#unit-testing","title":"Unit Testing","text":"<ul> <li>Mock logger captures for status verification</li> <li>FileManager testing with temporary directories</li> <li>ActionExecutor testing with mock actions</li> <li>Status transition testing for all scenarios</li> </ul>"},{"location":"design/logging_architecture_design/#integration-testing","title":"Integration Testing","text":"<ul> <li>End-to-end logging through WorkflowExecutor</li> <li>CLI parameter integration testing</li> <li>File output format validation</li> <li>Progress reporter integration testing</li> </ul>"},{"location":"design/logging_architecture_design/#performance-testing","title":"Performance Testing","text":"<ul> <li>Logging overhead measurement</li> <li>Large matrix workflow testing</li> <li>Memory usage monitoring</li> <li>File I/O performance validation</li> </ul>"},{"location":"design/logging_architecture_design/#future-extensions","title":"Future Extensions","text":""},{"location":"design/logging_architecture_design/#advanced-features","title":"Advanced Features","text":"<ul> <li>Log parsing tools: Analysis scripts for workflow execution logs</li> <li>Monitoring integration: Prometheus/Grafana metrics export</li> <li>Notification system: Slack/email alerts for workflow completion</li> <li>Dashboard integration: Web UI for workflow execution monitoring</li> </ul>"},{"location":"design/logging_architecture_design/#action-enhancements","title":"Action Enhancements","text":"<ul> <li>Detailed progress: Sub-task progress reporting within actions</li> <li>Resource prediction: More sophisticated runtime/memory estimation</li> <li>Parallel logging: Thread-safe logging for parallel action execution</li> <li>Custom status types: Action-specific status extensions</li> </ul>"},{"location":"design/logging_architecture_design/#conclusion","title":"Conclusion","text":"<p>This logging architecture provides: - Complete visibility into workflow execution at appropriate granularity - User-friendly experience with progress indication and clear error reporting - Developer-friendly integration with optional, backward-compatible interface - Foundation for advanced features like compare mode and monitoring - Testing-ready design with mockable interfaces and captured output</p> <p>The phased implementation approach allows for incremental development with real-world feedback at each stage, ensuring the final design meets actual user needs while maintaining system reliability and performance.</p>"},{"location":"design/matrix_expansion_design/","title":"CI Workflow Matrix Strategy Implementation","text":""},{"location":"design/matrix_expansion_design/#overview","title":"Overview","text":"<p>The matrix strategy implementation provides the GitHub Actions-style <code>strategy.matrix</code> functionality within the unified CI workflow engine. This is not a separate matrix expansion system, but rather a core component of the CI workflow architecture that handles the complex logic of experiment combination generation.</p>"},{"location":"design/matrix_expansion_design/#github-actions-matrix-strategy","title":"GitHub Actions Matrix Strategy","text":""},{"location":"design/matrix_expansion_design/#core-matrix-features","title":"Core Matrix Features","text":"<p>The implementation supports all GitHub Actions matrix strategy features:</p> <pre><code>strategy:\n  matrix:\n    algorithm: [\"PC\", \"GES\", \"LINGAM\"]\n    network: [\"asia\", \"sachs\"]\n    sample_size: [100, 500]\n\n  exclude:\n    - algorithm: \"LINGAM\"\n      network: \"alarm\"  # Remove specific combinations\n\n  include:\n    - algorithm: \"PC\"\n      network: \"asia\"\n      alpha: 0.01      # Add combinations with extra parameters\n\n  fail_fast: false      # Continue other jobs if one fails\n  max_parallel: 8       # Limit concurrent execution\n</code></pre>"},{"location":"design/matrix_expansion_design/#matrix-strategy-implementation","title":"Matrix Strategy Implementation","text":"<pre><code>import itertools\nfrom typing import Dict, List, Any\n\nclass CIMatrixStrategy:\n    \"\"\"Implement GitHub Actions matrix strategy within CI workflow engine.\"\"\"\n\n    def expand_matrix_strategy(self, strategy_config: Dict) -&gt; List[Dict]:\n        \"\"\"\n        Generate all job combinations from strategy.matrix configuration.\n\n        This is part of the CI workflow engine, not a separate expansion system.\n\n        Input:\n          strategy:\n            matrix:\n              algorithm: [\"PC\", \"GES\", \"LINGAM\"]\n              network: [\"asia\", \"sachs\"]\n              sample_size: [100, 500]\n\n        Output:\n          12 jobs with all combinations of algorithm \u00d7 network \u00d7 sample_size\n        \"\"\"\n        matrix_config = strategy_config.get(\"matrix\", {})\n        exclude_rules = strategy_config.get(\"exclude\", [])\n        include_rules = strategy_config.get(\"include\", [])\n\n        # Generate base cartesian product\n        base_jobs = self._generate_base_combinations(matrix_config)\n\n        # Apply exclude rules\n        filtered_jobs = self._apply_exclude_rules(base_jobs, exclude_rules)\n\n        # Apply include rules  \n        final_jobs = self._apply_include_rules(filtered_jobs, include_rules)\n\n        return final_jobs\n</code></pre>"},{"location":"design/matrix_expansion_design/#advanced-matrix-features","title":"Advanced Matrix Features","text":"<pre><code>def apply_exclude_rules(self, jobs: List[Dict], exclude_rules: List[Dict]) -&gt; List[Dict]:\n    \"\"\"\n    Remove jobs matching exclude patterns.\n\n    Example:\n      exclude:\n        - algorithm: \"LINGAM\"\n          network: \"alarm\"  # Remove LINGAM + alarm combination\n        - sample_size: 100\n          algorithm: \"GES\"   # Remove GES + 100 sample size combination\n    \"\"\"\n    filtered_jobs = []\n\n    for job in jobs:\n        should_exclude = False\n\n        for exclude_rule in exclude_rules:\n            if self._job_matches_pattern(job, exclude_rule):\n                should_exclude = True\n                break\n\n        if not should_exclude:\n            filtered_jobs.append(job)\n\n    return filtered_jobs\n\ndef apply_include_rules(self, jobs: List[Dict], include_rules: List[Dict]) -&gt; List[Dict]:\n    \"\"\"\n    Add specific job combinations even if not in matrix.\n\n    Example:\n      include:\n        - algorithm: \"PC\"\n          network: \"asia\"\n          sample_size: 50    # Add PC + asia + 50 even though 50 not in matrix\n          alpha: 0.01        # Add extra parameter for this combination\n    \"\"\"\n    extended_jobs = jobs.copy()\n\n    for include_rule in include_rules:\n        # Check if this combination already exists\n        if not any(self._job_matches_pattern(job, include_rule) for job in jobs):\n            extended_jobs.append(include_rule.copy())\n\n    return extended_jobs\n\ndef _job_matches_pattern(self, job: Dict, pattern: Dict) -&gt; bool:\n    \"\"\"Check if job matches the given pattern (all pattern keys must match).\"\"\"\n    for key, value in pattern.items():\n        if key not in job or job[key] != value:\n            return False\n    return True\n</code></pre>"},{"location":"design/matrix_expansion_design/#template-substitution-engine","title":"Template Substitution Engine","text":""},{"location":"design/matrix_expansion_design/#jinja2-integration-for-variable-substitution","title":"Jinja2 Integration for Variable Substitution","text":"<pre><code>import jinja2\nfrom typing import Dict, Any\n\nclass TemplateProcessor:\n    \"\"\"Handle GitHub Actions-style template substitution.\"\"\"\n\n    def __init__(self):\n        # Configure Jinja2 with GitHub Actions syntax\n        self.env = jinja2.Environment(\n            variable_start_string=\"${{\",\n            variable_end_string=\"}}\",\n            trim_blocks=True,\n            lstrip_blocks=True\n        )\n\n    def substitute_job_variables(self, template: str, job_context: Dict) -&gt; str:\n        \"\"\"\n        Process template with job-specific variables.\n\n        Example:\n          template: \"network_${{ matrix.network }}_${{ matrix.sample_size }}.csv\"\n          job_context: {\"matrix\": {\"network\": \"asia\", \"sample_size\": 100}}\n          result: \"network_asia_100.csv\"\n        \"\"\"\n        jinja_template = self.env.from_string(template)\n        return jinja_template.render(**job_context)\n\n    def build_job_context(self, job: Dict, workflow_env: Dict, \n                         step_outputs: Dict = None) -&gt; Dict:\n        \"\"\"\n        Build complete context for template substitution.\n\n        Available variables:\n        - matrix.*: Matrix variables for current job\n        - env.*: Environment variables\n        - steps.*: Outputs from previous steps\n        - github.*: GitHub Actions compatibility variables\n        \"\"\"\n        context = {\n            \"matrix\": job,\n            \"env\": workflow_env,\n            \"github\": {\n                \"workspace\": \"/tmp/causaliq-workspace\",  # Configurable\n                \"run_id\": \"12345\",  # Generated\n                \"run_number\": \"1\"\n            }\n        }\n\n        if step_outputs:\n            context[\"steps\"] = step_outputs\n\n        return context\n</code></pre>"},{"location":"design/matrix_expansion_design/#integration-with-dask-task-graphs","title":"Integration with DASK Task Graphs","text":""},{"location":"design/matrix_expansion_design/#job-dask-task-conversion","title":"Job \u2192 DASK Task Conversion","text":"<pre><code>import dask\nfrom typing import Dict, List, Callable\n\nclass DaskTaskGraphBuilder:\n    \"\"\"Convert matrix jobs into DASK computation graph.\"\"\"\n\n    def build_workflow_graph(self, jobs: List[Dict], \n                            workflow_definition: Dict) -&gt; Dict:\n        \"\"\"\n        Convert expanded matrix jobs into DASK task graph.\n\n        Each job becomes a series of DASK tasks corresponding to workflow steps.\n        Dependencies between steps are handled through DASK delayed mechanism.\n        \"\"\"\n        graph = {}\n\n        for job_idx, job in enumerate(jobs):\n            job_id = f\"job_{job_idx}\"\n            step_results = {}\n\n            # Process each step in the workflow\n            for step_idx, step in enumerate(workflow_definition[\"steps\"]):\n                step_id = f\"{job_id}_step_{step_idx}\"\n                step_name = step.get(\"name\", f\"step_{step_idx}\")\n\n                # Build step task with dependencies\n                step_task = self._build_step_task(\n                    step=step,\n                    job_context=job,\n                    previous_results=step_results,\n                    workflow_env=workflow_definition.get(\"env\", {})\n                )\n\n                graph[step_id] = step_task\n                step_results[step_name] = step_id\n\n        return graph\n\n    def _build_step_task(self, step: Dict, job_context: Dict, \n                        previous_results: Dict, workflow_env: Dict) -&gt; tuple:\n        \"\"\"\n        Build individual DASK task for workflow step.\n\n        Returns tuple: (function, *args) suitable for DASK graph\n        \"\"\"\n        action_name = step[\"uses\"]\n        action_inputs = step.get(\"with\", {})\n\n        # Substitute templates in action inputs\n        template_processor = TemplateProcessor()\n        full_context = template_processor.build_job_context(\n            job=job_context,\n            workflow_env=workflow_env,\n            step_outputs=previous_results\n        )\n\n        substituted_inputs = {}\n        for key, value in action_inputs.items():\n            if isinstance(value, str):\n                substituted_inputs[key] = template_processor.substitute_job_variables(\n                    value, full_context\n                )\n            else:\n                substituted_inputs[key] = value\n\n        # Return DASK task tuple\n        return (\n            self._execute_action,\n            action_name,\n            substituted_inputs,\n            job_context,\n            previous_results\n        )\n\n    def _execute_action(self, action_name: str, inputs: Dict, \n                       job_context: Dict, previous_results: Dict) -&gt; Any:\n        \"\"\"Execute workflow action with proper error handling.\"\"\"\n        from causaliq_workflow.actions import ActionRegistry\n\n        registry = ActionRegistry()\n        return registry.execute_action(action_name, inputs)\n</code></pre>"},{"location":"design/matrix_expansion_design/#performance-optimisations","title":"Performance Optimisations","text":""},{"location":"design/matrix_expansion_design/#intelligent-job-batching","title":"Intelligent Job Batching","text":"<pre><code>class MatrixOptimiser:\n    \"\"\"Optimise matrix job execution for performance.\"\"\"\n\n    def batch_similar_jobs(self, jobs: List[Dict], max_batch_size: int = 10) -&gt; List[List[Dict]]:\n        \"\"\"\n        Group jobs by similarity for efficient resource usage.\n\n        Jobs using same algorithm/package can share initialisation costs.\n        Jobs using same dataset can share data loading costs.\n        \"\"\"\n        batches = []\n\n        # Group by algorithm package for shared initialisation\n        algorithm_groups = {}\n        for job in jobs:\n            algorithm = job.get(\"algorithm\", \"unknown\")\n            if algorithm not in algorithm_groups:\n                algorithm_groups[algorithm] = []\n            algorithm_groups[algorithm].append(job)\n\n        # Further group by dataset within each algorithm\n        for algorithm, algo_jobs in algorithm_groups.items():\n            dataset_groups = {}\n            for job in algo_jobs:\n                dataset = job.get(\"network\", \"unknown\")\n                if dataset not in dataset_groups:\n                    dataset_groups[dataset] = []\n                dataset_groups[dataset].append(job)\n\n            # Create batches within each dataset group\n            for dataset, dataset_jobs in dataset_groups.items():\n                for i in range(0, len(dataset_jobs), max_batch_size):\n                    batch = dataset_jobs[i:i + max_batch_size]\n                    batches.append(batch)\n\n        return batches\n\n    def estimate_job_resources(self, job: Dict) -&gt; Dict[str, Any]:\n        \"\"\"\n        Estimate resource requirements for job.\n\n        Used for intelligent DASK worker allocation.\n        \"\"\"\n        algorithm = job.get(\"algorithm\", \"\")\n        sample_size = job.get(\"sample_size\", 100)\n        network_size = self._estimate_network_complexity(job.get(\"network\", \"\"))\n\n        # Simple heuristics - can be refined with benchmarking data\n        memory_mb = max(500, sample_size * network_size * 0.1)\n        cpu_time_minutes = max(1, (sample_size * network_size) / 10000)\n\n        return {\n            \"memory_mb\": memory_mb,\n            \"cpu_time_minutes\": cpu_time_minutes,\n            \"io_intensive\": algorithm.lower() in [\"ges\", \"gies\"],  # Score-based algorithms\n            \"cross_language\": self._requires_cross_language_bridge(algorithm)\n        }\n</code></pre>"},{"location":"design/matrix_expansion_design/#integration-with-ci-workflow-engine","title":"Integration with CI Workflow Engine","text":""},{"location":"design/matrix_expansion_design/#unified-architecture","title":"Unified Architecture","text":"<p>The matrix strategy implementation is a core component of the unified CI workflow engine, not a separate system:</p> <pre><code>class CIWorkflowEngine:\n    \"\"\"Unified CI workflow engine with integrated matrix strategy support.\"\"\"\n\n    def __init__(self):\n        self.matrix_strategy = CIMatrixStrategy()\n        self.template_processor = TemplateProcessor()\n        self.action_registry = ActionRegistry()\n        self.dask_builder = DaskTaskGraphBuilder()\n\n    def execute_workflow(self, workflow_yaml: str) -&gt; WorkflowResult:\n        \"\"\"Execute complete CI workflow with matrix strategy expansion.\"\"\"\n        workflow_def = self.parse_workflow(workflow_yaml)\n\n        # Expand matrix strategy if present\n        if \"strategy\" in workflow_def:\n            jobs = self.matrix_strategy.expand_matrix_strategy(workflow_def[\"strategy\"])\n        else:\n            jobs = [{}]  # Single job with empty context\n\n        # Build DASK task graph for all jobs\n        task_graph = self.dask_builder.build_workflow_graph(jobs, workflow_def)\n\n        # Execute with DASK\n        return self._execute_dask_graph(task_graph)\n</code></pre>"},{"location":"design/matrix_expansion_design/#key-design-principles","title":"Key Design Principles","text":"<ol> <li>Unified System: Matrix strategy is part of CI workflow engine, not separate component</li> <li>GitHub Actions Compatibility: Full compatibility with GitHub Actions matrix syntax</li> <li>DASK Integration: Matrix jobs convert directly to DASK task graph</li> <li>Template Support: Full Jinja2 template substitution with <code>${{ matrix.variable }}</code> syntax</li> <li>Resource Management: Matrix features like <code>max_parallel</code> map to DASK execution controls</li> </ol> <p>This implementation provides the sophisticated matrix capabilities of GitHub Actions while maintaining the unified architecture of the CI workflow system for causal discovery research.</p>"},{"location":"design/workflow_executor_design/","title":"WorkflowExecutor Implementation Design","text":""},{"location":"design/workflow_executor_design/#overview","title":"Overview","text":"<p>The <code>WorkflowExecutor</code> class provides the foundation for parsing and executing GitHub Actions-style YAML workflows with matrix expansion support. This implementation focuses on the parsing and preparation phase, establishing the infrastructure for workflow execution.</p>"},{"location":"design/workflow_executor_design/#implementation-architecture","title":"Implementation Architecture","text":""},{"location":"design/workflow_executor_design/#class-structure","title":"Class Structure","text":"<pre><code>class WorkflowExecutor:\n    \"\"\"Parse and execute GitHub Actions-style workflows with matrix expansion.\"\"\"\n\n    def parse_workflow(self, workflow_path: Union[str, Path]) -&gt; Dict[str, Any]\n    def expand_matrix(self, matrix: Dict[str, List[Any]]) -&gt; List[Dict[str, Any]]\n    def construct_paths(self, job: Dict[str, Any], data_root: str, \n                       output_root: str, workflow_id: str) -&gt; Dict[str, str]\n</code></pre>"},{"location":"design/workflow_executor_design/#key-design-decisions","title":"Key Design Decisions","text":""},{"location":"design/workflow_executor_design/#1-integration-with-existing-schema-validation","title":"1. Integration with Existing Schema Validation","text":"<p>The <code>WorkflowExecutor</code> leverages the existing <code>causaliq_workflow.schema</code> module for workflow validation:</p> <pre><code>def parse_workflow(self, workflow_path):\n    workflow = load_workflow_file(workflow_path)  # Existing function\n    validate_workflow(workflow)                   # Existing function\n    return workflow\n</code></pre> <p>Rationale: Reuse proven validation logic, maintain single source of truth for workflow structure validation.</p>"},{"location":"design/workflow_executor_design/#2-matrix-expansion-strategy","title":"2. Matrix Expansion Strategy","text":"<p>Matrix expansion uses cartesian product generation:</p> <pre><code>def expand_matrix(self, matrix):\n    variables = list(matrix.keys())\n    value_lists = list(matrix.values())\n    combinations = list(itertools.product(*value_lists))\n\n    jobs = []\n    for combination in combinations:\n        job = dict(zip(variables, combination))\n        jobs.append(job)\n    return jobs\n</code></pre> <p>Rationale:  - Simple, predictable algorithm matching GitHub Actions behaviour - Easy to understand and debug - Supports arbitrary matrix dimensions - Deterministic ordering for reproducible results</p>"},{"location":"design/workflow_executor_design/#3-path-construction-pattern","title":"3. Path Construction Pattern","text":"<p>Paths follow the established pattern from the examples:</p> <pre><code># Input: {data_root}/{dataset}/input.csv\n# Output: {output_root}/{workflow_id}/{dataset}_{algorithm}/\n</code></pre> <p>Rationale: - Consistent with documentation examples - Organises outputs by workflow and experiment parameters - Supports hierarchical result organisation - Compatible with existing action framework expectations</p>"},{"location":"design/workflow_executor_design/#4-error-handling-strategy","title":"4. Error Handling Strategy","text":"<p>All methods use consistent error propagation:</p> <pre><code>try:\n    # Core logic\nexcept Exception as e:\n    raise WorkflowExecutionError(f\"Operation failed: {e}\") from e\n</code></pre> <p>Rationale: - Maintains error chain for debugging - Provides consistent error interface - Follows established patterns in the codebase</p>"},{"location":"design/workflow_executor_design/#implementation-phases","title":"Implementation Phases","text":""},{"location":"design/workflow_executor_design/#phase-1-parsing-foundation-complete","title":"Phase 1: Parsing Foundation (Complete \u2705)","text":"<p>Scope: Basic workflow parsing and matrix expansion - Parse and validate YAML workflow files - Expand matrix variables into job configurations - Construct file paths from matrix variables - Comprehensive error handling</p> <p>Test Coverage: 100% with edge case coverage - Unit tests with mocked dependencies - Functional tests with real YAML files - Exception handling verification</p>"},{"location":"design/workflow_executor_design/#phase-2-execution-engine-future","title":"Phase 2: Execution Engine (Future)","text":"<p>Scope: Step execution and orchestration - Execute workflow steps with action coordination - Environment variable management - Conditional execution (<code>if:</code> conditions) - Step output handling and dependencies</p>"},{"location":"design/workflow_executor_design/#phase-3-advanced-features-future","title":"Phase 3: Advanced Features (Future)","text":"<p>Scope: Enterprise workflow features - DASK task graph integration - Progress monitoring and status reporting - Resource management and limits - Workflow queue management</p>"},{"location":"design/workflow_executor_design/#integration-points","title":"Integration Points","text":""},{"location":"design/workflow_executor_design/#with-action-framework","title":"With Action Framework","text":"<pre><code># Future integration pattern\nfor step in workflow[\"steps\"]:\n    action = ActionRegistry.get_action(step[\"uses\"])\n    inputs = construct_action_inputs(step[\"with\"], job_context)\n    result = action.run(inputs)\n</code></pre>"},{"location":"design/workflow_executor_design/#with-schema-validation","title":"With Schema Validation","text":"<pre><code># Current integration\nworkflow = load_workflow_file(path)     # schema.py\nvalidate_workflow(workflow)             # schema.py\njobs = executor.expand_matrix(workflow[\"matrix\"])  # workflow.py\n</code></pre>"},{"location":"design/workflow_executor_design/#with-dask-future","title":"With DASK (Future)","text":"<pre><code># Planned integration\njobs = executor.expand_matrix(matrix)\ntask_graph = DaskBuilder.build_workflow_graph(jobs, workflow)\nresults = executor.execute_dask_graph(task_graph)\n</code></pre>"},{"location":"design/workflow_executor_design/#testing-strategy","title":"Testing Strategy","text":""},{"location":"design/workflow_executor_design/#unit-tests-isolated-logic","title":"Unit Tests (Isolated Logic)","text":"<ul> <li>Matrix expansion algorithm verification</li> <li>Path construction with various inputs</li> <li>Exception handling edge cases</li> <li>Mocked dependencies for isolation</li> </ul>"},{"location":"design/workflow_executor_design/#functional-tests-real-operations","title":"Functional Tests (Real Operations)","text":"<ul> <li>YAML file parsing with real workflow files</li> <li>Filesystem operations for temporary test files</li> <li>Integration with schema validation</li> <li>End-to-end workflow parsing scenarios</li> </ul>"},{"location":"design/workflow_executor_design/#edge-case-coverage","title":"Edge Case Coverage","text":"<ul> <li>Exception handling in matrix expansion (<code>itertools.product</code> failures)</li> <li>Empty matrices and default value handling</li> <li>Invalid workflow file scenarios</li> <li>Missing matrix variable handling</li> </ul>"},{"location":"design/workflow_executor_design/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"design/workflow_executor_design/#matrix-expansion-scaling","title":"Matrix Expansion Scaling","text":"<ul> <li>Time Complexity: O(n\u2081 \u00d7 n\u2082 \u00d7 ... \u00d7 n\u2096) where n\u1d62 is the size of each matrix dimension</li> <li>Space Complexity: O(jobs) linear in the number of generated job combinations</li> <li>Practical Limits: Reasonable for typical causal discovery experiments (&lt; 1000 jobs)</li> </ul>"},{"location":"design/workflow_executor_design/#memory-usage","title":"Memory Usage","text":"<ul> <li>Workflow parsing: Linear in YAML file size</li> <li>Matrix expansion: Linear in number of generated jobs</li> <li>Path construction: Constant per job</li> </ul>"},{"location":"design/workflow_executor_design/#future-optimisations","title":"Future Optimisations","text":"<ul> <li>Lazy matrix expansion for large matrices</li> <li>Streaming job processing for memory efficiency</li> <li>Job batching for DASK execution</li> </ul>"},{"location":"design/workflow_executor_design/#alignment-with-causaliq-standards","title":"Alignment with CausalIQ Standards","text":""},{"location":"design/workflow_executor_design/#development-guidelines-compliance","title":"Development Guidelines Compliance","text":"<ul> <li>\u2705 Small incremental changes: 99-line focused implementation</li> <li>\u2705 100% test coverage: Comprehensive unit and functional tests  </li> <li>\u2705 CI compliance: All formatting, linting, and type checking standards met</li> <li>\u2705 British English: Documentation and code comments</li> <li>\u2705 Type safety: Complete type annotations with mypy validation</li> </ul>"},{"location":"design/workflow_executor_design/#architecture-principles","title":"Architecture Principles","text":"<ul> <li>\u2705 GitHub Actions foundation: Consistent with CI/CD workflow patterns</li> <li>\u2705 Action-based components: Integration with existing action framework</li> <li>\u2705 Schema-first design: Leverage existing validation infrastructure</li> <li>\u2705 Incremental functionality: Foundation for future execution features</li> </ul> <p>This design provides a solid foundation for workflow execution while maintaining the incremental development approach and high quality standards established in the CausalIQ Workflow project.</p>"}]}